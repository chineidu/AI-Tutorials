{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aba8be1",
   "metadata": {},
   "source": [
    "# Document Analysis Using Vision LLMs\n",
    "\n",
    "This system can do the following:\n",
    "- Process image documents\n",
    "- Extract text from image documents using Vision LLMs\n",
    "- Perform calculations when needed (using tools)\n",
    "- Analyze the content and provide concise summaries\n",
    "- Execute specific instructions related to documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e958c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import (\n",
    "    Annotated,\n",
    "    Any,\n",
    "    Generator,\n",
    "    Iterable,\n",
    "    Literal,\n",
    "    Optional,\n",
    "    TypedDict,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "# Standard imports\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"white\": \"#FFFFFF\",  # Bright white\n",
    "        \"info\": \"#00FF00\",  # Bright green\n",
    "        \"warning\": \"#FFD700\",  # Bright gold\n",
    "        \"error\": \"#FF1493\",  # Deep pink\n",
    "        \"success\": \"#00FFFF\",  # Cyan\n",
    "        \"highlight\": \"#FF4500\",  # Orange-red\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "# import matplotlib.pyplot as pltife\n",
    "\n",
    "# NumPy settings\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "# Polars settings\n",
    "pl.Config.set_fmt_str_lengths(1_000)\n",
    "pl.Config.set_tbl_cols(n=1_000)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_up_from_current_directory(*, go_up: int = 1) -> None:\n",
    "    \"\"\"This is used to up a number of directories.\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "    go_up: int, default=1\n",
    "        This indicates the number of times to go back up from the current directory.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    CONST: str = \"../\"\n",
    "    NUM: str = CONST * go_up\n",
    "\n",
    "    # Goto the previous directory\n",
    "    prev_directory = os.path.join(os.path.dirname(__name__), NUM)\n",
    "    # Get the 'absolute path' of the previous directory\n",
    "    abs_path_prev_directory = os.path.abspath(prev_directory)\n",
    "\n",
    "    # Add the path to the System paths\n",
    "    sys.path.insert(0, abs_path_prev_directory)\n",
    "    print(abs_path_prev_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/neidu/Desktop/Projects/Personal/My_Projects/AI-Tutorials\n"
     ]
    }
   ],
   "source": [
    "go_up_from_current_directory(go_up=2)\n",
    "\n",
    "\n",
    "from schemas import ModelEnum  # noqa: E402\n",
    "from settings import refresh_settings  # noqa: E402\n",
    "from utilities.utilities import async_retrying_with_print, simple_retry\n",
    "\n",
    "settings = refresh_settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45460743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2 chunks:\n",
      "Chunk 1: 7 sentences, 65 tokens\n",
      "  This is sentence one with several tokens.\n",
      "  Here is another sentence that is quite long and has many tokens for demonstration.\n",
      "  This is a short one.\n",
      "  The algorithm needs to handle varying sentence lengths effectively.\n",
      "  Some sentences might have a lot of tokens and be very informative requiring significant processing.\n",
      "  Others might be brief.\n",
      "  We need to ensure that no chunk exceeds the token limit.\n",
      "\n",
      "Chunk 2: 3 sentences, 28 tokens\n",
      "  At the same time, we want to maintain context between chunks using overlaps.\n",
      "  Overlapping sentences help maintain context between chunks.\n",
      "  This is the last sentence of our example.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable, List\n",
    "\n",
    "\n",
    "def chunk_document(\n",
    "    sentences: List[str],\n",
    "    max_tokens_per_chunk: int,\n",
    "    overlap_size: int = 2,\n",
    "    get_token_count: Callable[[str], int] = lambda text: len(\n",
    "        text.split()\n",
    "    ),  # Simple token counter\n",
    ") -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Chunk a document into smaller chunks based on sentence boundaries while respecting\n",
    "    maximum tokens per chunk constraint.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentences : List[str]\n",
    "        List of sentences from the document\n",
    "    max_tokens_per_chunk : int\n",
    "        Maximum number of tokens allowed in a chunk\n",
    "    overlap_size : int, optional\n",
    "        Number of sentences to overlap between adjacent chunks, default 2\n",
    "    get_token_count : callable, optional\n",
    "        Function to count tokens in text, defaults to simple word splitting\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[List[str]]\n",
    "        List of chunks, where each chunk is a list of sentences\n",
    "    \"\"\"\n",
    "    if not sentences:\n",
    "        return []\n",
    "\n",
    "    # Calculate token count for each sentence\n",
    "    sentence_token_counts = [get_token_count(sentence) for sentence in sentences]\n",
    "\n",
    "    # Check if any individual sentence exceeds the token limit\n",
    "    max_sentence_tokens = max(sentence_token_counts)\n",
    "    if max_sentence_tokens > max_tokens_per_chunk:\n",
    "        print(\n",
    "            f\"Warning: Some sentences exceed the token limit ({max_sentence_tokens} > {max_tokens_per_chunk})\"\n",
    "        )\n",
    "        print(\n",
    "            \"These sentences will be placed in their own chunks, exceeding the token limit.\"\n",
    "        )\n",
    "\n",
    "    # Create chunks based solely on token limit\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_token_count = 0\n",
    "\n",
    "    for sentence, token_count in zip(sentences, sentence_token_counts, strict=True):\n",
    "        # If adding this sentence would exceed the limit and we already have sentences in the chunk,\n",
    "        # finalize the current chunk and start a new one\n",
    "        if current_token_count + token_count > max_tokens_per_chunk and current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = []\n",
    "            current_token_count = 0\n",
    "\n",
    "        # Add the sentence to the current chunk\n",
    "        current_chunk.append(sentence)\n",
    "        current_token_count += token_count\n",
    "\n",
    "    # Add the last chunk if not empty\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    # Create overlapping chunks\n",
    "    overlapping_chunks = []\n",
    "\n",
    "    for i in range(len(chunks)):\n",
    "        if i < len(chunks) - 1:\n",
    "            # Get overlap from next chunk\n",
    "            next_chunk_overlap = chunks[i + 1][: min(overlap_size, len(chunks[i + 1]))]\n",
    "\n",
    "            # Calculate token count with overlap\n",
    "            current_with_overlap = chunks[i] + next_chunk_overlap\n",
    "            total_tokens_with_overlap = sum(\n",
    "                get_token_count(s) for s in current_with_overlap\n",
    "            )\n",
    "\n",
    "            # Check if adding overlap exceeds token limit\n",
    "            if total_tokens_with_overlap <= max_tokens_per_chunk:\n",
    "                overlapping_chunks.append(current_with_overlap)\n",
    "            else:\n",
    "                # If too large, don't add overlap or add partial overlap if possible\n",
    "                partial_overlap = []\n",
    "                remaining_tokens = max_tokens_per_chunk - sum(\n",
    "                    get_token_count(s) for s in chunks[i]\n",
    "                )\n",
    "\n",
    "                for overlap_sentence in next_chunk_overlap:\n",
    "                    overlap_tokens = get_token_count(overlap_sentence)\n",
    "                    if remaining_tokens >= overlap_tokens:\n",
    "                        partial_overlap.append(overlap_sentence)\n",
    "                        remaining_tokens -= overlap_tokens\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                if partial_overlap:\n",
    "                    overlapping_chunks.append(chunks[i] + partial_overlap)\n",
    "                else:\n",
    "                    overlapping_chunks.append(chunks[i])\n",
    "        else:\n",
    "            # Last chunk, no overlap to add\n",
    "            overlapping_chunks.append(chunks[i])\n",
    "\n",
    "    return overlapping_chunks\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Example document (list of sentences)\n",
    "    sample_sentences = [\n",
    "        \"This is sentence one with several tokens.\",\n",
    "        \"Here is another sentence that is quite long and has many tokens for demonstration.\",\n",
    "        \"This is a short one.\",\n",
    "        \"The algorithm needs to handle varying sentence lengths effectively.\",\n",
    "        \"Some sentences might have a lot of tokens and be very informative requiring significant processing.\",\n",
    "        \"Others might be brief.\",\n",
    "        \"We need to ensure that no chunk exceeds the token limit.\",\n",
    "        \"At the same time, we want to maintain context between chunks using overlaps.\",\n",
    "        \"Overlapping sentences help maintain context between chunks.\",\n",
    "        \"This is the last sentence of our example.\",\n",
    "    ]\n",
    "\n",
    "    # Example constraint\n",
    "    MAX_TOKENS = 77  # Max tokens per chunk\n",
    "\n",
    "    # Get chunks\n",
    "    chunks = chunk_document(sentences=sample_sentences, max_tokens_per_chunk=MAX_TOKENS)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Created {len(chunks)} chunks:\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        token_count = sum(len(s.split()) for s in chunk)\n",
    "        print(f\"Chunk {i + 1}: {len(chunk)} sentences, {token_count} tokens\")\n",
    "        print(\"  \" + \"\\n  \".join(chunk))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Some sentences exceed the token limit (33 > 20)\n",
      "These sentences will be placed in their own chunks, exceeding the token limit.\n",
      "Created 3 chunks:\n",
      "Chunk 1: 3 sentences, 16 tokens\n",
      "  My name is Jon Doe. \n",
      "  I live in SF. \n",
      "  I have a 1 year old daughter. \n",
      "\n",
      "Chunk 2: 1 sentences, 33 tokens\n",
      "  Thanks for letting me know that uv add pip fixed the issue! That makes sense. It indicates you were likely using uv (a fast Python package installer and resolver) to manage your environment. \n",
      "\n",
      "Chunk 3: 2 sentences, 9 tokens\n",
      "  God is love. \n",
      "  G.O.A.T. That guy is a legend!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_sentences: list[str] = [\n",
    "    \"My name is Jon Doe. \",\n",
    "    \"I live in SF. \",\n",
    "    \"I have a 1 year old daughter. \",\n",
    "    \"Thanks for letting me know that uv add pip fixed the issue! That makes sense. It \"\n",
    "    \"indicates you were likely using uv (a fast Python package installer and resolver) \"\n",
    "    \"to manage your environment. \",\n",
    "    \"God is love. \",\n",
    "    \"G.O.A.T. That guy is a legend!\",\n",
    "]\n",
    "\n",
    "# Example constraints\n",
    "MAX_TOKENS = 20  # Max tokens per chunk\n",
    "\n",
    "# Get chunks\n",
    "chunks = chunk_document(\n",
    "    sentences=sample_sentences,\n",
    "    max_tokens_per_chunk=MAX_TOKENS,\n",
    "    overlap_size=3,\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(f\"Created {len(chunks)} chunks:\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    token_count = sum(len(s.split()) for s in chunk)\n",
    "    print(f\"Chunk {i + 1}: {len(chunk)} sentences, {token_count} tokens\")\n",
    "    print(\"  \" + \"\\n  \".join(chunk))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, SystemMessage\n",
    "from langchain_litellm import ChatLiteLLM\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c595484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anyio\n",
    "\n",
    "\n",
    "async def aencode_image_to_base64(image_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Asynchronously encode an image file to base64 string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_path : str\n",
    "        Path to the image file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Base64 encoded string of the image.\n",
    "    \"\"\"\n",
    "    image_path: Path = Path(image_path)\n",
    "    async with await anyio.open_file(image_path, \"rb\") as f:\n",
    "        image_data: bytes = await f.read()\n",
    "        image_base64: str = base64.b64encode(image_data).decode(\"utf-8\")\n",
    "        return image_base64\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    \"\"\"State class to store input file and messages.\"\"\"\n",
    "\n",
    "    input_file: str | None\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# TOOLS\n",
    "# ==============================================\n",
    "\n",
    "\n",
    "async def extract_text(image_path: str) -> str:\n",
    "    \"\"\"Extract text from an image file using a Vision Language Model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_path : str\n",
    "        Path to the image file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Extracted text from the image, empty string if error occurs.\n",
    "\n",
    "    Global Variables\n",
    "    ---------------\n",
    "    vision_llm : Any\n",
    "        Vision language model instance.\n",
    "    \"\"\"\n",
    "    global vision_llm\n",
    "\n",
    "    all_text: str = \"\"\n",
    "    try:\n",
    "        image_base64 = await aencode_image_to_base64(image_path)\n",
    "\n",
    "        # Prepare the prompt with the base64 image data\n",
    "        message = [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"Extract all the text from this image. Return only \"\n",
    "                        \"the extracted text with no explanations.\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/png;base64,{image_base64}\",\n",
    "                        },\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "        # Call the VLM to extract text\n",
    "        response = await vision_llm.ainvoke(message)\n",
    "        all_text += response.content + \"\\n\\n\"\n",
    "\n",
    "        return all_text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from image {image_path}: {e}\")\n",
    "        return \"\"  # Return empty string on error\n",
    "\n",
    "\n",
    "def divide(a: float, b: float) -> float:\n",
    "    \"\"\"Divide two numbers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a : float\n",
    "        Numerator.\n",
    "    b : float\n",
    "        Denominator.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Result of division rounded to 4 decimal places.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If denominator is zero.\n",
    "    \"\"\"\n",
    "    if b == 0:\n",
    "        raise ValueError(\"Cannot divide by zero.\")\n",
    "    return round((a / b), 4)\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# NODES\n",
    "# ==============================================\n",
    "async def assistant(state: State) -> dict[str, Any]:\n",
    "    \"\"\"Process state and generate assistant response using LLM.\n",
    "\n",
    "    This function takes a state object containing messages and input file,\n",
    "    generates a system message with tool descriptions, and invokes the LLM\n",
    "    to get a response.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : State\n",
    "        State object containing:\n",
    "            messages : list[Message]\n",
    "                List of conversation messages\n",
    "            input_file : str\n",
    "                Path to input image file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, Any]\n",
    "        Dictionary containing:\n",
    "            messages : list[Message]\n",
    "                List with the LLM response message\n",
    "            input_file : str\n",
    "                Path to input image file\n",
    "\n",
    "    Global Variables\n",
    "    ---------------\n",
    "    llm_with_tools : Any\n",
    "        LLM instance configured with tools\n",
    "    \"\"\"\n",
    "    global llm_with_tools\n",
    "\n",
    "    textual_description_of_tool: str = \"\"\"\n",
    "    extract_text(image_path: str) -> str\n",
    "        Extract text from an image file using a Vision Language Model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        image_path : str\n",
    "            Path to the image file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Extracted text from the image, empty string if error occurs.\n",
    "\n",
    "        Global Variables\n",
    "        ---------------\n",
    "        vision_llm : Any\n",
    "            Vision language model instance.\n",
    "    divide(a: float, b: float) -> float\n",
    "        Divide two numbers.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        a : float\n",
    "            Numerator.\n",
    "        b : float\n",
    "            Denominator.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Result of division rounded to 4 decimal places.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If denominator is zero.\n",
    "    \"\"\"\n",
    "    image: str = state[\"input_file\"]\n",
    "    sys_msg: SystemMessage = SystemMessage(\n",
    "        content=f\"You're a helpful butler named Alfred that serves Mr. Neidu \"\n",
    "        f\"and Batman. You can analyze documents and run computations with \"\n",
    "        f\"provided tools: \\n{textual_description_of_tool} You have access \"\n",
    "        f\"to some optional images. Currently, the loaded image is: {image}\"\n",
    "    )\n",
    "    response: Any = await llm_with_tools.ainvoke([sys_msg] + state[\"messages\"])\n",
    "    return {\"messages\": [response], \"input_file\": image}\n",
    "\n",
    "\n",
    "tools = [extract_text, divide]\n",
    "tool_node = ToolNode(tools=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98412970",
   "metadata": {},
   "source": [
    "### The ReAct Pattern\n",
    "\n",
    "- Re: React to the user's input.\n",
    "- Act: Act on the user's input by using the appropriate tools.\n",
    "- Observe the results of the tools and provide feedback to the user.\n",
    "- Reapeat as necessary until the user is satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d098bdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM and VLM\n",
    "llm = init_chat_model(model=\"mistralai:mistral-large-latest\", temperature=0.3)\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "vision_llm = init_chat_model(\n",
    "    model=\"mistralai:pixtral-large-latest\",\n",
    "    temperature=0.3,\n",
    ")\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node(\"assistant\", assistant)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Connect nodes\n",
    "graph_builder.add_edge(START, \"assistant\")\n",
    "graph_builder.add_conditional_edges(\"assistant\", tools_condition)\n",
    "graph_builder.add_edge(\"tools\", \"assistant\")\n",
    "graph_builder.add_edge(\"assistant\", END)\n",
    "\n",
    "# Compile the graph\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3289595b",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1905e6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the graph\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605cc193",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Example 1: Simple Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "818caf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "@simple_retry(attempts=3, delay=1, timeout=60)\n",
    "async def unstable_function() -> str:\n",
    "    \"\"\"An example unstable function that may fail.\"\"\"\n",
    "    import random\n",
    "\n",
    "    if random.random() < 0.9:  # 50% chance to raise an error\n",
    "        raise ValueError(\"Simulated failure\")\n",
    "    return \"Function executed successfully\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dee791ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1...\n",
      "Completed attempt 1\n",
      "Attempt 2...\n",
      "Completed attempt 2\n",
      "Attempt 3...\n",
      "Completed attempt 3\n",
      "Failed after 3 attempts: Simulated failure\n"
     ]
    }
   ],
   "source": [
    "await unstable_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba88d59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@simple_retry(attempts=3, delay=1, timeout=60)\n",
    "async def run_graph(messages: list[Any], input_file: str | None) -> None:\n",
    "    \"\"\"Run the graph with the initial messages.\"\"\"\n",
    "    messages = await graph.ainvoke({\"messages\": messages, \"input_file\": input_file})\n",
    "\n",
    "    # Show the messages\n",
    "    for message in messages[\"messages\"]:\n",
    "        # console.log(message)\n",
    "        message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d2f9b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1...\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Divide 6790 by 5\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  divide (5E9pCzuOj)\n",
      " Call ID: 5E9pCzuOj\n",
      "  Args:\n",
      "    a: 6790\n",
      "    b: 5\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: divide\n",
      "\n",
      "1358.0\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The result of dividing 6790 by 5 is 1358.0.\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Divide 6790 by 5\")]\n",
    "input_file = None  # No input file for this example\n",
    "\n",
    "await run_graph(messages=messages, input_file=input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "780ae25b",
   "metadata": {},
   "source": [
    "### Example 2: Analyzing Training Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0979d3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1...\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "According tho the note provided by Mr. Wayne in the provided images, what is the list of items I should buy for the dinner menu?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  extract_text (08Dj5DwgH)\n",
      " Call ID: 08Dj5DwgH\n",
      "  Args:\n",
      "    image_path: ../../data/Batman_training_and_meals.png\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: extract_text\n",
      "\n",
      "### TRAINING SCHEDULE\n",
      "\n",
      "For the week of 2/20-2/26\n",
      "\n",
      "**SUNDAY 2/20**\n",
      "\n",
      "**MORNING**\n",
      "30 minute jog\n",
      "30 minute meditation\n",
      "\n",
      "**EVENING**\n",
      "clean and jerk (lite)—3 rep/8 sets, 262 lbs.\n",
      "5 sets metabolic conditioning\n",
      "21 kettlebell swings\n",
      "12 pull-ups\n",
      "30 minutes flexibility\n",
      "30 minutes sparring\n",
      "\n",
      "**MONDAY 2/21**\n",
      "\n",
      "**MORNING**\n",
      "30 minute jog\n",
      "30 minutes traditional kata (focus on Japanese forms)\n",
      "\n",
      "**EVENING**\n",
      "5 sets 20 ft rope climb\n",
      "30 minutes gymnastic rings (work on muscle ups in high bar)\n",
      "jumping—12 rep/8 sets, 525 lbs.\n",
      "30 minutes heavy bag\n",
      "30 minutes target practice\n",
      "\n",
      "**TUESDAY 2/22**\n",
      "\n",
      "**MORNING**\n",
      "30 minute jog\n",
      "30 minutes rope\n",
      "\n",
      "**EVENING**\n",
      "milk run\n",
      "heavy dead lift—5 rep/7 sets, 620 lbs.\n",
      "30 minutes sparring\n",
      "\n",
      "**WEDNESDAY 2/23**\n",
      "\n",
      "**OFF DAY**\n",
      "\n",
      "**MORNING**\n",
      "30 minute run (last week's time was 4'52 per mile. Need to better that time by a half a minute.)\n",
      "\n",
      "**EVENING**\n",
      "all running only\n",
      "30 minutes target practice\n",
      "30 minutes rope\n",
      "30 minutes upper body basics\n",
      "30 minutes observation\n",
      "30 minutes meditation\n",
      "30 minutes holds and pressure points\n",
      "\n",
      "**THURSDAY 2/24**\n",
      "\n",
      "**MORNING**\n",
      "30 minute jog\n",
      "30 minutes traditional kata (focus on Chinese forms)\n",
      "\n",
      "**EVENING**\n",
      "squats—5 rep/10 sets, 525 lbs.\n",
      "30 minutes flexibility\n",
      "crunches—50 rep/5 sets\n",
      "\n",
      "**FRIDAY 2/25**\n",
      "\n",
      "**MORNING**\n",
      "30 minute jog\n",
      "30 minute meditation\n",
      "\n",
      "**EVENING**\n",
      "clean and jerk (lite)—3 rep/8 sets, 262 lbs.\n",
      "5 sets metabolic conditioning\n",
      "21 kettlebell swings\n",
      "12 pull-ups\n",
      "30 minutes flexibility\n",
      "30 minutes sparring\n",
      "\n",
      "**SATURDAY 2/26**\n",
      "\n",
      "**MORNING**\n",
      "30 minute jog\n",
      "30 minutes yoga\n",
      "\n",
      "**EVENING**\n",
      "crunches—50 rep/5 sets\n",
      "squats—5 rep/10 sets, 525 lbs.\n",
      "30 minutes monkey bars\n",
      "30 minutes heavy bag\n",
      "30 minutes target practice\n",
      "\n",
      "---\n",
      "\n",
      "**Tuesday's Menu**\n",
      "\n",
      "**Breakfast**\n",
      "\n",
      "six poached eggs laid over artichoke bottoms with a sage pesto sauce\n",
      "thinly sliced\n",
      "macedonian fresh fruit bowl\n",
      "freshly squeezed orange juice\n",
      "organic, ground-roasted coffee\n",
      "4 grams ground-up rhino horn\n",
      "2 grams fish oil\n",
      "\n",
      "**Lunch**\n",
      "\n",
      "local salmon with a ginger glaze\n",
      "organic asparagus with lemon garlic dusting\n",
      "Asian yam soup with dried onions\n",
      "2 grams fish oil\n",
      "\n",
      "**Dinner**\n",
      "\n",
      "grass-fed local sirloin steak\n",
      "bed of organic spinach and piquillo peppers\n",
      "oven-baked golden beet potato\n",
      "5 grams fish oil\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "### Dinner Menu\n",
      "\n",
      "- Grass-fed local sirloin steak\n",
      "- Bed of organic spinach and piquillo peppers\n",
      "- Oven-baked golden beet potato\n",
      "- 5 grams fish oil\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"According tho the note provided by Mr. Wayne in the \"\n",
    "        \"provided images, what is the list of items I should buy for \"\n",
    "        \"the dinner menu?\"\n",
    "    )\n",
    "]\n",
    "input_file: str = \"../../data/Batman_training_and_meals.png\"\n",
    "\n",
    "await run_graph(messages=messages, input_file=input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316cb462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# - Add more tools for different tasks\n",
    "# - Implement error handling for tool failures\n",
    "# - Add memory to store previous messages and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e455a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
