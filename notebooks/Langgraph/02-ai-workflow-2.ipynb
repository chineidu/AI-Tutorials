{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e958c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Annotated, Any, Iterable, Literal, Optional, Union\n",
    "\n",
    "# Standard imports\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"white\": \"#FFFFFF\",  # Bright white\n",
    "        \"info\": \"#00FF00\",  # Bright green\n",
    "        \"warning\": \"#FFD700\",  # Bright gold\n",
    "        \"error\": \"#FF1493\",  # Deep pink\n",
    "        \"success\": \"#00FFFF\",  # Cyan\n",
    "        \"highlight\": \"#FF4500\",  # Orange-red\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "# import matplotlib.pyplot as pltife\n",
    "\n",
    "# NumPy settings\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "# Polars settings\n",
    "pl.Config.set_fmt_str_lengths(1_000)\n",
    "pl.Config.set_tbl_cols(n=1_000)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_up_from_current_directory(*, go_up: int = 1) -> None:\n",
    "    \"\"\"This is used to up a number of directories.\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "    go_up: int, default=1\n",
    "        This indicates the number of times to go back up from the current directory.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    CONST: str = \"../\"\n",
    "    NUM: str = CONST * go_up\n",
    "\n",
    "    # Goto the previous directory\n",
    "    prev_directory = os.path.join(os.path.dirname(__name__), NUM)\n",
    "    # Get the 'absolute path' of the previous directory\n",
    "    abs_path_prev_directory = os.path.abspath(prev_directory)\n",
    "\n",
    "    # Add the path to the System paths\n",
    "    sys.path.insert(0, abs_path_prev_directory)\n",
    "    print(abs_path_prev_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/neidu/Desktop/Projects/Personal/My_Projects/AI-Tutorials\n"
     ]
    }
   ],
   "source": [
    "go_up_from_current_directory(go_up=2)\n",
    "\n",
    "\n",
    "from schemas import ModelEnum  # noqa: E402\n",
    "from settings import refresh_settings  # noqa: E402\n",
    "from utilities.client_utils import check_rate_limit  # noqa: E402\n",
    "\n",
    "settings = refresh_settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ab23b0",
   "metadata": {},
   "source": [
    "## LiteLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de3d9cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[19:27:13] </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ModelResponse</span><span style=\"font-weight: bold\">(</span>                                                                          <a href=\"file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_64004/3909896267.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3909896267.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_64004/3909896267.py#30\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">30</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gen-1747592832-cQ7u7y0BOmh3uxY3KCpJ'</span>,                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>    <span style=\"color: #808000; text-decoration-color: #808000\">created</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1747592832</span>,                                                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'meta-llama/llama-3.1-8b-instruct'</span>,                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>    <span style=\"color: #808000; text-decoration-color: #808000\">object</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chat.completion'</span>,                                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>    <span style=\"color: #808000; text-decoration-color: #808000\">system_fingerprint</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,                                                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>    <span style=\"color: #808000; text-decoration-color: #808000\">choices</span>=<span style=\"font-weight: bold\">[</span>                                                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Choices</span><span style=\"font-weight: bold\">(</span>                                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>            <span style=\"color: #808000; text-decoration-color: #808000\">finish_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,                                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>            <span style=\"color: #808000; text-decoration-color: #808000\">index</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,                                                                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>            <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Message</span><span style=\"font-weight: bold\">(</span>                                                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'{  \"content\": \"pours.\" }'</span>,                                     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,                                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                <span style=\"color: #808000; text-decoration-color: #808000\">function_call</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,                                                     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                <span style=\"color: #808000; text-decoration-color: #808000\">provider_specific_fields</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'refusal'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'reasoning'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">}</span>           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>            <span style=\"font-weight: bold\">)</span>                                                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>        <span style=\"font-weight: bold\">)</span>                                                                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>    <span style=\"font-weight: bold\">]</span>,                                                                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>    <span style=\"color: #808000; text-decoration-color: #808000\">usage</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Usage</span><span style=\"font-weight: bold\">(</span>                                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>        <span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>,                                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>        <span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">67</span>,                                                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>        <span style=\"color: #808000; text-decoration-color: #808000\">total_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">79</span>,                                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>        <span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens_details</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,                                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>        <span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens_details</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>    <span style=\"font-weight: bold\">)</span>,                                                                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>    <span style=\"color: #808000; text-decoration-color: #808000\">provider</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Friendli'</span>                                                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">)</span>                                                                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[19:27:13]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;35mModelResponse\u001b[0m\u001b[1m(\u001b[0m                                                                          \u001b]8;id=75801;file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_64004/3909896267.py\u001b\\\u001b[2m3909896267.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=298698;file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_64004/3909896267.py#30\u001b\\\u001b[2m30\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m    \u001b[33mid\u001b[0m=\u001b[32m'gen-1747592832-cQ7u7y0BOmh3uxY3KCpJ'\u001b[0m,                                           \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m    \u001b[33mcreated\u001b[0m=\u001b[1;36m1747592832\u001b[0m,                                                                 \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m    \u001b[33mmodel\u001b[0m=\u001b[32m'meta-llama/llama-3.1-8b-instruct'\u001b[0m,                                           \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m    \u001b[33mobject\u001b[0m=\u001b[32m'chat.completion'\u001b[0m,                                                           \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m    \u001b[33msystem_fingerprint\u001b[0m=\u001b[3;35mNone\u001b[0m,                                                            \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m    \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m                                                                           \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m        \u001b[1;35mChoices\u001b[0m\u001b[1m(\u001b[0m                                                                        \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m            \u001b[33mfinish_reason\u001b[0m=\u001b[32m'stop'\u001b[0m,                                                       \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m            \u001b[33mindex\u001b[0m=\u001b[1;36m0\u001b[0m,                                                                    \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m            \u001b[33mmessage\u001b[0m=\u001b[1;35mMessage\u001b[0m\u001b[1m(\u001b[0m                                                            \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                \u001b[33mcontent\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m  \"content\": \"pours.\" \u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,                                     \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,                                                       \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                \u001b[33mtool_calls\u001b[0m=\u001b[3;35mNone\u001b[0m,                                                        \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                \u001b[33mfunction_call\u001b[0m=\u001b[3;35mNone\u001b[0m,                                                     \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                \u001b[33mprovider_specific_fields\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'refusal'\u001b[0m: \u001b[3;35mNone\u001b[0m, \u001b[32m'reasoning'\u001b[0m: \u001b[3;35mNone\u001b[0m\u001b[1m}\u001b[0m           \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m            \u001b[1m)\u001b[0m                                                                           \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m        \u001b[1m)\u001b[0m                                                                               \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m    \u001b[1m]\u001b[0m,                                                                                  \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m    \u001b[33musage\u001b[0m=\u001b[1;35mUsage\u001b[0m\u001b[1m(\u001b[0m                                                                        \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m        \u001b[33mcompletion_tokens\u001b[0m=\u001b[1;36m12\u001b[0m,                                                           \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m        \u001b[33mprompt_tokens\u001b[0m=\u001b[1;36m67\u001b[0m,                                                               \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m        \u001b[33mtotal_tokens\u001b[0m=\u001b[1;36m79\u001b[0m,                                                                \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m        \u001b[33mcompletion_tokens_details\u001b[0m=\u001b[3;35mNone\u001b[0m,                                                 \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m        \u001b[33mprompt_tokens_details\u001b[0m=\u001b[3;35mNone\u001b[0m                                                      \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m    \u001b[1m)\u001b[0m,                                                                                  \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m    \u001b[33mprovider\u001b[0m=\u001b[32m'Friendli'\u001b[0m                                                                 \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1m)\u001b[0m                                                                                       \u001b[2m                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from litellm import acompletion\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Response(BaseModel):\n",
    "    content: str = Field(description=\"The content of the response.\")\n",
    "\n",
    "\n",
    "messages: list[dict[str, Any]] = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"<inst>/no_think You're an expert AI assstance that replies to \"\n",
    "        \"questions in a very polite and concise manner.</inst>\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"When it rains it ....\",\n",
    "    },\n",
    "]\n",
    "response = await acompletion(\n",
    "    model=f\"openrouter/{ModelEnum.BASE_REMOTE_MODEL_1_8B.value}\",\n",
    "    messages=messages,\n",
    "    max_tokens=700,\n",
    "    max_retries=5,\n",
    "    temperature=0.0,\n",
    "    seed=0,\n",
    "    response_format=Response,\n",
    ")\n",
    "\n",
    "console.log(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2732d6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"data\": {\n",
      "    \"label\": \"sk-or-v1-902...c45\",\n",
      "    \"limit\": 2,\n",
      "    \"usage\": 0.1977190685,\n",
      "    \"is_provisioning_key\": false,\n",
      "    \"limit_remaining\": 1.8022809315,\n",
      "    \"is_free_tier\": false,\n",
      "    \"rate_limit\": {\n",
      "      \"requests\": 20,\n",
      "      \"interval\": \"10s\"\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "check_rate_limit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11702ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from outlines import models\n",
    "\n",
    "model = models.openai(\n",
    "    # ModelEnum.BASE_REMOTE_MODEL_1_8B,\n",
    "    ModelEnum.BASE_MODEL_LOCAL_1,\n",
    "    api_key=settings.OPENROUTER_API_KEY.get_secret_value(),\n",
    "    # base_url=settings.OPENROUTER_URL,\n",
    "    base_url=settings.OLLAMA_URL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdbacd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chicken\n"
     ]
    }
   ],
   "source": [
    "import outlines.models as models\n",
    "from outlines import generate\n",
    "from pydantic import BaseModel, ConfigDict\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    model_config = ConfigDict(extra=\"forbid\")  # required for openai\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "    age: int\n",
    "\n",
    "\n",
    "generator = generate.json(model, Person)\n",
    "generator(\"current indian prime minister on january 1st 2023\")\n",
    "# Person(first_name='Narendra', last_name='Modi', age=72)\n",
    "\n",
    "generator = generate.choice(model, [\"Chicken\", \"Egg\"])\n",
    "print(generator(\"Which came first?\"))\n",
    "# Chicken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dfddbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93992b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person(BaseModel):\n",
    "    \"\"\"A schema for a person.\"\"\"\n",
    "\n",
    "    name: str = Field(description=\"The name of the person.\")\n",
    "    age: int = Field(description=\"The age of the person.\", ge=5, le=100)\n",
    "\n",
    "\n",
    "class Persons(BaseModel):\n",
    "    persons: list[Person] = Field(description=\"A list of persons.\", alias=\"engineers\")\n",
    "\n",
    "\n",
    "messages: list[dict[str, Any]] = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"<inst>/no_think You're an expert AI assstance that replies to \"\n",
    "        \"questions in a very polite and concise manner. When you respond, reply \"\n",
    "        \"with a cleanly formatted JSON without including backticks.</inst>\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"<user>There are two engineers working at Fred AI. Kunle, 28 years old is \"\n",
    "        \"a Python developer while Francis is a Golang guru at a ripe age of 32. Extract \"\n",
    "        \"their information in a JSON format.</user>\",\n",
    "    },\n",
    "]\n",
    "\n",
    "raw_response = await acompletion(\n",
    "    model=f\"openrouter/{ModelEnum.GEMMA_3p0_12B_REMOTE_FREE.value}\",\n",
    "    messages=messages,\n",
    "    max_tokens=700,\n",
    "    max_retries=5,\n",
    "    temperature=0.0,\n",
    "    seed=0,\n",
    "    response_format=Persons,\n",
    ")\n",
    "\n",
    "console.log(raw_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4371e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = raw_response.choices[0].message.content\n",
    "    console.log(Persons.model_validate_json(response).model_dump())\n",
    "except Exception as e:\n",
    "    console.log(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c48f97",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "- The `response_format` helps to validate the response.\n",
    "- It not guaranteed to always work.\n",
    "- Another alternative is to use the `Instructor` library to validate the response.\n",
    "\n",
    "<hr><br>\n",
    "\n",
    "## Instructor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f7c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "\n",
    "aclient = instructor.from_litellm(acompletion, mode=instructor.Mode.JSON)\n",
    "\n",
    "response, raw_response = await aclient.chat.completions.create_with_completion(\n",
    "    response_model=Persons,\n",
    "    model=f\"openrouter/{ModelEnum.GEMMA_3p0_12B_REMOTE_FREE.value}\",\n",
    "    messages=messages,\n",
    "    max_tokens=700,\n",
    "    max_retries=5,\n",
    "    temperature=0.0,\n",
    "    seed=0,\n",
    ")\n",
    "console.log(raw_response)\n",
    "\n",
    "print(\"Validated response: \")\n",
    "console.log(response.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9e5264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Iterable does NOT work with create_with_completion\n",
    "response = await aclient.chat.completions.create(\n",
    "    response_model=Iterable[Person],\n",
    "    model=f\"openrouter/{ModelEnum.GEMMA_3p0_12B_REMOTE.value}\",\n",
    "    messages=messages,\n",
    "    max_tokens=700,\n",
    "    max_retries=5,\n",
    "    temperature=0.0,\n",
    "    seed=0,\n",
    ")\n",
    "result = [person.model_dump() async for person in response]\n",
    "console.log(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a44022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9a009d7",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "# Add Memory To LangGraph Workflow\n",
    "\n",
    "- the previous chatbot can use tools but can't remember the previous conversation.\n",
    "- LangGraph solves this by using `persistent checkpointing`.\n",
    "- This can be achieved by providing a `checkpointer` when compiling the graph and a `thread_id` when running the graph.\n",
    "- LangGraph automatically saves the state after each step and when the graph is invoked later using the same `thread_id`, the graph loads up its saved state.\n",
    "- LangGraph claims `checkpoint` is more powerful than using a `simple chat memory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb368496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# A simple memory saver for this tutorial. In production,\n",
    "# it's recommennded to use SqliteSaver or PostgresSaver\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a23acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_tavily import TavilySearch\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    # This appends messages instead of overwriting\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "llm = init_chat_model(\"mistralai:ministral-8b-latest\")\n",
    "tavily_search = TavilySearch(max_results=2)\n",
    "tools = [tavily_search]\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c6b19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = init_chat_model(\"mistralai:ministral-8b-latest\")\n",
    "response = await llm.ainvoke(\"Sup ma nigga?\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee2d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chatbot(state: State) -> dict[str, Any]:\n",
    "    \"\"\"Process chat messages and generate a response using LLM with tools.\n",
    "\n",
    "    Parameters:\n",
    "        state : State\n",
    "            The current state object containing chat messages.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, Any]\n",
    "            A dictionary containing the 'messages' key with a list of the LLM response.\n",
    "\n",
    "    Notes:\n",
    "        The function uses an asynchronous LLM invocation to process the messages\n",
    "        and returns a single response wrapped in a list.\n",
    "    \"\"\"\n",
    "    response = await llm_with_tools.ainvoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Init the state graph\n",
    "graph_builder: StateGraph = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cd310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the nodes\n",
    "tool_node = ToolNode(tools=tools)\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Connect the nodes\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_conditional_edges(\"chatbot\", tools_condition)\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4721f3cc",
   "metadata": {},
   "source": [
    "### Compile The Graph\n",
    "\n",
    "- Add the memory saver.\n",
    "- Add [LangFuse](https://github.com/langfuse/langfuse) callback handler for observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d7459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.callback import CallbackHandler\n",
    "\n",
    "# langfuse_handler = CallbackHandler()\n",
    "\n",
    "# Create a callback handler with a session and user id\n",
    "langfuse_handler = CallbackHandler(\n",
    "    session_id=\"chatbot_with_tools\",\n",
    "    user_id=\"user_123\",\n",
    ")\n",
    "\n",
    "# Compile the graph\n",
    "graph = graph_builder.compile(checkpointer=memory).with_config({\"callbacks\": [langfuse_handler]})\n",
    "\n",
    "\n",
    "# Visualize the graph\n",
    "from IPython.display import Image, display  # noqa: E402\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "# or display(Image(graph.get_graph().draw_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550f4f20",
   "metadata": {},
   "source": [
    "### Interact With Chatbot\n",
    "\n",
    "- Add a `thread` to use the graph.\n",
    "- Add [LangFuse](https://github.com/langfuse/langfuse) callback handler for observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e81805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the langfuse handler using this approach didn't work for some reason\n",
    "config = {\n",
    "    \"configurable\": {\"thread_id\": \"1\"},\n",
    "    # \"callback\": [langfuse_handler],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42985204",
   "metadata": {},
   "source": [
    "### Call The Chatbot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cda65ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input: str = \"'Sup! I'm Neidu.\"\n",
    "\n",
    "# NB: config variable is the 2nd positional argument\n",
    "events = graph.astream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "    config=config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "\n",
    "async for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a409c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a follow up question\n",
    "user_input: str = \"Yo! You remember my name?\"\n",
    "\n",
    "# NB: config variable is the 2nd positional argument\n",
    "events = graph.astream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "    config=config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "\n",
    "async for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62f42a2",
   "metadata": {},
   "source": [
    "#### Verify The Persistence\n",
    "\n",
    "- Change the thread id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30b17d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a follow up question\n",
    "user_input: str = \"Yo! You remember my name?\"\n",
    "\n",
    "# NB: config variable is the 2nd positional argument\n",
    "events = graph.astream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "    config={\"configurable\": {\"thread_id\": \"2\"}},\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "\n",
    "async for event in events:\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4f8596",
   "metadata": {},
   "source": [
    "### Inspect The State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b9296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = graph.get_state(config)\n",
    "console.log(snapshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a9af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot.next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f67c0dd",
   "metadata": {},
   "source": [
    "## Add Human-in-the-loop Controls\n",
    "- LangGraph's persistence layer supports human-in-the-loop workflows to handle unreliable agents needing human input or approval. \n",
    "- The `interrupt` function pauses execution for user feedback, which is then provided via a Command to resume, similar to Python's `input()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028c0c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langgraph.types import Command, interrupt\n",
    "\n",
    "\n",
    "@tool\n",
    "def human_assistance(query: str) -> str:\n",
    "    \"\"\"Request human assistance for a given query.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    query : str\n",
    "        The question or request to be handled by a human.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The response data provided by the human assistant.\n",
    "    \"\"\"\n",
    "    human_assistance = interrupt({\"query\": query})\n",
    "    return human_assistance[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50904cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [tavily_search, human_assistance]\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c549007",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await llm_with_tools.ainvoke(\"Who won the FA Cup final match today?\")\n",
    "console.log(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chatbot(state: State) -> dict[str, Any]:\n",
    "    \"\"\"Process chat messages through LLM with tools and return response.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : State\n",
    "        Current state containing message history.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, Any]\n",
    "        Dictionary containing LLM response message.\n",
    "        Contains key 'messages' with list of one message.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Disables parallel tool calling to prevent duplicate tool invocations\n",
    "    when restarting the graph flow. Asserts at most one tool call per message.\n",
    "    \"\"\"\n",
    "    message = await llm_with_tools.ainvoke(state[\"messages\"])\n",
    "    # Disable parallel tool calling because we'll be interrupting (human-in-the-loop)\n",
    "    # to prevent repeating any tool invocations when we restart the graph\n",
    "    assert len(message.tool_calls) <= 1\n",
    "    return {\"messages\": [message]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f2ace6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd3ece9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6061b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bfe4d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
