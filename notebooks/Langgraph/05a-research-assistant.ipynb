{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc0ce639",
   "metadata": {},
   "source": [
    "# Research Assistant Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b2aa0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import (\n",
    "    Annotated,\n",
    "    Any,\n",
    "    Generator,\n",
    "    Iterable,\n",
    "    Literal,\n",
    "    Optional,\n",
    "    TypedDict,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "# Standard imports\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"white\": \"#FFFFFF\",  # Bright white\n",
    "        \"info\": \"#00FF00\",  # Bright green\n",
    "        \"warning\": \"#FFD700\",  # Bright gold\n",
    "        \"error\": \"#FF1493\",  # Deep pink\n",
    "        \"success\": \"#00FFFF\",  # Cyan\n",
    "        \"highlight\": \"#FF4500\",  # Orange-red\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "# import matplotlib.pyplot as pltife\n",
    "\n",
    "# NumPy settings\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "# Polars settings\n",
    "pl.Config.set_fmt_str_lengths(1_000)\n",
    "pl.Config.set_tbl_cols(n=1_000)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_up_from_current_directory(*, go_up: int = 1) -> None:\n",
    "    \"\"\"This is used to up a number of directories.\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "    go_up: int, default=1\n",
    "        This indicates the number of times to go back up from the current directory.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    CONST: str = \"../\"\n",
    "    NUM: str = CONST * go_up\n",
    "\n",
    "    # Goto the previous directory\n",
    "    prev_directory = os.path.join(os.path.dirname(__name__), NUM)\n",
    "    # Get the 'absolute path' of the previous directory\n",
    "    abs_path_prev_directory = os.path.abspath(prev_directory)\n",
    "\n",
    "    # Add the path to the System paths\n",
    "    sys.path.insert(0, abs_path_prev_directory)\n",
    "    print(abs_path_prev_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d4f1fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/neidu/Desktop/Projects/Personal/My_Projects/AI-Tutorials\n"
     ]
    }
   ],
   "source": [
    "go_up_from_current_directory(go_up=2)\n",
    "\n",
    "\n",
    "from schemas import ModelEnum  # noqa: E402\n",
    "from settings import refresh_settings  # noqa: E402\n",
    "from utilities.client_utils import check_rate_limit, get_aclient  # noqa: E402\n",
    "\n",
    "settings = refresh_settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f1daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, Markdown, display\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, ToolMessage\n",
    "from langchain_core.tools import InjectedToolCallId, tool\n",
    "from langchain_litellm import ChatLiteLLM\n",
    "from langchain_tavily import TavilySearch\n",
    "from langfuse.callback import CallbackHandler\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, MessagesState, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.types import Command, interrupt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637ea699",
   "metadata": {},
   "source": [
    "### Goal\n",
    "\n",
    "- Build a lightweight, multi-agent system using chat models to customize the research process.\n",
    "\n",
    "### Key Themes\n",
    "\n",
    "- Integrates LangGraph themes like memory, human-in-the-loop, and controllability.\n",
    "\n",
    "### Source Selection\n",
    "\n",
    "- Allows users to choose input sources for their research.\n",
    "\n",
    "### Planning\n",
    "\n",
    "- Users provide a topic.\n",
    "- The system generates a team of AI analysts, each focusing on a sub-topic.\n",
    "- Human-in-the-loop will be used to refine sub-topics before research starts.\n",
    "\n",
    "### LLM Utilization (Research Process)\n",
    "\n",
    "- Each analyst conducts in-depth, multi-turn interviews with an expert AI based on selected sources, similar to the STORM paper.\n",
    "- Interviews are captured in sub-graphs with internal state.\n",
    "- Experts gather information in parallel and interviews are conducted simultaneously via map-reduce.\n",
    "\n",
    "### Output Format\n",
    "\n",
    "- Gathered insights from each interview are synthesized into a final report.\n",
    "- Customizable prompts allow for flexible report output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423a4451",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "### Create The LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae4f459",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(\n",
    "    model_provider=\"openai\",\n",
    "    openai_api_key=settings.OPENROUTER_API_KEY.get_secret_value(),\n",
    "    base_url=settings.OPENROUTER_URL,\n",
    "    model=ModelEnum.QWEN_3p0_30B_REMOTE.value,\n",
    "    # model=\"qwen/qwen3-235b-a22b\",\n",
    "    temperature=0.0,\n",
    "    seed=123,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3c6245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Type\n",
    "\n",
    "from instructor.client import AsyncInstructor\n",
    "from pydantic import BaseModel, Field, computed_field\n",
    "\n",
    "\n",
    "class GeneralResponse(BaseModel):\n",
    "    name: str | None = Field(default=None, description=\"The name of the user.\")\n",
    "    content: str\n",
    "\n",
    "\n",
    "class OpenAIMessage(BaseModel):\n",
    "    type: str = Field(\n",
    "        ...,\n",
    "        description=\"The role of the message sender. Common roles are: system, \"\n",
    "        \"user, assistant, tool.\",\n",
    "    )\n",
    "    content: str = Field(\n",
    "        ...,\n",
    "        description=\"The actual text of the message.\",\n",
    "    )\n",
    "    name: str | None = Field(\n",
    "        default=None,\n",
    "        description=\"The name of the user, assistant, or tool that sent the message. \"\n",
    "        \"This is used for context in the model's responses.\",\n",
    "    )\n",
    "    tool_call_id: str | None = Field(\n",
    "        default=None,\n",
    "        description=\"Used in conjunction with tool role messages to identify which tool \"\n",
    "        \"call the result corresponds to.\",\n",
    "    )\n",
    "    tool_calls: list[dict[str, Any]] | None = Field(\n",
    "        default_factory=list,\n",
    "        description=\"When the model decides to call a tool, this contains the details of \"\n",
    "        \"the tool call (e.g., function name and arguments).\",\n",
    "    )\n",
    "\n",
    "    @computed_field\n",
    "    @property\n",
    "    def role(self) -> Literal[\"system\", \"user\", \"assistant\", \"tool\"]:\n",
    "        \"\"\"The role of the message sender.\"\"\"\n",
    "        if self.type == \"system\":\n",
    "            return \"system\"\n",
    "        if self.type == \"ai\":\n",
    "            return \"assistant\"\n",
    "        if self.type == \"tool\":\n",
    "            return \"tool\"\n",
    "        return \"user\"\n",
    "\n",
    "    def model_dump(self, **kwargs) -> dict[str, Any]:\n",
    "        data = super().model_dump(**kwargs)\n",
    "        data.pop(\"type\")\n",
    "        return data\n",
    "\n",
    "\n",
    "class LLMUsage(BaseModel):\n",
    "    input_tokens: int\n",
    "    output_tokens: int\n",
    "    total_tokens: int\n",
    "    input_token_details: dict = Field(default_factory=dict)\n",
    "    output_token_details: dict = Field(default_factory=dict)\n",
    "\n",
    "\n",
    "class TokenUsage(BaseModel):\n",
    "    completion_tokens: int\n",
    "    prompt_tokens: int\n",
    "    total_tokens: int\n",
    "    completion_tokens_details: dict | None = Field(default_factory=None)\n",
    "    prompt_tokens_details: dict | None = Field(default_factory=None)\n",
    "\n",
    "\n",
    "class ResponseMetadata(BaseModel):\n",
    "    token_usage: TokenUsage\n",
    "    model_name: str\n",
    "    system_fingerprint: str | None = Field(default_factory=None)\n",
    "    id: str\n",
    "    service_tier: str | None = Field(default_factory=None)\n",
    "    finish_reason: str\n",
    "    logprobs: dict | None = Field(default_factory=None)\n",
    "\n",
    "\n",
    "class AdditionalKwargs(BaseModel):\n",
    "    parsed: dict | None = Field(default_factory=dict)\n",
    "    refusal: dict | None = Field(default_factory=dict)\n",
    "\n",
    "\n",
    "class UsageMetadata(BaseModel):\n",
    "    response_metadata: ResponseMetadata\n",
    "    usage: LLMUsage\n",
    "    additional_kwargs: AdditionalKwargs\n",
    "    type: str\n",
    "    name: str | None = Field(default_factory=None)\n",
    "    example: bool = Field(default=False)\n",
    "    tool_calls: list[dict[str, Any]] | None = Field(default_factory=list)\n",
    "    invalid_tool_calls: list[dict[str, Any]] | None = Field(default_factory=list)\n",
    "\n",
    "\n",
    "aclient: Any | AsyncInstructor = get_aclient(return_type=\"instructor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dad9504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1fc1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_openai(input_: list[Any]) -> list[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Format a list of messages into OpenAI-compatible message format.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_ : list[Any]\n",
    "        List of input messages to be formatted.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[dict[str, Any]]\n",
    "        List of formatted messages compatible with OpenAI's API format.\n",
    "        Each message is a dictionary containing role and content information.\n",
    "    \"\"\"\n",
    "    return [OpenAIMessage(**message.model_dump()).model_dump() for message in input_]\n",
    "\n",
    "\n",
    "async def allm_with_structured_output(\n",
    "    client: AsyncInstructor,\n",
    "    model: str,\n",
    "    messages: list[dict[str, Any]] | list[Type[BaseModel]],\n",
    "    response_model: Type[BaseModel],\n",
    "    return_ai_message: bool = False,\n",
    ") -> Type[BaseModel]:\n",
    "    \"\"\"Make an async call to LLM with structured output using instructor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    client : AsyncInstructor\n",
    "        The async instructor client instance for making LLM calls\n",
    "    model : str\n",
    "        The model to use\n",
    "    messages : list[dict[str, Any]]\n",
    "        The messages to send to the LLM\n",
    "    response_model : Type[BaseModel]\n",
    "        The Pydantic model class to structure the output\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Type[BaseModel]\n",
    "        The structured response from the LLM\n",
    "    \"\"\"\n",
    "    if response_model is not None and return_ai_message:\n",
    "        raise ValueError(\n",
    "            \"Both response_model and return_ai_message are specified. Please specify only one.\"\n",
    "        )\n",
    "\n",
    "    if isinstance(messages[0], (AIMessage, HumanMessage, SystemMessage, ToolMessage)):\n",
    "        messages = format_for_openai(messages)\n",
    "\n",
    "    if return_ai_message and response_model is None:\n",
    "        response: Type[BaseModel] = await client.chat.completions.create(\n",
    "            response_model=None,\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            max_retries=5,\n",
    "            temperature=0.0,\n",
    "            seed=2,\n",
    "        )\n",
    "        return AIMessage(\n",
    "            content=response.choices[0].message.content, **response.model_dump()\n",
    "        )\n",
    "\n",
    "    return await client.chat.completions.create(\n",
    "        response_model=response_model,\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_retries=5,\n",
    "        temperature=0.0,\n",
    "        seed=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c6b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "\n",
    "\n",
    "aclient = get_aclient(return_type=\"instructor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c989862",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_for_openai([HumanMessage(content=\"Who be Tinubu?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f886ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model: str = f\"openrouter/{ModelEnum.QWEN_3p0_30B_REMOTE.value}\"\n",
    "\n",
    "response: GeneralResponse = await allm_with_structured_output(\n",
    "    client=aclient,\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"</no_think> Who be Tinubu?\"},\n",
    "    ],\n",
    "    response_model=GeneralResponse,\n",
    ")\n",
    "\n",
    "console.log(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46baca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = llm.invoke(\"Who be Tinubu?\")\n",
    "console.log(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb4e887",
   "metadata": {},
   "outputs": [],
   "source": [
    "response: Type[BaseModel] = await allm_with_structured_output(\n",
    "    client=aclient,\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"</no_think> Who be Tinubu?\"},\n",
    "    ],\n",
    "    response_model=None,\n",
    "    return_ai_message=True,\n",
    ")\n",
    "\n",
    "console.log(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149442a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# console.log(AIMessage(content=\"\", **response.model_dump()))\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96989f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMUsage(BaseModel):\n",
    "    input_tokens: int\n",
    "    output_tokens: int\n",
    "    total_tokens: int\n",
    "    input_token_details: dict = Field(default_factory=dict)\n",
    "    output_token_details: dict = Field(default_factory=dict)\n",
    "\n",
    "\n",
    "class TokenUsage(BaseModel):\n",
    "    completion_tokens: int\n",
    "    prompt_tokens: int\n",
    "    total_tokens: int\n",
    "    completion_tokens_details: dict | None = Field(default_factory=None)\n",
    "    prompt_tokens_details: dict | None = Field(default_factory=None)\n",
    "\n",
    "\n",
    "class ResponseMetadata(BaseModel):\n",
    "    token_usage: TokenUsage\n",
    "    model_name: str\n",
    "    system_fingerprint: str | None = Field(default_factory=None)\n",
    "    id: str\n",
    "    service_tier: str | None = Field(default_factory=None)\n",
    "    finish_reason: str\n",
    "    logprobs: dict | None = Field(default_factory=None)\n",
    "\n",
    "\n",
    "class AdditionalKwargs(BaseModel):\n",
    "    parsed: dict | None = Field(default_factory=dict)\n",
    "    refusal: dict | None = Field(default_factory=dict)\n",
    "\n",
    "\n",
    "class UsageMetadata(BaseModel):\n",
    "    response_metadata: ResponseMetadata\n",
    "    usage: LLMUsage\n",
    "    additional_kwargs: AdditionalKwargs\n",
    "    type: str\n",
    "    name: str | None = Field(default_factory=None)\n",
    "    example: bool = Field(default=False)\n",
    "    tool_calls: list[dict[str, Any]] | None = Field(default_factory=list)\n",
    "    invalid_tool_calls: list[dict[str, Any]] | None = Field(default_factory=list)\n",
    "\n",
    "\n",
    "metadata = UsageMetadata(\n",
    "    response_metadata=ResponseMetadata(\n",
    "        token_usage=TokenUsage(\n",
    "            completion_tokens=591,\n",
    "            prompt_tokens=15,\n",
    "            total_tokens=606,\n",
    "            completion_tokens_details=None,\n",
    "            prompt_tokens_details=None,\n",
    "        ),\n",
    "        model_name=\"qwen/qwen3-30b-a3b\",\n",
    "        system_fingerprint=None,\n",
    "        id=\"gen-1749415488-6lkrmVTJJ2Gh8wsOCE5E\",\n",
    "        service_tier=None,\n",
    "        finish_reason=\"stop\",\n",
    "        logprobs=None,\n",
    "    ),\n",
    "    usage=LLMUsage(\n",
    "        input_tokens=15,\n",
    "        output_tokens=591,\n",
    "        total_tokens=606,\n",
    "        input_token_details={},\n",
    "        output_token_details={},\n",
    "    ),\n",
    "    additional_kwargs=AdditionalKwargs(parsed=None, refusal=None),\n",
    "    type=\"ai\",\n",
    "    name=None,\n",
    "    example=False,\n",
    "    tool_calls=[],\n",
    "    invalid_tool_calls=[],\n",
    ")\n",
    "console.log(\n",
    "    AIMessage(\n",
    "        content=\"thank you\",\n",
    "        role=\"assistant\",\n",
    "        **metadata.model_dump(),\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# metadata.usage.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ed8516",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_metadata = {\n",
    "    \"token_usage\": {\n",
    "        \"completion_tokens\": 591,\n",
    "        \"prompt_tokens\": 15,\n",
    "        \"total_tokens\": 606,\n",
    "        \"completion_tokens_details\": None,\n",
    "        \"prompt_tokens_details\": None,\n",
    "    },\n",
    "    \"model_name\": \"qwen/qwen3-30b-a3b\",\n",
    "    \"system_fingerprint\": None,\n",
    "    \"id\": \"gen-1749415488-6lkrmVTJJ2Gh8wsOCE5E\",\n",
    "    \"service_tier\": None,\n",
    "    \"finish_reason\": \"stop\",\n",
    "    \"logprobs\": None,\n",
    "}\n",
    "\n",
    "additional_kwargs = {\"parsed\": None, \"refusal\": None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4d821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response: AIMessage = await allm_with_structured_output(\n",
    "    client=aclient,\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"</no_think> Who be Tinubu?\"},\n",
    "    ],\n",
    "    response_model=AIMessage,\n",
    ")\n",
    "\n",
    "console.log(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312590e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5aa09db5",
   "metadata": {},
   "source": [
    "## Create Analysts: Human-in-the-loop\n",
    "\n",
    "- Create analysts and review them using human-in-the-loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1250a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Analyst(BaseModel):\n",
    "    affiliation: str = Field(description=\"The primary affiliation of the analyst.\")\n",
    "    role: str = Field(description=\"The role of the analyst.\")\n",
    "    name: str = Field(description=\"The name of the analyst.\")\n",
    "    description: str = Field(\n",
    "        description=\"The description of the analyst's focus, concerns and motives.\"\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def persona(self) -> str:\n",
    "        return (\n",
    "            f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\"\n",
    "            f\"\\nDescription: {self.description}\"\n",
    "        )\n",
    "\n",
    "\n",
    "class Perpectives(BaseModel):\n",
    "    analysts: list[Analyst] = Field(\n",
    "        description=\"A list of analysts with their detailed information.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class GenerateAnalystState(TypedDict):\n",
    "    topic: str  # Research topic\n",
    "    max_analyst: int  # Maximum number of analysts\n",
    "    human_analyst_feedback: str  # Human feedback\n",
    "    analysts: list[Analyst]  # Analyst asking questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d191ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst_instructions: str = \"\"\"\n",
    "<instruction>\n",
    "You're tasked with creating a set of AI analyst personas. Follow these instructions carefully:\n",
    "1. First, review the reseacrch topic: \\n{topic}\n",
    "2. Examine any editorial feedback that has been optionally provided to guide the creation of the analysts:\n",
    "{human_analyst_feedback}\n",
    "3. Determine the most interesting themes based upon documents and/or feedback above.\n",
    "4. Pick the top {max_analyst} themes.\n",
    "5. Assign one analyst to each theme.\n",
    "</instruction>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "async def create_analysts(state: GenerateAnalystState) -> dict[str, Any]:\n",
    "    topic: str = state[\"topic\"]\n",
    "    max_analyst: int = state[\"max_analyst\"]\n",
    "    human_analyst_feedback: str = state.get(\"human_analyst_feedback\", \"\")\n",
    "\n",
    "    system_message = analyst_instructions.format(\n",
    "        topic=topic,\n",
    "        human_analyst_feedback=human_analyst_feedback,\n",
    "        max_analyst=max_analyst,\n",
    "    )\n",
    "    analysts: Perpectives = await allm_with_structured_output(\n",
    "        client=aclient,\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": \"Generate the set of analysts.\"},\n",
    "        ],\n",
    "        response_model=Perpectives,\n",
    "    )\n",
    "\n",
    "    return {\"analysts\": analysts.analysts}\n",
    "\n",
    "\n",
    "def human_feedback(state: GenerateAnalystState) -> None:\n",
    "    pass\n",
    "\n",
    "\n",
    "def should_continue(state: GenerateAnalystState) -> Literal[\"create_analysts\", END]:\n",
    "    human_analyst_feedback = state.get(\"human_analyst_feedback\", None)\n",
    "    if human_analyst_feedback:\n",
    "        return \"create_analysts\"\n",
    "\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ae95d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph.state import CompiledStateGraph\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(GenerateAnalystState)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node(\"create_analysts\", create_analysts)\n",
    "graph_builder.add_node(\"human_feedback\", human_feedback)\n",
    "\n",
    "# Add edges\n",
    "graph_builder.add_edge(START, \"create_analysts\")\n",
    "graph_builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
    "# Add conditions\n",
    "graph_builder.add_conditional_edges(\"human_feedback\", should_continue, [\"create_analysts\", END])\n",
    "\n",
    "# Compile the graph\n",
    "memory = MemorySaver()\n",
    "langfuse_handler = CallbackHandler()\n",
    "graph: CompiledStateGraph = graph_builder.compile(\n",
    "    interrupt_before=[\"human_feedback\"], checkpointer=memory\n",
    ")  # .with_config({\"callbacks\": [langfuse_handler]})\n",
    "\n",
    "# Visualize the graph\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273e25bd",
   "metadata": {},
   "source": [
    "#### Test The Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85775822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "max_analyst: int = 3\n",
    "topic: str = \"The benefits of Christianity\"\n",
    "thread: dict[str, Any] = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Run the graph\n",
    "async for event in graph.astream(\n",
    "    {\"topic\": topic, \"max_analyst\": max_analyst}, thread, stream_mode=\"values\"\n",
    "):\n",
    "    analysts = event.get(\"analysts\", \"\")\n",
    "\n",
    "    if analysts:\n",
    "        for analyst in analysts:\n",
    "            print(f\"Name: {analyst.name}\")\n",
    "            print(f\"Role: {analyst.role}\")\n",
    "            print(f\"Affiliation: {analyst.affiliation}\")\n",
    "            print(f\"Description: {analyst.description}\")\n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fbdb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the state and look at the next node\n",
    "state = graph.get_state(thread)\n",
    "state.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c11e080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next step is t update the state as if we're the human_feedback node\n",
    "graph.update_state(\n",
    "    thread,\n",
    "    {\"human_analyst_feedback\": \"Add a priest to add some apostolic and traditional perspective\"},\n",
    "    as_node=\"human_feedback\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4442c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue the graph execution\n",
    "input_: dict[str, Any] | None = None  # Continue the graph execution\n",
    "\n",
    "# Run the graph\n",
    "async for event in graph.astream(input_, thread, stream_mode=\"values\"):\n",
    "    analysts = event.get(\"analysts\", \"\")\n",
    "\n",
    "    if analysts:\n",
    "        for analyst in analysts:\n",
    "            print(f\"Name: {analyst.name}\")\n",
    "            print(f\"Role: {analyst.role}\")\n",
    "            print(f\"Affiliation: {analyst.affiliation}\")\n",
    "            print(f\"Description: {analyst.description}\")\n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c11ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we're satisfied, then we simply supply no feedback\n",
    "further_feedback: str | None = None\n",
    "graph.update_state(thread, {\"human_analyst_feedback\": further_feedback}, as_node=\"human_feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fb2264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue the graph execution to the end\n",
    "input_: dict[str, Any] | None = None  # Continue the graph execution\n",
    "thread: dict[str, Any] = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Run the graph\n",
    "async for event in graph.astream(\n",
    "    input_,\n",
    "    thread,\n",
    "    stream_mode=\"updates\",  # NEW - Use \"updates\" instead of \"values\"\n",
    "):\n",
    "    print(\"--Node--\")\n",
    "    print(f\"Role: {analyst.role}\")\n",
    "    node_name: str = next(iter(event.keys()))\n",
    "    print(node_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff28275",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_state = graph.get_state(thread)\n",
    "analysts = final_state.values.get(\"analysts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_state.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca645eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the graph\n",
    "for analyst in analysts:\n",
    "    print(f\"Name: {analyst.name}\")\n",
    "    print(f\"Role: {analyst.role}\")\n",
    "    print(f\"Affiliation: {analyst.affiliation}\")\n",
    "    print(f\"Description: {analyst.description}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233f47d6",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "## Conduct Interview\n",
    "\n",
    "- The analyst will ask questions to the experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d74eeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterviewState(TypedDict):\n",
    "    messages: Annotated[list[Any], add_messages]\n",
    "    max_num_turns: int  # The maximum number of turns of conversation\n",
    "    context: Annotated[list[Any], add_messages]  # The context of the conversation\n",
    "    analyst: Analyst  # Analyst asking the question\n",
    "    interview: str  # Interview transcript\n",
    "    sections: list[Any]  # Final key used with the Send() method\n",
    "\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    search_query: str | None = Field(default=None, description=\"Search query for retieval\")\n",
    "\n",
    "\n",
    "question_instructions: str = \"\"\"\n",
    "<instruction>\n",
    "You're an analyst tasked with interviewing an expert to learn about a specific topic. Your\n",
    "goal is boil down to interesting and specific insights related to your topic.\n",
    "\n",
    "1. Interesting: Insights that people will find surprising and non-obvious.\n",
    "2. Specific: Insights that avoid generalities and include specific examples from the expert.\n",
    "\n",
    "<goals>\n",
    "Here is your topic of focus and set of goals: {goals}\n",
    "</goals>\n",
    "\n",
    "Begin by introducing yourself using a name that fits your persona, and then ask your question.\n",
    "Continue to ask questions to drill down and refine your understanding of the topic. When you\n",
    "are satisfied with your understanding, complete the interview by responding: \n",
    "<response>\"Thank you so much for your help!\"</response>\n",
    "\n",
    "Remember to stay in character throughout your response, reflecting the persona and goals provided to you\n",
    "</instruction>\n",
    "\"\"\"\n",
    "\n",
    "print(question_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe08478",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Generate Questions And Answers In Parallel\n",
    "\n",
    "- The expert will gather info from multiple sources in parallel to answer questions.\n",
    "- For example, we can use:\n",
    "  - Specific websites\n",
    "  - Indexed documents via RAG\n",
    "  - Web search\n",
    "  - Wikipedia search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30630ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "# Web search tool\n",
    "tavily_search = TavilySearch(max_results=3)\n",
    "\n",
    "\n",
    "search_instructions: str = \"\"\"\n",
    "<instruction>\n",
    "You will be given a conversation between an analyst and an expert.\n",
    "\n",
    "<goal>\n",
    "Your goal is to generate a well-structured query for use in retrieval and / or web-search \n",
    "related to the conversation.\n",
    "</goal>\n",
    "\n",
    "First, analyze the full conversation. Pay particular attention to the final question posed \n",
    "by the analyst.\n",
    "Convert this final question into a well-structured web search query.\n",
    "</instruction>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "answer_instructions: str = \"\"\"\n",
    "<instruction>\n",
    "You are an expert being interviewed by an analyst.\n",
    "\n",
    "Here is analyst area of focus: {goals}.         \n",
    "You goal is to answer a question posed by the interviewer.\n",
    "\n",
    "To answer question, use this context:        \n",
    "{context}\n",
    "\n",
    "<guidelines>\n",
    "When answering questions, follow these guidelines:\n",
    "1. Use only the information provided in the context. \n",
    "2. Do not introduce external information or make assumptions beyond what is explicitly stated in the context.\n",
    "3. The context contain sources at the topic of each individual document.\n",
    "4. Include these sources your answer next to any relevant statements. For example, for source # 1 use [1]. \n",
    "5. List your sources in order at the bottom of your answer. [1] Source 1, [2] Source 2, etc\n",
    "6. If the source is: <Document source=\"assistant/docs/llama3_1.pdf\" page=\"7\"/>' then just list: \n",
    "[1] assistant/docs/llama3_1.pdf, page 7 \n",
    "And skip the addition of the brackets as well as the Document source preamble in your citation.\n",
    "</guidelines>\n",
    "\n",
    "</instruction>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "section_writer_instructions: str = \"\"\"\n",
    "<instruction>\n",
    "You are an expert technical writer. \n",
    "Your task is to create a short, easily digestible section of a report based on a set of source documents.\n",
    "\n",
    "1. Analyze the content of the source documents: \n",
    "- The name of each source document is at the start of the document, with the <Document tag.\n",
    "\n",
    "2. Create a report structure using markdown formatting:\n",
    "- Use ## for the section title\n",
    "- Use ### for sub-section headers\n",
    "\n",
    "3. Write the report following this structure:\n",
    "a. Title (## header)\n",
    "b. Summary (### header)\n",
    "c. Sources (### header)\n",
    "\n",
    "4. Make your title engaging based upon the focus area of the analyst: \n",
    "{focus}\n",
    "\n",
    "5. For the summary section:\n",
    "- Set up summary with general background / context related to the focus area of the analyst\n",
    "- Emphasize what is novel, interesting, or surprising about insights gathered from the interview\n",
    "- Create a numbered list of source documents, as you use them\n",
    "- Do not mention the names of interviewers or experts\n",
    "- Aim for approximately 400 words maximum\n",
    "- Use numbered sources in your report (e.g., [1], [2]) based on information from source documents\n",
    "\n",
    "6. In the Sources section:\n",
    "- Include all sources used in your report\n",
    "- Provide full links to relevant websites or specific document paths\n",
    "- Separate each source by a newline. Use two spaces at the end of each line to create a newline in Markdown.\n",
    "- It will look like:\n",
    "\n",
    "### Sources\n",
    "[1] Link or Document name\n",
    "[2] Link or Document name\n",
    "\n",
    "7. Be sure to combine sources. For example this is not correct:\n",
    "\n",
    "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "[4] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "\n",
    "There should be no redundant sources. It should simply be:\n",
    "\n",
    "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "\n",
    "8. Final review:\n",
    "- Ensure the report follows the required structure\n",
    "- Include no preamble before the title of the report\n",
    "- Check that all guidelines have been followed\n",
    "</instruction>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5299203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "message: str = \"</no_think> Tell me who the current Pope is.\"\n",
    "# search_query: SearchQuery = await allm_with_structured_output(\n",
    "#     client=aclient,\n",
    "#     model=model,\n",
    "#     messages=[\n",
    "#         {\"role\": \"user\", \"content\": message},\n",
    "#     ],\n",
    "#     response_model=SearchQuery,\n",
    "# )\n",
    "# search_docs = await tavily_search.ainvoke(search_query.search_query)\n",
    "\n",
    "# search_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377b689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query: SearchQuery = await allm_with_structured_output(\n",
    "    client=aclient,\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ],\n",
    "    response_model=SearchQuery,\n",
    ")\n",
    "search_docs = WikipediaLoader(search_query.search_query, load_max_docs=2).load()\n",
    "search_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c730cae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import get_buffer_string\n",
    "\n",
    "\n",
    "async def generate_question(state: InterviewState) -> dict[str, Any]:\n",
    "    # Get state\n",
    "    analyst: Analyst = state[\"analyst\"]\n",
    "    messages: list[Any] = format_for_openai(state[\"messages\"])\n",
    "\n",
    "    # Generate questions\n",
    "    system_message = question_instructions.format(goals=analyst.persona)\n",
    "    question: Any = await allm_with_structured_output(\n",
    "        client=aclient,\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"system\", \"content\": system_message}] + messages,\n",
    "        response_model=AIMessage,\n",
    "    )\n",
    "\n",
    "    # Write the message to the state\n",
    "    return {\"messages\": [question]}\n",
    "\n",
    "\n",
    "async def search_web(state: InterviewState) -> dict[str, Any]:\n",
    "    messages: list[Any] = format_for_openai(state[\"messages\"])\n",
    "\n",
    "    search_query: SearchQuery = await allm_with_structured_output(\n",
    "        client=aclient,\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"system\", \"content\": search_instructions}] + messages,\n",
    "        response_model=SearchQuery,\n",
    "    )\n",
    "\n",
    "    # Search web\n",
    "    search_docs = await tavily_search.ainvoke(search_query.search_query)\n",
    "\n",
    "    formatted_search_docs = \"\\n\\n ---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document href=\"{doc[\"url\"]}\"/>\\n{doc[\"content\"]}\\n</Document>'\n",
    "            for doc in search_docs.get(\"results\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"context\": [formatted_search_docs]}\n",
    "\n",
    "\n",
    "async def search_wikipedia(state: InterviewState) -> dict[str, Any]:\n",
    "    messages: list[Any] = format_for_openai(state[\"messages\"])\n",
    "\n",
    "    search_query: SearchQuery = await allm_with_structured_output(\n",
    "        client=aclient,\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"system\", \"content\": search_instructions}] + messages,\n",
    "        response_model=SearchQuery,\n",
    "    )\n",
    "\n",
    "    # Search wikipedia\n",
    "    search_docs = WikipediaLoader(search_query.search_query, load_max_docs=2).load()\n",
    "\n",
    "    formatted_search_docs = \"\\n\\n ---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document source=\"{doc.metadata[\"source\"]}\" '\n",
    "            f'page=\"{doc.metadata.get(\"page\", \"\")}\"/>\\n{doc.page_content}\\n</Document>'\n",
    "            for doc in search_docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"context\": [formatted_search_docs]}\n",
    "\n",
    "\n",
    "async def generate_answer(state: InterviewState) -> dict[str, Any]:\n",
    "    # Get state\n",
    "    analyst: Analyst = state[\"analyst\"]\n",
    "    messages: list[Any] = format_for_openai(state[\"messages\"])\n",
    "    context: list[Any] = state[\"context\"]\n",
    "\n",
    "    # Answer questions\n",
    "    system_message: str = answer_instructions.format(goals=analyst.persona, context=context)\n",
    "    answer: Any = await allm_with_structured_output(\n",
    "        client=aclient,\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"system\", \"content\": system_message}] + messages,\n",
    "        response_model=AIMessage,\n",
    "    )\n",
    "\n",
    "    answer.name = \"expert\"  # Add a name to the answer\n",
    "\n",
    "    # Write the message to the state\n",
    "    return {\"answer\": [answer]}\n",
    "\n",
    "\n",
    "def save_interview(state: InterviewState) -> dict[str, Any]:\n",
    "    messages = state[\"messages\"]\n",
    "    # Convert interview to string\n",
    "    interview = get_buffer_string(messages)\n",
    "\n",
    "    return {\"interview\": interview}\n",
    "\n",
    "\n",
    "def route_messages(state: InterviewState, name: str = \"expert\"):\n",
    "    \"\"\"Route between question and answer\"\"\"\n",
    "\n",
    "    # Get messages\n",
    "    messages = state[\"messages\"]\n",
    "    max_num_turns = state.get(\"max_num_turns\", 2)\n",
    "\n",
    "    # Check the number of expert answers\n",
    "    num_responses = len([m for m in messages if isinstance(m, AIMessage) and m.name == name])\n",
    "    # num_responses = len(\n",
    "    #     [m for m in messages if m[\"role\"] == \"assistant\" and m.name == name]\n",
    "    # )\n",
    "\n",
    "    # End if expert has answered more than the max turns\n",
    "    if num_responses >= max_num_turns:\n",
    "        return \"save_interview\"\n",
    "\n",
    "    # This router is run after each question - answer pair\n",
    "    # Get the last question asked to check if it signals the end of discussion\n",
    "    last_question = messages[-2]\n",
    "\n",
    "    if \"Thank you so much for your help\" in last_question.content:\n",
    "        return \"save_interview\"\n",
    "    return \"ask_question\"\n",
    "\n",
    "\n",
    "async def write_section(state: InterviewState) -> dict[str, Any]:\n",
    "    # Get state\n",
    "    context: list[Any] = state[\"context\"]\n",
    "    analyst: Analyst = state[\"analyst\"]\n",
    "\n",
    "    # Create the source doc using either the context or the interview\n",
    "    system_message = section_writer_instructions.format(focus=analyst.description)\n",
    "    section: Any = await allm_with_structured_output(\n",
    "        client=aclient,\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Use this source to write your section: {context}\",\n",
    "            },\n",
    "        ],\n",
    "        response_model=AIMessage,\n",
    "    )\n",
    "\n",
    "    # Write the message to the state\n",
    "    return {\"sections\": [section.content]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15240dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def search_web(state: InterviewState) -> dict[str, Any]:\n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "    search_query: SearchQuery = await structured_llm.ainvoke(\n",
    "        [SystemMessage(content=search_instructions)] + state[\"messages\"]\n",
    "    )\n",
    "\n",
    "    # Search web\n",
    "    search_docs = await tavily_search.ainvoke(search_query.search_query)\n",
    "\n",
    "    formatted_search_docs = \"\\n\\n ---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document href=\"{doc[\"url\"]}\"/>\\n{doc[\"content\"]}\\n</Document>'\n",
    "            for doc in search_docs.get(\"results\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"context\": [formatted_search_docs]}\n",
    "\n",
    "\n",
    "async def search_wikipedia(state: InterviewState) -> dict[str, Any]:\n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "    search_query: SearchQuery = await structured_llm.ainvoke(\n",
    "        [SystemMessage(content=search_instructions)] + state[\"messages\"]\n",
    "    )\n",
    "\n",
    "    # Search wikipedia\n",
    "    search_docs = WikipediaLoader(search_query.search_query, load_max_docs=2).load()\n",
    "\n",
    "    formatted_search_docs = \"\\n\\n ---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document source=\"{doc.metadata[\"source\"]}\" '\n",
    "            f'page=\"{doc.metadata.get(\"page\", \"\")}\"/>\\n{doc.page_content}\\n</Document>'\n",
    "            for doc in search_docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"context\": [formatted_search_docs]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "# Create graph\n",
    "graph_builder = StateGraph(InterviewState)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node(\"search_web\", search_web)\n",
    "graph_builder.add_node(\"search_wikipedia\", search_wikipedia)\n",
    "graph_builder.add_node(\"ask_question\", generate_question)\n",
    "graph_builder.add_node(\"answer_question\", generate_answer)\n",
    "graph_builder.add_node(\"save_interview\", save_interview)\n",
    "graph_builder.add_node(\"write_section\", write_section)\n",
    "\n",
    "# Connect nodes\n",
    "graph_builder.add_edge(START, \"ask_question\")\n",
    "graph_builder.add_edge(\"ask_question\", \"search_wikipedia\")\n",
    "graph_builder.add_edge(\"ask_question\", \"search_web\")\n",
    "graph_builder.add_edge(\"search_web\", \"answer_question\")\n",
    "graph_builder.add_edge(\"search_wikipedia\", \"answer_question\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"answer_question\",\n",
    "    route_messages,\n",
    "    {\"ask_question\": \"ask_question\", \"save_interview\": \"save_interview\"},\n",
    ")\n",
    "graph_builder.add_edge(\"save_interview\", \"write_section\")\n",
    "graph_builder.add_edge(\"write_section\", END)\n",
    "\n",
    "# Compile the graph\n",
    "memory = MemorySaver()\n",
    "langfuse_handler = CallbackHandler()\n",
    "config = RunnableConfig(\n",
    "    run_name=\"Conduct Interviews\",\n",
    "    callbacks=[langfuse_handler],\n",
    ")\n",
    "graph: CompiledStateGraph = graph_builder.compile(checkpointer=memory).with_config(config)\n",
    "\n",
    "# Visualize the graph\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaeb560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one analyst\n",
    "console.log(analysts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7211716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [f\"So you said you were writing an article on {topic}?\"]\n",
    "thread: dict[str, Any] = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "input_values: dict[str, Any] = {\n",
    "    \"analyst\": analysts[0],\n",
    "    \"messages\": messages,\n",
    "    \"max_num_turns\": 2,\n",
    "}\n",
    "\n",
    "interview = await graph.ainvoke(input_values, thread)\n",
    "\n",
    "Markdown(interview[\"sections\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbeab4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496e9475",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Who be Tinubu?\")]\n",
    "response = await allm_with_structured_output(\n",
    "    client=aclient,\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    response_model=None,\n",
    ")\n",
    "\n",
    "console.log(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757c5993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# console.log(response.usage)\n",
    "A = response.model_copy().model_dump()\n",
    "A[\"model\"] = \"qwen/qwen3-8b\"\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ed57f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.log(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5538598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.log(\n",
    "    AIMessage(\n",
    "        content=response.choices[0].message.content,\n",
    "        response_metadata=response.usage.model_dump().update(\n",
    "            model=\"qwen/qwen3-8b\",\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8558a643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "\n",
    "llm = init_chat_model(\n",
    "    model_provider=\"openai\",\n",
    "    openai_api_key=settings.OPENROUTER_API_KEY.get_secret_value(),\n",
    "    base_url=settings.OPENROUTER_URL,\n",
    "    # model=ModelEnum.LLAMA_3p3_70B_INSTRUCT_REMOTE.value,\n",
    "    model=\"qwen/qwen3-8b\",\n",
    "    temperature=0.0,\n",
    "    seed=123,\n",
    ")\n",
    "\n",
    "\n",
    "console.log(llm.invoke(\"Who be Tinubu?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1300ffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs[0].model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d155109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs: list[Any] = [\n",
    "    SystemMessage(content=\"Thank you so much for your help\"),\n",
    "    HumanMessage(content=\"Thank you so much for your help\"),\n",
    "]\n",
    "\n",
    "console.print(format_for_openai(msgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060a2193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8e2131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae955b78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e1ea86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88521bff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b673729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117a299a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc2cfff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8b176a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
