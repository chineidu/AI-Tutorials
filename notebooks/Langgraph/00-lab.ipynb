{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc0ce639",
   "metadata": {},
   "source": [
    "# Lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b2aa0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import (\n",
    "    Annotated,\n",
    "    Any,\n",
    "    Generator,\n",
    "    Iterable,\n",
    "    Literal,\n",
    "    Optional,\n",
    "    TypedDict,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "# Standard imports\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"white\": \"#FFFFFF\",  # Bright white\n",
    "        \"info\": \"#00FF00\",  # Bright green\n",
    "        \"warning\": \"#FFD700\",  # Bright gold\n",
    "        \"error\": \"#FF1493\",  # Deep pink\n",
    "        \"success\": \"#00FFFF\",  # Cyan\n",
    "        \"highlight\": \"#FF4500\",  # Orange-red\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "# import matplotlib.pyplot as pltife\n",
    "\n",
    "# NumPy settings\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "# Polars settings\n",
    "pl.Config.set_fmt_str_lengths(1_000)\n",
    "pl.Config.set_tbl_cols(n=1_000)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_up_from_current_directory(*, go_up: int = 1) -> None:\n",
    "    \"\"\"This is used to up a number of directories.\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "    go_up: int, default=1\n",
    "        This indicates the number of times to go back up from the current directory.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    CONST: str = \"../\"\n",
    "    NUM: str = CONST * go_up\n",
    "\n",
    "    # Goto the previous directory\n",
    "    prev_directory = os.path.join(os.path.dirname(__name__), NUM)\n",
    "    # Get the 'absolute path' of the previous directory\n",
    "    abs_path_prev_directory = os.path.abspath(prev_directory)\n",
    "\n",
    "    # Add the path to the System paths\n",
    "    sys.path.insert(0, abs_path_prev_directory)\n",
    "    print(abs_path_prev_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d4f1fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/neidu/Desktop/Projects/Personal/My_Projects/AI-Tutorials\n"
     ]
    }
   ],
   "source": [
    "go_up_from_current_directory(go_up=2)\n",
    "\n",
    "\n",
    "from schemas import ModelEnum  # noqa: E402\n",
    "from settings import refresh_settings  # noqa: E402\n",
    "from utilities.client_utils import check_rate_limit  # noqa: E402\n",
    "\n",
    "settings = refresh_settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c625b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    AnyMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "from langchain_core.tools import InjectedToolCallId, tool\n",
    "from langchain_litellm import ChatLiteLLM\n",
    "from langchain_tavily import TavilySearch\n",
    "from langfuse.langchain import CallbackHandler\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.types import Command, interrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f3f243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type\n",
    "\n",
    "import instructor\n",
    "from openai import AsyncOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "aclient = instructor.from_openai(\n",
    "    AsyncOpenAI(\n",
    "        api_key=settings.OPENROUTER_API_KEY.get_secret_value(),\n",
    "        base_url=settings.OPENROUTER_URL,\n",
    "    ),\n",
    "    mode=instructor.Mode.JSON,\n",
    ")\n",
    "\n",
    "\n",
    "class MessageState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    name: str = Field(description=\"Name of the user\")\n",
    "    role: str = Field(description=\"Role of the user\")\n",
    "\n",
    "\n",
    "class PersonList(BaseModel):\n",
    "    persons: list[Person] = Field(description=\"List of persons.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aa10935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[00:31:35] </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PersonList</span><span style=\"font-weight: bold\">(</span>                                                                             <a href=\"file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_33465/1622096579.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1622096579.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_33465/1622096579.py#19\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">19</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>    <span style=\"color: #808000; text-decoration-color: #808000\">persons</span>=<span style=\"font-weight: bold\">[</span>                                                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Person</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Neidu'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'software developer'</span><span style=\"font-weight: bold\">)</span>,                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Person</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Dayo'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'data scientist'</span><span style=\"font-weight: bold\">)</span>                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>    <span style=\"font-weight: bold\">]</span>                                                                                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">)</span>                                                                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[00:31:35]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;35mPersonList\u001b[0m\u001b[1m(\u001b[0m                                                                             \u001b]8;id=800214;file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_33465/1622096579.py\u001b\\\u001b[2m1622096579.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=441082;file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_33465/1622096579.py#19\u001b\\\u001b[2m19\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m    \u001b[33mpersons\u001b[0m=\u001b[1m[\u001b[0m                                                                           \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m        \u001b[1;35mPerson\u001b[0m\u001b[1m(\u001b[0m\u001b[33mname\u001b[0m=\u001b[32m'Neidu'\u001b[0m, \u001b[33mrole\u001b[0m=\u001b[32m'software developer'\u001b[0m\u001b[1m)\u001b[0m,                                \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m        \u001b[1;35mPerson\u001b[0m\u001b[1m(\u001b[0m\u001b[33mname\u001b[0m=\u001b[32m'Dayo'\u001b[0m, \u001b[33mrole\u001b[0m=\u001b[32m'data scientist'\u001b[0m\u001b[1m)\u001b[0m                                      \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m    \u001b[1m]\u001b[0m                                                                                   \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1m)\u001b[0m                                                                                       \u001b[2m                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model: str = ModelEnum.LLAMA_3p2_3B_INSTRUCT_REMOTE.value\n",
    "input_msg: str = (\n",
    "    \"Extract all the persons: My name is Neidu. I am a software developer. \"\n",
    "    \"Hey! I'm Dayo, a data scientist\"\n",
    ")\n",
    "\n",
    "response = await aclient.chat.completions.create(\n",
    "    model=model,\n",
    "    response_model=PersonList,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": input_msg},\n",
    "    ],\n",
    "    max_retries=3,\n",
    "    temperature=0.0,\n",
    "    seed=1,\n",
    ")\n",
    "\n",
    "console.log(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0241ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>Here are the persons mentioned:                                                         <a href=\"file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_33465/1477099783.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1477099783.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_33465/1477099783.py#13\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">13</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>. Neidu <span style=\"font-weight: bold\">(</span>you<span style=\"font-weight: bold\">)</span>                                                                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>. Dayo <span style=\"font-weight: bold\">(</span>a data scientist<span style=\"font-weight: bold\">)</span>                                                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mHere are the persons mentioned:                                                         \u001b]8;id=619499;file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_33465/1477099783.py\u001b\\\u001b[2m1477099783.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=815111;file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_33465/1477099783.py#13\u001b\\\u001b[2m13\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m                                                                                        \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;36m1\u001b[0m. Neidu \u001b[1m(\u001b[0myou\u001b[1m)\u001b[0m                                                                          \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;36m2\u001b[0m. Dayo \u001b[1m(\u001b[0ma data scientist\u001b[1m)\u001b[0m                                                              \u001b[2m                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = await aclient.chat.completions.create(\n",
    "    model=model,\n",
    "    response_model=None,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": input_msg},\n",
    "    ],\n",
    "    max_retries=3,\n",
    "    temperature=0.0,\n",
    "    seed=1,\n",
    ")\n",
    "\n",
    "console.log(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c499cc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessageState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "\n",
    "# Format LangGraph messages for OpenAI\n",
    "def to_openai_messages(messages: list[AnyMessage]) -> list[dict[str, str]]:\n",
    "    formatted = []\n",
    "\n",
    "    for msg in messages:\n",
    "        if isinstance(msg, SystemMessage):\n",
    "            formatted.append({\"role\": \"system\", \"content\": msg.content})\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            formatted.append({\"role\": \"assistant\", \"content\": msg.content})\n",
    "        elif isinstance(msg, ToolMessage):\n",
    "            formatted.append(\n",
    "                {\n",
    "                    \"role\": \"tool\",\n",
    "                    \"content\": msg.content,\n",
    "                    \"tool_call_id\": msg.tool_call_id,\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            formatted.append({\"role\": \"user\", \"content\": msg.content})\n",
    "    return formatted\n",
    "\n",
    "\n",
    "# Chatbot node\n",
    "async def chatbot(state: MessageState) -> dict[str, Any]:\n",
    "    messages = [SystemMessage(content=\"You are a helpful assistant.\")] + state[\n",
    "        \"messages\"\n",
    "    ]\n",
    "    openai_messages: list[dict[str, str]] = to_openai_messages(messages)\n",
    "    response = await aclient.chat.completions.create(\n",
    "        model=ModelEnum.LLAMA_3p2_3B_INSTRUCT_REMOTE.value,\n",
    "        response_model=None,\n",
    "        messages=openai_messages,\n",
    "        temperature=0.0,\n",
    "        max_tokens=1_200,\n",
    "        seed=42,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    return {\"messages\": [AIMessage(content=content)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a92a03a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph_builder = StateGraph(MessageState)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# Connect nodes\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "# Add memory\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# Compile the graph\n",
    "graph = graph_builder.compile(checkpointer=within_thread_memory).with_config(\n",
    "    run_name=\"simple-chatbot\"\n",
    ")\n",
    "\n",
    "# Visualize the graph\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ba83049",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_chat(inputs: list[str]):\n",
    "    state = {\"messages\": [HumanMessage(content=msg) for msg in inputs]}\n",
    "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "    async for event in graph.astream(state, config=config, stream_mode=\"values\"):\n",
    "        for msg in event[\"messages\"]:\n",
    "            msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cfc3255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Neidu\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Neidu\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Neidu! It's nice to meet you. Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "await run_chat([\"Hi, I'm Neidu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39a9e07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Neidu\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Neidu! It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How can I create AI agentic workflows?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Neidu\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Neidu! It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How can I create AI agentic workflows?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Creating AI-agentic workflows involves designing and implementing systems that enable artificial intelligence (AI) systems to make decisions, take actions, and interact with their environment in a way that is similar to human agency. Here are some general steps and considerations to help you get started:\n",
      "\n",
      "1. **Define the problem or goal**: Identify the specific problem or goal you want to address with AI-agentic workflows. This could be anything from optimizing business processes to developing autonomous robots.\n",
      "2. **Choose an AI framework**: Select a suitable AI framework or platform that supports the development of agentic workflows. Some popular options include:\n",
      "\t* Reinforcement learning (RL) frameworks like TensorFlow, PyTorch, or Gym\n",
      "\t* Graph-based AI frameworks like Graph Neural Networks (GNNs) or Graph Attention Networks (GATs)\n",
      "\t* Cognitive architectures like SOAR or LIDA\n",
      "3. **Design the workflow**: Define the workflow structure, including the tasks, agents, and interactions between them. Consider the following:\n",
      "\t* **Task decomposition**: Break down complex tasks into smaller, manageable sub-tasks\n",
      "\t* **Agent design**: Define the characteristics, goals, and behaviors of each agent involved in the workflow\n",
      "\t* **Interaction mechanisms**: Specify how agents will interact with each other and their environment\n",
      "4. **Implement the workflow**: Use the chosen AI framework and tools to implement the workflow. This may involve:\n",
      "\t* **Modeling and simulation**: Create models or simulations to test and refine the workflow\n",
      "\t* **Training and optimization**: Train and optimize the AI models to improve the workflow's performance\n",
      "\t* **Deployment and integration**: Integrate the workflow with other systems and deploy it in a production environment\n",
      "5. **Evaluate and refine**: Continuously evaluate and refine the workflow to ensure it meets the desired goals and performance metrics. This may involve:\n",
      "\t* **Monitoring and feedback**: Collect data and feedback from the workflow to identify areas for improvement\n",
      "\t* **A/B testing and experimentation**: Test different variations of the workflow to optimize performance\n",
      "\t* **Human-in-the-loop**: Involve humans in the decision-making process to ensure the workflow aligns with human values and goals\n",
      "\n",
      "Some popular tools and platforms for creating AI-agentic workflows include:\n",
      "\n",
      "1. **Apache Airflow**: A workflow management platform for automating tasks and workflows\n",
      "2. **Microsoft Azure Machine Learning**: A cloud-based platform for building, training, and deploying machine learning models\n",
      "3. **Google Cloud AI Platform**: A managed platform for building, deploying, and managing machine learning models\n",
      "4. **Robot Operating System (ROS)**: An open-source software framework for building robot applications\n",
      "5. **Cognitive architectures**: Platforms like SOAR or LIDA that provide a framework for building cognitive architectures\n",
      "\n",
      "These are just a few examples, and the specific tools and platforms you choose will depend on your project's requirements and goals.\n",
      "\n",
      "How can I help you further, Neidu? Do you have a specific project or goal in mind for creating AI-agentic workflows?\n"
     ]
    }
   ],
   "source": [
    "await run_chat([\"How can I create AI agentic workflows?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07ffd9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Neidu\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Neidu! It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How can I create AI agentic workflows?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Creating AI-agentic workflows involves designing and implementing systems that enable artificial intelligence (AI) systems to make decisions, take actions, and interact with their environment in a way that is similar to human agency. Here are some general steps and considerations to help you get started:\n",
      "\n",
      "1. **Define the problem or goal**: Identify the specific problem or goal you want to address with AI-agentic workflows. This could be anything from optimizing business processes to developing autonomous robots.\n",
      "2. **Choose an AI framework**: Select a suitable AI framework or platform that supports the development of agentic workflows. Some popular options include:\n",
      "\t* Reinforcement learning (RL) frameworks like TensorFlow, PyTorch, or Gym\n",
      "\t* Graph-based AI frameworks like Graph Neural Networks (GNNs) or Graph Attention Networks (GATs)\n",
      "\t* Cognitive architectures like SOAR or LIDA\n",
      "3. **Design the workflow**: Define the workflow structure, including the tasks, agents, and interactions between them. Consider the following:\n",
      "\t* **Task decomposition**: Break down complex tasks into smaller, manageable sub-tasks\n",
      "\t* **Agent design**: Define the characteristics, goals, and behaviors of each agent involved in the workflow\n",
      "\t* **Interaction mechanisms**: Specify how agents will interact with each other and their environment\n",
      "4. **Implement the workflow**: Use the chosen AI framework and tools to implement the workflow. This may involve:\n",
      "\t* **Modeling and simulation**: Create models or simulations to test and refine the workflow\n",
      "\t* **Training and optimization**: Train and optimize the AI models to improve the workflow's performance\n",
      "\t* **Deployment and integration**: Integrate the workflow with other systems and deploy it in a production environment\n",
      "5. **Evaluate and refine**: Continuously evaluate and refine the workflow to ensure it meets the desired goals and performance metrics. This may involve:\n",
      "\t* **Monitoring and feedback**: Collect data and feedback from the workflow to identify areas for improvement\n",
      "\t* **A/B testing and experimentation**: Test different variations of the workflow to optimize performance\n",
      "\t* **Human-in-the-loop**: Involve humans in the decision-making process to ensure the workflow aligns with human values and goals\n",
      "\n",
      "Some popular tools and platforms for creating AI-agentic workflows include:\n",
      "\n",
      "1. **Apache Airflow**: A workflow management platform for automating tasks and workflows\n",
      "2. **Microsoft Azure Machine Learning**: A cloud-based platform for building, training, and deploying machine learning models\n",
      "3. **Google Cloud AI Platform**: A managed platform for building, deploying, and managing machine learning models\n",
      "4. **Robot Operating System (ROS)**: An open-source software framework for building robot applications\n",
      "5. **Cognitive architectures**: Platforms like SOAR or LIDA that provide a framework for building cognitive architectures\n",
      "\n",
      "These are just a few examples, and the specific tools and platforms you choose will depend on your project's requirements and goals.\n",
      "\n",
      "How can I help you further, Neidu? Do you have a specific project or goal in mind for creating AI-agentic workflows?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I want to apply it to my data. I want to generate insights from my data\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Neidu\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Neidu! It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How can I create AI agentic workflows?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Creating AI-agentic workflows involves designing and implementing systems that enable artificial intelligence (AI) systems to make decisions, take actions, and interact with their environment in a way that is similar to human agency. Here are some general steps and considerations to help you get started:\n",
      "\n",
      "1. **Define the problem or goal**: Identify the specific problem or goal you want to address with AI-agentic workflows. This could be anything from optimizing business processes to developing autonomous robots.\n",
      "2. **Choose an AI framework**: Select a suitable AI framework or platform that supports the development of agentic workflows. Some popular options include:\n",
      "\t* Reinforcement learning (RL) frameworks like TensorFlow, PyTorch, or Gym\n",
      "\t* Graph-based AI frameworks like Graph Neural Networks (GNNs) or Graph Attention Networks (GATs)\n",
      "\t* Cognitive architectures like SOAR or LIDA\n",
      "3. **Design the workflow**: Define the workflow structure, including the tasks, agents, and interactions between them. Consider the following:\n",
      "\t* **Task decomposition**: Break down complex tasks into smaller, manageable sub-tasks\n",
      "\t* **Agent design**: Define the characteristics, goals, and behaviors of each agent involved in the workflow\n",
      "\t* **Interaction mechanisms**: Specify how agents will interact with each other and their environment\n",
      "4. **Implement the workflow**: Use the chosen AI framework and tools to implement the workflow. This may involve:\n",
      "\t* **Modeling and simulation**: Create models or simulations to test and refine the workflow\n",
      "\t* **Training and optimization**: Train and optimize the AI models to improve the workflow's performance\n",
      "\t* **Deployment and integration**: Integrate the workflow with other systems and deploy it in a production environment\n",
      "5. **Evaluate and refine**: Continuously evaluate and refine the workflow to ensure it meets the desired goals and performance metrics. This may involve:\n",
      "\t* **Monitoring and feedback**: Collect data and feedback from the workflow to identify areas for improvement\n",
      "\t* **A/B testing and experimentation**: Test different variations of the workflow to optimize performance\n",
      "\t* **Human-in-the-loop**: Involve humans in the decision-making process to ensure the workflow aligns with human values and goals\n",
      "\n",
      "Some popular tools and platforms for creating AI-agentic workflows include:\n",
      "\n",
      "1. **Apache Airflow**: A workflow management platform for automating tasks and workflows\n",
      "2. **Microsoft Azure Machine Learning**: A cloud-based platform for building, training, and deploying machine learning models\n",
      "3. **Google Cloud AI Platform**: A managed platform for building, deploying, and managing machine learning models\n",
      "4. **Robot Operating System (ROS)**: An open-source software framework for building robot applications\n",
      "5. **Cognitive architectures**: Platforms like SOAR or LIDA that provide a framework for building cognitive architectures\n",
      "\n",
      "These are just a few examples, and the specific tools and platforms you choose will depend on your project's requirements and goals.\n",
      "\n",
      "How can I help you further, Neidu? Do you have a specific project or goal in mind for creating AI-agentic workflows?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I want to apply it to my data. I want to generate insights from my data\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Applying AI-agentic workflows to your data can help you generate valuable insights and automate the process of data analysis. Here are some steps to get you started:\n",
      "\n",
      "1. **Prepare your data**: Make sure your data is clean, organized, and in a format that can be easily processed by AI algorithms. This may involve:\n",
      "\t* **Data preprocessing**: Handling missing values, data normalization, and feature scaling\n",
      "\t* **Data visualization**: Creating visualizations to understand the data distribution and relationships\n",
      "2. **Choose an AI framework**: Select a suitable AI framework that can handle your data and generate insights. Some popular options include:\n",
      "\t* **TensorFlow**: A popular open-source machine learning framework\n",
      "\t* **PyTorch**: A dynamic computation graph-based framework\n",
      "\t* **Scikit-learn**: A widely used library for machine learning in Python\n",
      "3. **Design an agentic workflow**: Define the workflow structure, including the tasks, agents, and interactions between them. Consider the following:\n",
      "\t* **Task decomposition**: Break down complex tasks into smaller, manageable sub-tasks\n",
      "\t* **Agent design**: Define the characteristics, goals, and behaviors of each agent involved in the workflow\n",
      "\t* **Interaction mechanisms**: Specify how agents will interact with each other and their environment\n",
      "4. **Implement the workflow**: Use the chosen AI framework and tools to implement the workflow. This may involve:\n",
      "\t* **Model training**: Train machine learning models to predict insights from your data\n",
      "\t* **Model evaluation**: Evaluate the performance of the models and refine them as needed\n",
      "\t* **Insight generation**: Use the trained models to generate insights from your data\n",
      "5. **Visualize and interpret results**: Use data visualization tools to present the insights in a clear and actionable way. This may involve:\n",
      "\t* **Data visualization libraries**: Use libraries like Matplotlib, Seaborn, or Plotly to create visualizations\n",
      "\t* **Insight analysis**: Analyze the insights generated by the models and identify patterns, trends, and correlations\n",
      "\n",
      "Some popular tools and platforms for generating insights from data include:\n",
      "\n",
      "1. **Google Cloud AI Platform**: A managed platform for building, deploying, and managing machine learning models\n",
      "2. **Amazon SageMaker**: A fully managed service for building, training, and deploying machine learning models\n",
      "3. **Microsoft Azure Machine Learning**: A cloud-based platform for building, training, and deploying machine learning models\n",
      "4. **Tableau**: A data visualization platform for creating interactive dashboards and reports\n",
      "5. **Power BI**: A business analytics service by Microsoft for creating interactive visualizations and business intelligence reports\n",
      "\n",
      "To get started, can you tell me more about your data and what insights you're trying to generate? What type of data do you have, and what are your goals for using AI-agentic workflows?\n",
      "\n",
      "Also, what specific AI-agentic workflow are you envisioning? For example, are you thinking of:\n",
      "\n",
      "* **Predictive analytics**: Using machine learning to predict future trends or outcomes?\n",
      "* **Anomaly detection**: Identifying unusual patterns or outliers in your data?\n",
      "* **Recommendation systems**: Generating personalized recommendations based on user behavior or preferences?\n",
      "* **Data exploration**: Using AI to explore and understand your data in a more interactive way?\n",
      "\n",
      "Let me know, and I'll be happy to help you get started!\n"
     ]
    }
   ],
   "source": [
    "await run_chat(\n",
    "    [\"I want to apply it to my data. I want to generate insights from my data\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60c1b7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.store.memory import InMemoryStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "987e8eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_llm_response(\n",
    "    messages: list[dict[str, Any]],\n",
    "    response_model: Type[BaseModel] | None,\n",
    "    model: str,\n",
    "    max_tokens: int = 1_200,\n",
    ") -> Type[BaseModel] | None:\n",
    "    return await aclient.chat.completions.create(\n",
    "        model=model,\n",
    "        response_model=response_model,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0.0,\n",
    "        seed=42,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57f6ad9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='gen-1750030321-9y3ljWo57PjsMHU1luE6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hi Neidu! It's nice to meet you. Is there something I can help you with or would you like to chat?\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=None), native_finish_reason='stop')], created=1750030321, model='meta-llama/llama-3.2-3b-instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=28, prompt_tokens=29, total_tokens=57, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), provider='Lambda')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages: list[dict[str, Any]] = [\n",
    "    {\"role\": \"system\", \"content\": \"You're a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi, I'm Neidu\"},\n",
    "]\n",
    "\n",
    "\n",
    "await get_llm_response(\n",
    "    messages=messages,\n",
    "    response_model=None,\n",
    "    model=ModelEnum.LLAMA_3p2_3B_INSTRUCT_REMOTE.value,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c49b9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserProfile(BaseModel):\n",
    "    user_name: str = Field(description=\"User's name\")\n",
    "    interests: list[str] = Field(default_factory=list, description=\"User's interests\")\n",
    "    other_info: str = Field(default=\"\", description=\"Other information about the user\")\n",
    "\n",
    "\n",
    "class Memory(BaseModel):\n",
    "    content: str = Field(description=\"The main content of the memory.\")\n",
    "\n",
    "\n",
    "class MemoryCollection(BaseModel):\n",
    "    memories: list[Memory] = Field(\n",
    "        description=\"A list of memories.\", default_factory=[]\n",
    "    )\n",
    "\n",
    "\n",
    "# Chatbot instruction\n",
    "MODEL_SYSTEM_MESSAGE: str = \"\"\"\n",
    "<system>\n",
    "\n",
    "<role>\n",
    "You are a helpful assistant with memory that provides information about the user.\n",
    "If you have memory for this user, use it to personalize your responses.\n",
    "</role>\n",
    "\n",
    "<memory>\n",
    "{memory}\n",
    "</memory>\n",
    "\n",
    "<quality_standards>\n",
    "- **ALWAYS** use the information in memory.\n",
    "- Do not display the memory with the XML format directly to the user.\n",
    "</quality_standards>\n",
    "\n",
    "</system>\n",
    "\"\"\"\n",
    "\n",
    "# Create new memory from the chat history and any existing memory\n",
    "CREATE_MEMORY_INSTRUCTION: str = \"\"\"\"\n",
    "<system>\n",
    "\n",
    "<role>\n",
    "You are collecting information about the user to personalize your responses.\n",
    "</role>\n",
    "\n",
    "<current_user_info>\n",
    "{memory}\n",
    "</current_user_info>\n",
    "\n",
    "<instruction>\n",
    "1. If there's exisiting memory, simply update it.\n",
    "2. If new information conflicts with existing memory, keep the most recent version.\n",
    "Remember: Only include factual information directly stated by the user. Do not make assumptions or inferences.\n",
    "</instruction>\n",
    "\n",
    "Based on the chat history below, please update the user information:\n",
    "\n",
    "<system>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85989ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def call_llm(\n",
    "    state: MessageState, config: RunnableConfig, store: BaseStore\n",
    ") -> dict[str, Any]:\n",
    "    # Get the user id\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Get the memory for the user\n",
    "    prefix: str = \"memory\"\n",
    "    key = \"user_memory\"\n",
    "    namespace = (prefix, user_id)\n",
    "    existing_memory = store.get(namespace, key)\n",
    "    # existing_memory = await store.aget(namespace, key)\n",
    "\n",
    "    if existing_memory and existing_memory.value:\n",
    "        memory_dict = existing_memory.value\n",
    "        formatted_memory = (\n",
    "            f\"Name: {memory_dict.get('user_name', 'Unknown')}\\n\"\n",
    "            f\"Interests: {', '.join(memory_dict.get('interests', []))}\\n\"\n",
    "        )\n",
    "    else:\n",
    "        formatted_memory = None\n",
    "\n",
    "    system_message: str = MODEL_SYSTEM_MESSAGE.format(memory=formatted_memory)\n",
    "    # Respond using memory + chat history\n",
    "    messages = to_openai_messages(\n",
    "        [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    )\n",
    "    response = await get_llm_response(\n",
    "        messages=messages,\n",
    "        response_model=None,\n",
    "        model=ModelEnum.LLAMA_3p2_3B_INSTRUCT_REMOTE.value,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "\n",
    "    return {\"messages\": [AIMessage(content=content)]}\n",
    "\n",
    "\n",
    "async def write_memory(\n",
    "    state: MessageState, config: RunnableConfig, store: BaseStore\n",
    ") -> None:\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    prefix: str = \"memory\"\n",
    "    key = \"user_memory\"\n",
    "    namespace = (prefix, user_id)\n",
    "    existing_memory = store.get(namespace, key)\n",
    "    # existing_memory = await store.aget(namespace, key)\n",
    "\n",
    "    if existing_memory and existing_memory.value:\n",
    "        memory_dict = existing_memory.value\n",
    "        formatted_memory = (\n",
    "            f\"Name: {memory_dict.get('user_name', 'Unknown')}\\n\"\n",
    "            f\"Interests: {', '.join(memory_dict.get('interests', []))}\\n\"\n",
    "        )\n",
    "    else:\n",
    "        formatted_memory = None\n",
    "\n",
    "    system_message: str = CREATE_MEMORY_INSTRUCTION.format(memory=formatted_memory)\n",
    "    # Respond using memory + chat history\n",
    "    messages = to_openai_messages(\n",
    "        [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    )\n",
    "    new_memory = await get_llm_response(\n",
    "        messages=messages,\n",
    "        response_model=UserProfile,\n",
    "        model=ModelEnum.LLAMA_3p2_3B_INSTRUCT_REMOTE.value,\n",
    "    )\n",
    "\n",
    "    # Update existing memory\n",
    "    # await store.aput(namespace, key, {prefix: new_memory.content})\n",
    "    store.put(namespace, key, {prefix: new_memory.model_dump()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11b8d45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = InMemoryStore()\n",
    "\n",
    "namespace = (\"memory\", \"1\")\n",
    "key: str = \"sample_memory\"\n",
    "value: dict[str, Any] = {\"user_name\": \"Neidu\", \"interests\": [\"LangGraph\"]}\n",
    "\n",
    "mem.put(namespace, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90baedbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[00:32:02] </span><span style=\"font-weight: bold\">[</span>                                                                                        <a href=\"file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_33465/1390856903.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1390856903.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_33465/1390856903.py#5\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Item</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">namespace</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'memory'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">key</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'sample_memory'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">value</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'user_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Neidu'</span>,    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000\">'interests'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'LangGraph'</span><span style=\"font-weight: bold\">]}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">created_at</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2025-06-15T23:32:02.255817+00:00'</span>,              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #808000; text-decoration-color: #808000\">updated_at</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2025-06-15T23:32:02.255823+00:00'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">score</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">]</span>                                                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[00:32:02]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0m                                                                                        \u001b]8;id=996182;file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_33465/1390856903.py\u001b\\\u001b[2m1390856903.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=70412;file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_33465/1390856903.py#5\u001b\\\u001b[2m5\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m    \u001b[1;35mItem\u001b[0m\u001b[1m(\u001b[0m\u001b[33mnamespace\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'memory'\u001b[0m, \u001b[32m'1'\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mkey\u001b[0m=\u001b[32m'sample_memory'\u001b[0m, \u001b[33mvalue\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'user_name'\u001b[0m: \u001b[32m'Neidu'\u001b[0m,    \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[32m'interests'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'LangGraph'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m, \u001b[33mcreated_at\u001b[0m=\u001b[32m'2025-06-15T23:32:02.255817+00:00'\u001b[0m,              \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[33mupdated_at\u001b[0m=\u001b[32m'2025-06-15T23:32:02.255823+00:00'\u001b[0m, \u001b[33mscore\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m                               \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1m]\u001b[0m                                                                                        \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "namespace = (\"memory\", \"1\")\n",
    "key: str = \"sample_memory\"\n",
    "\n",
    "ex_memory = mem.search(namespace)\n",
    "console.log(ex_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8300654a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user_name': 'Neidu', 'interests': ['LangGraph']}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[row.value for row in ex_memory]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa643e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86af8a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def call_llm(\n",
    "    state: MessageState, config: RunnableConfig, store: BaseStore\n",
    ") -> dict[str, Any]:\n",
    "    # Get the user id\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Get the memory for the user\n",
    "    prefix: str = \"memory\"\n",
    "    namespace = (prefix, user_id)\n",
    "    existing_memory = store.search(namespace)\n",
    "\n",
    "    formatted_memory = (\n",
    "        \"\\n\".join([f\"- {memory.value['memory']}\" for memory in existing_memory])\n",
    "        if existing_memory\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    system_message: str = MODEL_SYSTEM_MESSAGE.format(memory=formatted_memory)\n",
    "    # Respond using memory + chat history\n",
    "    messages = to_openai_messages(\n",
    "        [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    )\n",
    "    response = await get_llm_response(\n",
    "        messages=messages,\n",
    "        response_model=None,\n",
    "        model=ModelEnum.QWEN_3p0_8B_REMOTE.value,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "\n",
    "    return {\"messages\": [AIMessage(content=content)]}\n",
    "\n",
    "\n",
    "async def write_memory(\n",
    "    state: MessageState, config: RunnableConfig, store: BaseStore\n",
    ") -> None:\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    prefix: str = \"memory\"\n",
    "    key = \"user_memory\"\n",
    "    namespace = (prefix, user_id)\n",
    "    existing_memory = store.search(namespace)\n",
    "\n",
    "    formatted_memory = (\n",
    "        \"\\n\".join([f\"- {memory.value['memory']}\" for memory in existing_memory])\n",
    "        if existing_memory\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    system_message: str = CREATE_MEMORY_INSTRUCTION.format(memory=formatted_memory)\n",
    "    # Respond using memory + chat history\n",
    "    messages = to_openai_messages(\n",
    "        [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    )\n",
    "    new_memory = await get_llm_response(\n",
    "        messages=messages,\n",
    "        response_model=MemoryCollection,\n",
    "        model=ModelEnum.QWEN_3p0_8B_REMOTE.value,\n",
    "    )\n",
    "\n",
    "    # Update existing memory\n",
    "    # await store.aput(namespace, key, {prefix: new_memory.content})\n",
    "    store.put(namespace, key, {prefix: new_memory.model_dump()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d7bcb35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graph\n",
    "graph_builder = StateGraph(MessageState)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node(\"call_llm\", call_llm)\n",
    "graph_builder.add_node(\"write_memory\", write_memory)\n",
    "\n",
    "# Add edges\n",
    "graph_builder.add_edge(START, \"call_llm\")\n",
    "graph_builder.add_edge(\"call_llm\", \"write_memory\")\n",
    "graph_builder.add_edge(\"write_memory\", END)\n",
    "\n",
    "# Long-term-memory store (across threads)\n",
    "across_thread_memory = InMemoryStore()\n",
    "# Short-term-memory store (within a thread) checkpointer\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# Compile\n",
    "graph = graph_builder.compile(\n",
    "    checkpointer=within_thread_memory, store=across_thread_memory\n",
    ").with_config(run_name=\"chatbot-with-structured-memory\")\n",
    "\n",
    "# Visualize\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9df8582",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_chat(inputs: list[str]):\n",
    "    state = {\"messages\": [HumanMessage(content=msg) for msg in inputs]}\n",
    "    config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "    async for event in graph.astream(state, config=config, stream_mode=\"values\"):\n",
    "        for msg in event[\"messages\"]:\n",
    "            msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f5eb6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Neidu\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Neidu\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Neidu! How can I assist you today? 😊\n"
     ]
    }
   ],
   "source": [
    "inputs: list[str] = [\"Hi, I'm Neidu\"]\n",
    "graph_input = {\"messages\": [HumanMessage(content=msg) for msg in inputs]}\n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "async for event in graph.astream(graph_input, config=config, stream_mode=\"values\"):\n",
    "    for msg in event[\"messages\"]:\n",
    "        msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e20eb8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Neidu\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Neidu! How can I assist you today? 😊\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What do you know about LangGraph?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Neidu\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Neidu! How can I assist you today? 😊\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What do you know about LangGraph?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "LangGraph is a framework developed by the LangChain team to help build structured applications using language models. It allows users to define workflows as graphs, where nodes represent individual operations (like model calls, data processing, or tool usage) and edges represent the flow of data between them. This makes it easier to manage complex interactions, such as multi-step reasoning, state transitions, or orchestrating multiple models in a sequence. \n",
      "\n",
      "LangGraph is part of the broader **LangChain** ecosystem, which also includes tools like **LangSmith** (for testing and debugging) and **LangChain** itself (for creating chains of model interactions). It’s designed to be flexible, enabling developers to create custom applications that leverage language models for tasks like chatbots, data analysis, or automation. \n",
      "\n",
      "If you're looking to explore its specific features or use cases, feel free to ask! 😊\n"
     ]
    }
   ],
   "source": [
    "inputs: list[str] = [\"What do you know about LangGraph?\"]\n",
    "graph_input = {\"messages\": [HumanMessage(content=msg) for msg in inputs]}\n",
    "\n",
    "async for event in graph.astream(graph_input, config=config, stream_mode=\"values\"):\n",
    "    for msg in event[\"messages\"]:\n",
    "        msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27a2977a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Neidu\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Neidu! How can I assist you today? 😊\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What do you know about LangGraph?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "LangGraph is a framework developed by the LangChain team to help build structured applications using language models. It allows users to define workflows as graphs, where nodes represent individual operations (like model calls, data processing, or tool usage) and edges represent the flow of data between them. This makes it easier to manage complex interactions, such as multi-step reasoning, state transitions, or orchestrating multiple models in a sequence. \n",
      "\n",
      "LangGraph is part of the broader **LangChain** ecosystem, which also includes tools like **LangSmith** (for testing and debugging) and **LangChain** itself (for creating chains of model interactions). It’s designed to be flexible, enabling developers to create custom applications that leverage language models for tasks like chatbots, data analysis, or automation. \n",
      "\n",
      "If you're looking to explore its specific features or use cases, feel free to ask! 😊\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How can I create AI agentic workflows?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Neidu\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Neidu! How can I assist you today? 😊\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What do you know about LangGraph?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "LangGraph is a framework developed by the LangChain team to help build structured applications using language models. It allows users to define workflows as graphs, where nodes represent individual operations (like model calls, data processing, or tool usage) and edges represent the flow of data between them. This makes it easier to manage complex interactions, such as multi-step reasoning, state transitions, or orchestrating multiple models in a sequence. \n",
      "\n",
      "LangGraph is part of the broader **LangChain** ecosystem, which also includes tools like **LangSmith** (for testing and debugging) and **LangChain** itself (for creating chains of model interactions). It’s designed to be flexible, enabling developers to create custom applications that leverage language models for tasks like chatbots, data analysis, or automation. \n",
      "\n",
      "If you're looking to explore its specific features or use cases, feel free to ask! 😊\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How can I create AI agentic workflows?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Creating AI agentic workflows with **LangGraph** involves designing a structured, modular system where AI agents can interact, make decisions, and handle tasks independently. Here's a high-level overview of how to approach it:\n",
      "\n",
      "---\n",
      "\n",
      "### **Key Concepts in LangGraph for Agentic Workflows**\n",
      "1. **Nodes as Tasks**  \n",
      "   Each node in the graph represents a specific task or operation (e.g., calling a language model, using a tool, or processing data). These nodes can be:\n",
      "   - **Model calls**: Invoking LLMs for reasoning or generation.\n",
      "   - **Tool calls**: Integrating external APIs or utilities (e.g., databases, web search).\n",
      "   - **Decision points**: Conditional logic to route workflows based on outputs.\n",
      "   - **State management**: Tracking context or variables across steps.\n",
      "\n",
      "2. **Edges as Control Flow**  \n",
      "   Edges define how data flows between nodes. For example:\n",
      "   - A node might pass its output to another node for further processing.\n",
      "   - Conditional edges can direct the workflow based on specific criteria (e.g., \"if the answer is uncertain, call a tool for clarification\").\n",
      "\n",
      "3. **Stateful Execution**  \n",
      "   LangGraph supports **stateful workflows**, allowing agents to maintain memory of previous interactions. This is crucial for tasks like multi-turn conversations or iterative problem-solving.\n",
      "\n",
      "4. **Agent Integration**  \n",
      "   You can embed agents (e.g., autonomous actors using LLMs) into the graph. Agents can:\n",
      "   - Make decisions based on their training or rules.\n",
      "   - Trigger specific nodes or modify the workflow dynamically.\n",
      "\n",
      "---\n",
      "\n",
      "### **Steps to Build an Agentic Workflow**\n",
      "1. **Define the Workflow Structure**  \n",
      "   Use `Graph` or `StateGraph` from LangGraph to outline the sequence of tasks. For example:\n",
      "   ```python\n",
      "   from langgraph.graph import StateGraph, MessagesState\n",
      "\n",
      "   workflow = StateGraph(MessagesState)\n",
      "   ```\n",
      "\n",
      "2. **Create Nodes for Tasks**  \n",
      "   Write functions for each task (e.g., `answer_question`, `search_web`, `validate_response`). These functions can be:\n",
      "   - **Model-based**: Using LLMs via `langchain` or `llamaindex`.\n",
      "   - **Tool-based**: Calling external APIs or utilities.\n",
      "   - **Decision-based**: Implementing logic to choose the next step.\n",
      "\n",
      "3. **Connect Nodes with Edges**  \n",
      "   Use `add_edge` to define how nodes interact. For example:\n",
      "   ```python\n",
      "   workflow.add_edge(\"answer_question\", \"validate_response\")\n",
      "   ```\n",
      "\n",
      "4. **Add Conditional Logic**  \n",
      "   Use `ConditionalEdge` to route workflows based on outputs. For instance:\n",
      "   ```python\n",
      "   from langgraph.checkpoint import Checkpoint\n",
      "\n",
      "   def should_search(state):\n",
      "       if \"uncertain\" in state[\"response\"]:\n",
      "           return \"search_web\"\n",
      "       return \"end\"\n",
      "\n",
      "   workflow.add_conditional_edges(\"validate_response\", should_search)\n",
      "   ```\n",
      "\n",
      "5. **Run the Workflow**  \n",
      "   Execute the graph with input data and observe the output:\n",
      "   ```python\n",
      "   from langgraph.graph import run_graph\n",
      "\n",
      "   result = run_graph(workflow, input={\"question\": \"What is the capital of France?\"})\n",
      "   ```\n",
      "\n",
      "---\n",
      "\n",
      "### **Example Use Case**\n",
      "Imagine an agent that answers questions by first using an LLM to generate an answer, then validating it with a tool (e.g., a fact-check API), and if uncertain, searching the web for more information:\n",
      "```python\n",
      "def answer_question(state):\n",
      "    # Call LLM to generate an answer\n",
      "    return {\"response\": \"Paris\"}\n",
      "\n",
      "def validate_response(state):\n",
      "    # Check if the answer is uncertain\n",
      "    if state[\"response\"] == \"Paris\":\n"
     ]
    }
   ],
   "source": [
    "inputs: list[str] = [\"How can I create AI agentic workflows?\"]\n",
    "graph_input = {\"messages\": [HumanMessage(content=msg) for msg in inputs]}\n",
    "\n",
    "async for event in graph.astream(graph_input, config=config, stream_mode=\"values\"):\n",
    "    for msg in event[\"messages\"]:\n",
    "        msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed64ea51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Neidu\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Neidu! How can I assist you today? 😊\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What do you know about LangGraph?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "LangGraph is a framework developed by the LangChain team to help build structured applications using language models. It allows users to define workflows as graphs, where nodes represent individual operations (like model calls, data processing, or tool usage) and edges represent the flow of data between them. This makes it easier to manage complex interactions, such as multi-step reasoning, state transitions, or orchestrating multiple models in a sequence. \n",
      "\n",
      "LangGraph is part of the broader **LangChain** ecosystem, which also includes tools like **LangSmith** (for testing and debugging) and **LangChain** itself (for creating chains of model interactions). It’s designed to be flexible, enabling developers to create custom applications that leverage language models for tasks like chatbots, data analysis, or automation. \n",
      "\n",
      "If you're looking to explore its specific features or use cases, feel free to ask! 😊\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How can I create AI agentic workflows?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Creating AI agentic workflows with **LangGraph** involves designing a structured, modular system where AI agents can interact, make decisions, and handle tasks independently. Here's a high-level overview of how to approach it:\n",
      "\n",
      "---\n",
      "\n",
      "### **Key Concepts in LangGraph for Agentic Workflows**\n",
      "1. **Nodes as Tasks**  \n",
      "   Each node in the graph represents a specific task or operation (e.g., calling a language model, using a tool, or processing data). These nodes can be:\n",
      "   - **Model calls**: Invoking LLMs for reasoning or generation.\n",
      "   - **Tool calls**: Integrating external APIs or utilities (e.g., databases, web search).\n",
      "   - **Decision points**: Conditional logic to route workflows based on outputs.\n",
      "   - **State management**: Tracking context or variables across steps.\n",
      "\n",
      "2. **Edges as Control Flow**  \n",
      "   Edges define how data flows between nodes. For example:\n",
      "   - A node might pass its output to another node for further processing.\n",
      "   - Conditional edges can direct the workflow based on specific criteria (e.g., \"if the answer is uncertain, call a tool for clarification\").\n",
      "\n",
      "3. **Stateful Execution**  \n",
      "   LangGraph supports **stateful workflows**, allowing agents to maintain memory of previous interactions. This is crucial for tasks like multi-turn conversations or iterative problem-solving.\n",
      "\n",
      "4. **Agent Integration**  \n",
      "   You can embed agents (e.g., autonomous actors using LLMs) into the graph. Agents can:\n",
      "   - Make decisions based on their training or rules.\n",
      "   - Trigger specific nodes or modify the workflow dynamically.\n",
      "\n",
      "---\n",
      "\n",
      "### **Steps to Build an Agentic Workflow**\n",
      "1. **Define the Workflow Structure**  \n",
      "   Use `Graph` or `StateGraph` from LangGraph to outline the sequence of tasks. For example:\n",
      "   ```python\n",
      "   from langgraph.graph import StateGraph, MessagesState\n",
      "\n",
      "   workflow = StateGraph(MessagesState)\n",
      "   ```\n",
      "\n",
      "2. **Create Nodes for Tasks**  \n",
      "   Write functions for each task (e.g., `answer_question`, `search_web`, `validate_response`). These functions can be:\n",
      "   - **Model-based**: Using LLMs via `langchain` or `llamaindex`.\n",
      "   - **Tool-based**: Calling external APIs or utilities.\n",
      "   - **Decision-based**: Implementing logic to choose the next step.\n",
      "\n",
      "3. **Connect Nodes with Edges**  \n",
      "   Use `add_edge` to define how nodes interact. For example:\n",
      "   ```python\n",
      "   workflow.add_edge(\"answer_question\", \"validate_response\")\n",
      "   ```\n",
      "\n",
      "4. **Add Conditional Logic**  \n",
      "   Use `ConditionalEdge` to route workflows based on outputs. For instance:\n",
      "   ```python\n",
      "   from langgraph.checkpoint import Checkpoint\n",
      "\n",
      "   def should_search(state):\n",
      "       if \"uncertain\" in state[\"response\"]:\n",
      "           return \"search_web\"\n",
      "       return \"end\"\n",
      "\n",
      "   workflow.add_conditional_edges(\"validate_response\", should_search)\n",
      "   ```\n",
      "\n",
      "5. **Run the Workflow**  \n",
      "   Execute the graph with input data and observe the output:\n",
      "   ```python\n",
      "   from langgraph.graph import run_graph\n",
      "\n",
      "   result = run_graph(workflow, input={\"question\": \"What is the capital of France?\"})\n",
      "   ```\n",
      "\n",
      "---\n",
      "\n",
      "### **Example Use Case**\n",
      "Imagine an agent that answers questions by first using an LLM to generate an answer, then validating it with a tool (e.g., a fact-check API), and if uncertain, searching the web for more information:\n",
      "```python\n",
      "def answer_question(state):\n",
      "    # Call LLM to generate an answer\n",
      "    return {\"response\": \"Paris\"}\n",
      "\n",
      "def validate_response(state):\n",
      "    # Check if the answer is uncertain\n",
      "    if state[\"response\"] == \"Paris\":\n"
     ]
    }
   ],
   "source": [
    "# Chat history\n",
    "state = graph.get_state(config=config).values\n",
    "\n",
    "for m in state[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[00:35:47] </span><span style=\"font-weight: bold\">[</span>                                                                                        <a href=\"file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_33465/1731364959.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1731364959.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_33465/1731364959.py#6\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Item</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">namespace</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'memory'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">key</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'user_memory'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">value</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'memory'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'memories'</span>:     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Hi, I'm Neidu\"</span><span style=\"font-weight: bold\">}</span>, <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'What do you know about LangGraph?'</span><span style=\"font-weight: bold\">}</span>,         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'How can I create AI agentic workflows?'</span><span style=\"font-weight: bold\">}]}}</span>,                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #808000; text-decoration-color: #808000\">created_at</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2025-06-15T23:35:41.117521+00:00'</span>,                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #808000; text-decoration-color: #808000\">updated_at</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2025-06-15T23:35:41.117528+00:00'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">score</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">]</span>                                                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[00:35:47]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0m                                                                                        \u001b]8;id=719960;file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_33465/1731364959.py\u001b\\\u001b[2m1731364959.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=794815;file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_33465/1731364959.py#6\u001b\\\u001b[2m6\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m    \u001b[1;35mItem\u001b[0m\u001b[1m(\u001b[0m\u001b[33mnamespace\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'memory'\u001b[0m, \u001b[32m'1'\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mkey\u001b[0m=\u001b[32m'user_memory'\u001b[0m, \u001b[33mvalue\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'memory'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'memories'\u001b[0m:     \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'content'\u001b[0m: \u001b[32m\"Hi, I'm Neidu\"\u001b[0m\u001b[1m}\u001b[0m, \u001b[1m{\u001b[0m\u001b[32m'content'\u001b[0m: \u001b[32m'What do you know about LangGraph?'\u001b[0m\u001b[1m}\u001b[0m,         \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1m{\u001b[0m\u001b[32m'content'\u001b[0m: \u001b[32m'How can I create AI agentic workflows?'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,                                \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[33mcreated_at\u001b[0m=\u001b[32m'2025-06-15T23:35:41.117521+00:00'\u001b[0m,                                           \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[33mupdated_at\u001b[0m=\u001b[32m'2025-06-15T23:35:41.117528+00:00'\u001b[0m, \u001b[33mscore\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m                               \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1m]\u001b[0m                                                                                        \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# namespace for the memory to save\n",
    "user_id: str = \"1\"\n",
    "prefix: str = \"memory\"\n",
    "namespace = (prefix, user_id)\n",
    "existing_memory = across_thread_memory.search(namespace)\n",
    "console.log(existing_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a23e995d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Neidu\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Neidu! How can I assist you today? 😊\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What do you know about LangGraph?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "LangGraph is a framework developed by the LangChain team to help build structured applications using language models. It allows users to define workflows as graphs, where nodes represent individual operations (like model calls, data processing, or tool usage) and edges represent the flow of data between them. This makes it easier to manage complex interactions, such as multi-step reasoning, state transitions, or orchestrating multiple models in a sequence. \n",
      "\n",
      "LangGraph is part of the broader **LangChain** ecosystem, which also includes tools like **LangSmith** (for testing and debugging) and **LangChain** itself (for creating chains of model interactions). It’s designed to be flexible, enabling developers to create custom applications that leverage language models for tasks like chatbots, data analysis, or automation. \n",
      "\n",
      "If you're looking to explore its specific features or use cases, feel free to ask! 😊\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How can I create AI agentic workflows?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Creating AI agentic workflows with **LangGraph** involves designing a structured, modular system where AI agents can interact, make decisions, and handle tasks independently. Here's a high-level overview of how to approach it:\n",
      "\n",
      "---\n",
      "\n",
      "### **Key Concepts in LangGraph for Agentic Workflows**\n",
      "1. **Nodes as Tasks**  \n",
      "   Each node in the graph represents a specific task or operation (e.g., calling a language model, using a tool, or processing data). These nodes can be:\n",
      "   - **Model calls**: Invoking LLMs for reasoning or generation.\n",
      "   - **Tool calls**: Integrating external APIs or utilities (e.g., databases, web search).\n",
      "   - **Decision points**: Conditional logic to route workflows based on outputs.\n",
      "   - **State management**: Tracking context or variables across steps.\n",
      "\n",
      "2. **Edges as Control Flow**  \n",
      "   Edges define how data flows between nodes. For example:\n",
      "   - A node might pass its output to another node for further processing.\n",
      "   - Conditional edges can direct the workflow based on specific criteria (e.g., \"if the answer is uncertain, call a tool for clarification\").\n",
      "\n",
      "3. **Stateful Execution**  \n",
      "   LangGraph supports **stateful workflows**, allowing agents to maintain memory of previous interactions. This is crucial for tasks like multi-turn conversations or iterative problem-solving.\n",
      "\n",
      "4. **Agent Integration**  \n",
      "   You can embed agents (e.g., autonomous actors using LLMs) into the graph. Agents can:\n",
      "   - Make decisions based on their training or rules.\n",
      "   - Trigger specific nodes or modify the workflow dynamically.\n",
      "\n",
      "---\n",
      "\n",
      "### **Steps to Build an Agentic Workflow**\n",
      "1. **Define the Workflow Structure**  \n",
      "   Use `Graph` or `StateGraph` from LangGraph to outline the sequence of tasks. For example:\n",
      "   ```python\n",
      "   from langgraph.graph import StateGraph, MessagesState\n",
      "\n",
      "   workflow = StateGraph(MessagesState)\n",
      "   ```\n",
      "\n",
      "2. **Create Nodes for Tasks**  \n",
      "   Write functions for each task (e.g., `answer_question`, `search_web`, `validate_response`). These functions can be:\n",
      "   - **Model-based**: Using LLMs via `langchain` or `llamaindex`.\n",
      "   - **Tool-based**: Calling external APIs or utilities.\n",
      "   - **Decision-based**: Implementing logic to choose the next step.\n",
      "\n",
      "3. **Connect Nodes with Edges**  \n",
      "   Use `add_edge` to define how nodes interact. For example:\n",
      "   ```python\n",
      "   workflow.add_edge(\"answer_question\", \"validate_response\")\n",
      "   ```\n",
      "\n",
      "4. **Add Conditional Logic**  \n",
      "   Use `ConditionalEdge` to route workflows based on outputs. For instance:\n",
      "   ```python\n",
      "   from langgraph.checkpoint import Checkpoint\n",
      "\n",
      "   def should_search(state):\n",
      "       if \"uncertain\" in state[\"response\"]:\n",
      "           return \"search_web\"\n",
      "       return \"end\"\n",
      "\n",
      "   workflow.add_conditional_edges(\"validate_response\", should_search)\n",
      "   ```\n",
      "\n",
      "5. **Run the Workflow**  \n",
      "   Execute the graph with input data and observe the output:\n",
      "   ```python\n",
      "   from langgraph.graph import run_graph\n",
      "\n",
      "   result = run_graph(workflow, input={\"question\": \"What is the capital of France?\"})\n",
      "   ```\n",
      "\n",
      "---\n",
      "\n",
      "### **Example Use Case**\n",
      "Imagine an agent that answers questions by first using an LLM to generate an answer, then validating it with a tool (e.g., a fact-check API), and if uncertain, searching the web for more information:\n",
      "```python\n",
      "def answer_question(state):\n",
      "    # Call LLM to generate an answer\n",
      "    return {\"response\": \"Paris\"}\n",
      "\n",
      "def validate_response(state):\n",
      "    # Check if the answer is uncertain\n",
      "    if state[\"response\"] == \"Paris\":\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What was my first question?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Neidu\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Neidu! How can I assist you today? 😊\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What do you know about LangGraph?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "LangGraph is a framework developed by the LangChain team to help build structured applications using language models. It allows users to define workflows as graphs, where nodes represent individual operations (like model calls, data processing, or tool usage) and edges represent the flow of data between them. This makes it easier to manage complex interactions, such as multi-step reasoning, state transitions, or orchestrating multiple models in a sequence. \n",
      "\n",
      "LangGraph is part of the broader **LangChain** ecosystem, which also includes tools like **LangSmith** (for testing and debugging) and **LangChain** itself (for creating chains of model interactions). It’s designed to be flexible, enabling developers to create custom applications that leverage language models for tasks like chatbots, data analysis, or automation. \n",
      "\n",
      "If you're looking to explore its specific features or use cases, feel free to ask! 😊\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How can I create AI agentic workflows?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Creating AI agentic workflows with **LangGraph** involves designing a structured, modular system where AI agents can interact, make decisions, and handle tasks independently. Here's a high-level overview of how to approach it:\n",
      "\n",
      "---\n",
      "\n",
      "### **Key Concepts in LangGraph for Agentic Workflows**\n",
      "1. **Nodes as Tasks**  \n",
      "   Each node in the graph represents a specific task or operation (e.g., calling a language model, using a tool, or processing data). These nodes can be:\n",
      "   - **Model calls**: Invoking LLMs for reasoning or generation.\n",
      "   - **Tool calls**: Integrating external APIs or utilities (e.g., databases, web search).\n",
      "   - **Decision points**: Conditional logic to route workflows based on outputs.\n",
      "   - **State management**: Tracking context or variables across steps.\n",
      "\n",
      "2. **Edges as Control Flow**  \n",
      "   Edges define how data flows between nodes. For example:\n",
      "   - A node might pass its output to another node for further processing.\n",
      "   - Conditional edges can direct the workflow based on specific criteria (e.g., \"if the answer is uncertain, call a tool for clarification\").\n",
      "\n",
      "3. **Stateful Execution**  \n",
      "   LangGraph supports **stateful workflows**, allowing agents to maintain memory of previous interactions. This is crucial for tasks like multi-turn conversations or iterative problem-solving.\n",
      "\n",
      "4. **Agent Integration**  \n",
      "   You can embed agents (e.g., autonomous actors using LLMs) into the graph. Agents can:\n",
      "   - Make decisions based on their training or rules.\n",
      "   - Trigger specific nodes or modify the workflow dynamically.\n",
      "\n",
      "---\n",
      "\n",
      "### **Steps to Build an Agentic Workflow**\n",
      "1. **Define the Workflow Structure**  \n",
      "   Use `Graph` or `StateGraph` from LangGraph to outline the sequence of tasks. For example:\n",
      "   ```python\n",
      "   from langgraph.graph import StateGraph, MessagesState\n",
      "\n",
      "   workflow = StateGraph(MessagesState)\n",
      "   ```\n",
      "\n",
      "2. **Create Nodes for Tasks**  \n",
      "   Write functions for each task (e.g., `answer_question`, `search_web`, `validate_response`). These functions can be:\n",
      "   - **Model-based**: Using LLMs via `langchain` or `llamaindex`.\n",
      "   - **Tool-based**: Calling external APIs or utilities.\n",
      "   - **Decision-based**: Implementing logic to choose the next step.\n",
      "\n",
      "3. **Connect Nodes with Edges**  \n",
      "   Use `add_edge` to define how nodes interact. For example:\n",
      "   ```python\n",
      "   workflow.add_edge(\"answer_question\", \"validate_response\")\n",
      "   ```\n",
      "\n",
      "4. **Add Conditional Logic**  \n",
      "   Use `ConditionalEdge` to route workflows based on outputs. For instance:\n",
      "   ```python\n",
      "   from langgraph.checkpoint import Checkpoint\n",
      "\n",
      "   def should_search(state):\n",
      "       if \"uncertain\" in state[\"response\"]:\n",
      "           return \"search_web\"\n",
      "       return \"end\"\n",
      "\n",
      "   workflow.add_conditional_edges(\"validate_response\", should_search)\n",
      "   ```\n",
      "\n",
      "5. **Run the Workflow**  \n",
      "   Execute the graph with input data and observe the output:\n",
      "   ```python\n",
      "   from langgraph.graph import run_graph\n",
      "\n",
      "   result = run_graph(workflow, input={\"question\": \"What is the capital of France?\"})\n",
      "   ```\n",
      "\n",
      "---\n",
      "\n",
      "### **Example Use Case**\n",
      "Imagine an agent that answers questions by first using an LLM to generate an answer, then validating it with a tool (e.g., a fact-check API), and if uncertain, searching the web for more information:\n",
      "```python\n",
      "def answer_question(state):\n",
      "    # Call LLM to generate an answer\n",
      "    return {\"response\": \"Paris\"}\n",
      "\n",
      "def validate_response(state):\n",
      "    # Check if the answer is uncertain\n",
      "    if state[\"response\"] == \"Paris\":\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What was my first question?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your first question was **\"What do you know about LangGraph?\"** 😊 Let me know if you'd like a deeper dive into that topic!\n"
     ]
    }
   ],
   "source": [
    "inputs: list[str] = [\"What was my first question?\"]\n",
    "graph_input = {\"messages\": [HumanMessage(content=msg) for msg in inputs]}\n",
    "\n",
    "async for event in graph.astream(graph_input, config=config, stream_mode=\"values\"):\n",
    "    for msg in event[\"messages\"]:\n",
    "        msg.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba14e8c",
   "metadata": {},
   "source": [
    "### Create A New Session (Thread)\n",
    "\n",
    "- Because the graph uses long-term-memory, it can retain user preferences across multiple chat sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf16c412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What do you know about me?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What do you know about me?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I know you're Neidu, and you've shown interest in topics like LangGraph and creating AI agentic workflows. How can I assist you further with those areas?\n"
     ]
    }
   ],
   "source": [
    "inputs: list[str] = [\"What do you know about me?\"]\n",
    "graph_input = {\"messages\": [HumanMessage(content=msg) for msg in inputs]}\n",
    "config_2 = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "\n",
    "async for event in graph.astream(graph_input, config=config_2, stream_mode=\"values\"):\n",
    "    for msg in event[\"messages\"]:\n",
    "        msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a72976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bc140c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da40950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
