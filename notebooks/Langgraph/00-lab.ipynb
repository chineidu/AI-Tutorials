{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc0ce639",
   "metadata": {},
   "source": [
    "# Lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b2aa0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import (\n",
    "    Annotated,\n",
    "    Any,\n",
    "    Generator,\n",
    "    Iterable,\n",
    "    Literal,\n",
    "    Optional,\n",
    "    TypedDict,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "# Standard imports\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"white\": \"#FFFFFF\",  # Bright white\n",
    "        \"info\": \"#00FF00\",  # Bright green\n",
    "        \"warning\": \"#FFD700\",  # Bright gold\n",
    "        \"error\": \"#FF1493\",  # Deep pink\n",
    "        \"success\": \"#00FFFF\",  # Cyan\n",
    "        \"highlight\": \"#FF4500\",  # Orange-red\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "# import matplotlib.pyplot as pltife\n",
    "\n",
    "# NumPy settings\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "# Polars settings\n",
    "pl.Config.set_fmt_str_lengths(1_000)\n",
    "pl.Config.set_tbl_cols(n=1_000)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_up_from_current_directory(*, go_up: int = 1) -> None:\n",
    "    \"\"\"This is used to up a number of directories.\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "    go_up: int, default=1\n",
    "        This indicates the number of times to go back up from the current directory.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    CONST: str = \"../\"\n",
    "    NUM: str = CONST * go_up\n",
    "\n",
    "    # Goto the previous directory\n",
    "    prev_directory = os.path.join(os.path.dirname(__name__), NUM)\n",
    "    # Get the 'absolute path' of the previous directory\n",
    "    abs_path_prev_directory = os.path.abspath(prev_directory)\n",
    "\n",
    "    # Add the path to the System paths\n",
    "    sys.path.insert(0, abs_path_prev_directory)\n",
    "    print(abs_path_prev_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d4f1fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/neidu/Desktop/Projects/Personal/My_Projects/AI-Tutorials\n"
     ]
    }
   ],
   "source": [
    "go_up_from_current_directory(go_up=2)\n",
    "\n",
    "\n",
    "from schemas import ModelEnum  # noqa: E402\n",
    "from settings import refresh_settings  # noqa: E402\n",
    "from utilities.client_utils import check_rate_limit  # noqa: E402\n",
    "\n",
    "settings = refresh_settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c625b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    AnyMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "from langchain_core.tools import InjectedToolCallId, tool\n",
    "from langchain_litellm import ChatLiteLLM\n",
    "from langchain_tavily import TavilySearch\n",
    "from langfuse.langchain import CallbackHandler\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.types import Command, interrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2f3f243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type\n",
    "\n",
    "import instructor\n",
    "from openai import AsyncOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "aclient = instructor.from_openai(\n",
    "    AsyncOpenAI(\n",
    "        api_key=settings.OPENROUTER_API_KEY.get_secret_value(),\n",
    "        base_url=settings.OPENROUTER_URL,\n",
    "    ),\n",
    "    mode=instructor.Mode.JSON,\n",
    ")\n",
    "\n",
    "\n",
    "class MessageState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    name: str = Field(description=\"Name of the user\")\n",
    "    role: str = Field(description=\"Role of the user\")\n",
    "\n",
    "\n",
    "class PersonList(BaseModel):\n",
    "    persons: list[Person] = Field(description=\"List of persons.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7aa10935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[23:06:13] </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PersonList</span><span style=\"font-weight: bold\">(</span>                                                                             <a href=\"file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_28846/1622096579.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1622096579.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_28846/1622096579.py#19\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">19</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>    <span style=\"color: #808000; text-decoration-color: #808000\">persons</span>=<span style=\"font-weight: bold\">[</span>                                                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Person</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Neidu'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'software developer'</span><span style=\"font-weight: bold\">)</span>,                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Person</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Dayo'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'data scientist'</span><span style=\"font-weight: bold\">)</span>                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>    <span style=\"font-weight: bold\">]</span>                                                                                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">)</span>                                                                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[23:06:13]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;35mPersonList\u001b[0m\u001b[1m(\u001b[0m                                                                             \u001b]8;id=429812;file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_28846/1622096579.py\u001b\\\u001b[2m1622096579.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=674919;file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_28846/1622096579.py#19\u001b\\\u001b[2m19\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m    \u001b[33mpersons\u001b[0m=\u001b[1m[\u001b[0m                                                                           \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m        \u001b[1;35mPerson\u001b[0m\u001b[1m(\u001b[0m\u001b[33mname\u001b[0m=\u001b[32m'Neidu'\u001b[0m, \u001b[33mrole\u001b[0m=\u001b[32m'software developer'\u001b[0m\u001b[1m)\u001b[0m,                                \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m        \u001b[1;35mPerson\u001b[0m\u001b[1m(\u001b[0m\u001b[33mname\u001b[0m=\u001b[32m'Dayo'\u001b[0m, \u001b[33mrole\u001b[0m=\u001b[32m'data scientist'\u001b[0m\u001b[1m)\u001b[0m                                      \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m    \u001b[1m]\u001b[0m                                                                                   \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1m)\u001b[0m                                                                                       \u001b[2m                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model: str = ModelEnum.LLAMA_3p2_3B_INSTRUCT_REMOTE.value\n",
    "input_msg: str = (\n",
    "    \"Extract all the persons: My name is Neidu. I am a software developer. \"\n",
    "    \"Hey! I'm Dayo, a data scientist\"\n",
    ")\n",
    "\n",
    "response = await aclient.chat.completions.create(\n",
    "    model=model,\n",
    "    response_model=PersonList,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": input_msg},\n",
    "    ],\n",
    "    max_retries=3,\n",
    "    temperature=0.0,\n",
    "    seed=1,\n",
    ")\n",
    "\n",
    "console.log(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0241ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[21:56:26] </span>Nice to meet you, Neidu! As a software developer, I'm sure you're always looking for    <a href=\"file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_28846/1477099783.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1477099783.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_28846/1477099783.py#13\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">13</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>ways to improve your coding skills and stay up-to-date with the latest technologies. Is <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>there something specific you'd like to chat about or ask for help with? Do you have a   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>project you're working on that you'd like some guidance on? Or perhaps you're looking   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>for recommendations on new tools or resources to learn? I'm here to help!               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[21:56:26]\u001b[0m\u001b[2;36m \u001b[0mNice to meet you, Neidu! As a software developer, I'm sure you're always looking for    \u001b]8;id=284819;file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_28846/1477099783.py\u001b\\\u001b[2m1477099783.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=120536;file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_28846/1477099783.py#13\u001b\\\u001b[2m13\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0mways to improve your coding skills and stay up-to-date with the latest technologies. Is \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0mthere something specific you'd like to chat about or ask for help with? Do you have a   \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0mproject you're working on that you'd like some guidance on? Or perhaps you're looking   \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m           \u001b[0mfor recommendations on new tools or resources to learn? I'm here to help!               \u001b[2m                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = await aclient.chat.completions.create(\n",
    "    model=model,\n",
    "    response_model=None,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": input_msg},\n",
    "    ],\n",
    "    max_retries=3,\n",
    "    temperature=0.0,\n",
    "    seed=1,\n",
    ")\n",
    "\n",
    "console.log(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c499cc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessageState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "\n",
    "# Format LangGraph messages for OpenAI\n",
    "def to_openai_messages(messages: list[AnyMessage]) -> list[dict[str, str]]:\n",
    "    formatted = []\n",
    "    for msg in messages:\n",
    "        if isinstance(msg, SystemMessage):\n",
    "            formatted.append({\"role\": \"system\", \"content\": msg.content})\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            formatted.append({\"role\": \"assistant\", \"content\": msg.content})\n",
    "        elif isinstance(msg, ToolMessage):\n",
    "            formatted.append(\n",
    "                {\n",
    "                    \"role\": \"tool\",\n",
    "                    \"content\": msg.content,\n",
    "                    \"tool_call_id\": msg.tool_call_id,\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            formatted.append({\"role\": \"user\", \"content\": msg.content})\n",
    "    return formatted\n",
    "\n",
    "\n",
    "# Chatbot node\n",
    "async def chatbot(state: MessageState) -> dict[str, Any]:\n",
    "    messages = [SystemMessage(content=\"You are a helpful assistant.\")] + state[\"messages\"]\n",
    "    openai_messages = to_openai_messages(messages)\n",
    "    response = await aclient.chat.completions.create(\n",
    "        model=ModelEnum.LLAMA_3p2_3B_INSTRUCT_REMOTE.value,\n",
    "        response_model=None,\n",
    "        messages=openai_messages,\n",
    "        temperature=0.0,\n",
    "        max_tokens=1_200,\n",
    "        seed=42,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    return {\"messages\": [AIMessage(content=content)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a92a03a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph_builder = StateGraph(MessageState)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# Connect nodes\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "# Add memory\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# Compile the graph\n",
    "graph = graph_builder.compile(checkpointer=within_thread_memory).with_config(\n",
    "    run_name=\"simple-chatbot\"\n",
    ")\n",
    "\n",
    "# Visualize the graph\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ba83049",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_chat(inputs: list[str]):\n",
    "    state = {\"messages\": [HumanMessage(content=msg) for msg in inputs]}\n",
    "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "    async for event in graph.astream(state, config=config, stream_mode=\"values\"):\n",
    "        for msg in event[\"messages\"]:\n",
    "            msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cfc3255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Neidu\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Neidu\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Neidu! It's nice to meet you. Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "await run_chat([\"Hi, I'm Neidu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39a9e07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Neidu\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Neidu! It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How can I create AI agentic workflows?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Neidu\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Neidu! It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How can I create AI agentic workflows?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Creating AI-agentic workflows involves designing and implementing systems that enable machines to make decisions and take actions autonomously, while also considering human values and goals. Here are some general steps to help you get started:\n",
      "\n",
      "1. **Define the problem and goals**: Identify the specific problem or task you want the AI system to address. Determine the desired outcomes and the key performance indicators (KPIs) to measure success.\n",
      "2. **Choose an AI framework**: Select a suitable AI framework that aligns with your problem and goals. Popular options include:\n",
      "\t* Rule-based systems\n",
      "\t* Machine learning (ML) and deep learning (DL)\n",
      "\t* Hybrid approaches combining symbolic and connectionist AI\n",
      "3. **Design the workflow**: Break down the problem into smaller, manageable tasks and define the workflow. Identify the inputs, processing steps, and outputs.\n",
      "4. **Integrate AI components**: Incorporate AI components, such as:\n",
      "\t* **Decision-making algorithms**: Use techniques like reinforcement learning, decision trees, or probabilistic models to enable the AI system to make decisions.\n",
      "\t* **Knowledge representation**: Use knowledge graphs, ontologies, or expert systems to represent domain knowledge and provide context for the AI system.\n",
      "\t* **Optimization techniques**: Apply optimization algorithms, such as linear or nonlinear programming, to find the best solutions.\n",
      "5. **Implement human-AI collaboration**: Design mechanisms for human-AI collaboration, such as:\n",
      "\t* **Human-in-the-loop**: Allow humans to review and correct AI decisions\n",
      "\t* **Explainability**: Provide insights into AI decision-making processes to facilitate trust and understanding\n",
      "\t* **Feedback mechanisms**: Establish feedback loops to improve AI performance and adapt to changing conditions\n",
      "6. **Evaluate and refine**: Continuously evaluate the AI system's performance and refine the workflow as needed. This may involve:\n",
      "\t* **Monitoring and logging**: Track system performance and identify areas for improvement\n",
      "\t* **A/B testing**: Compare different versions of the workflow to determine which performs better\n",
      "\t* **Human evaluation**: Assess the system's effectiveness and gather feedback from users\n",
      "7. **Ensure transparency and accountability**: Implement mechanisms to ensure transparency and accountability, such as:\n",
      "\t* **Audit trails**: Maintain records of AI decision-making processes and outcomes\n",
      "\t* **Explainability**: Provide clear explanations for AI decisions and their impact\n",
      "\t* **Governance**: Establish governance structures to oversee AI development and deployment\n",
      "\n",
      "Some popular tools and platforms for creating AI-agentic workflows include:\n",
      "\n",
      "1. **Apache Airflow**: A workflow management platform for automating complex tasks\n",
      "2. **Google Cloud AI Platform**: A suite of AI and machine learning tools for building and deploying AI models\n",
      "3. **Microsoft Azure Machine Learning**: A cloud-based platform for building, training, and deploying machine learning models\n",
      "4. **Robot Operating System (ROS)**: An open-source software framework for building robot applications\n",
      "5. **PyTorch**: A popular open-source machine learning framework for building and training neural networks\n",
      "\n",
      "These are just a few examples, and the specific tools and platforms you choose will depend on your project's requirements and your team's expertise.\n",
      "\n",
      "How can I help you further, Neidu? Do you have a specific project or problem in mind that you'd like to tackle?\n"
     ]
    }
   ],
   "source": [
    "await run_chat([\"How can I create AI agentic workflows?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ffd9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "await run_chat([\"I want to apply it to my data. I want to generate insights from my data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60c1b7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.store.memory import InMemoryStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "987e8eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_llm_response(\n",
    "    messages: list[dict[str, Any]],\n",
    "    response_model: Type[BaseModel] | None,\n",
    "    model: str,\n",
    "    max_tokens: int = 1_200,\n",
    ") -> Type[BaseModel] | None:\n",
    "    return await aclient.chat.completions.create(\n",
    "        model=model,\n",
    "        response_model=response_model,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0.0,\n",
    "        seed=42,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f6ad9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='gen-1750022126-nvb3W7ynyl4p5tKIBRF0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hi Neidu! It's nice to meet you. Is there something I can help you with or would you like to chat?\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=None), native_finish_reason='stop')], created=1750022126, model='meta-llama/llama-3.2-3b-instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=28, prompt_tokens=29, total_tokens=57, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)), provider='Lambda')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages: list[dict[str, Any]] = [\n",
    "    {\"role\": \"system\", \"content\": \"You're a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi, I'm Neidu\"},\n",
    "]\n",
    "\n",
    "\n",
    "await get_llm_response(\n",
    "    messages=messages,\n",
    "    response_model=None,\n",
    "    model=ModelEnum.LLAMA_3p2_3B_INSTRUCT_REMOTE.value,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7c49b9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserProfile(BaseModel):\n",
    "    user_name: str = Field(description=\"User's name\")\n",
    "    interests: list[str] = Field(default_factory=list, description=\"User's interests\")\n",
    "    other_info: str = Field(default=\"\", description=\"Other information about the user\")\n",
    "\n",
    "\n",
    "class Memory(BaseModel):\n",
    "    content: str = Field(description=\"The main content of the memory.\")\n",
    "\n",
    "\n",
    "class MemoryCollection(BaseModel):\n",
    "    memories: list[Memory] = Field(\n",
    "        description=\"A list of memories.\", default_factory=[]\n",
    "    )\n",
    "\n",
    "\n",
    "# Chatbot instruction\n",
    "MODEL_SYSTEM_MESSAGE: str = \"\"\"\n",
    "<system>\n",
    "\n",
    "<role>\n",
    "You are a helpful assistant with memory that provides information about the user.\n",
    "If you have memory for this user, use it to personalize your responses.\n",
    "</role>\n",
    "\n",
    "<memory>\n",
    "{memory}\n",
    "</memory>\n",
    "\n",
    "<quality_standards>\n",
    "- **ALWAYS** use the information in memory.\n",
    "- Do not display the memory with the XML format directly to the user.\n",
    "</quality_standards>\n",
    "\n",
    "</system>\n",
    "\"\"\"\n",
    "\n",
    "# Create new memory from the chat history and any existing memory\n",
    "CREATE_MEMORY_INSTRUCTION: str = \"\"\"\"\n",
    "<system>\n",
    "\n",
    "<role>\n",
    "You are collecting information about the user to personalize your responses.\n",
    "</role>\n",
    "\n",
    "<current_user_info>\n",
    "{memory}\n",
    "</current_user_info>\n",
    "\n",
    "<instruction>\n",
    "1. If there's exisiting memory, simply update it.\n",
    "2. If new information conflicts with existing memory, keep the most recent version.\n",
    "Remember: Only include factual information directly stated by the user. Do not make assumptions or inferences.\n",
    "</instruction>\n",
    "\n",
    "Based on the chat history below, please update the user information:\n",
    "\n",
    "<system>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "85989ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def call_llm(\n",
    "    state: MessageState, config: RunnableConfig, store: BaseStore\n",
    ") -> dict[str, Any]:\n",
    "    # Get the user id\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Get the memory for the user\n",
    "    prefix: str = \"memory\"\n",
    "    key = \"user_memory\"\n",
    "    namespace = (prefix, user_id)\n",
    "    existing_memory = store.get(namespace, key)\n",
    "    # existing_memory = await store.aget(namespace, key)\n",
    "\n",
    "    if existing_memory and existing_memory.value:\n",
    "        memory_dict = existing_memory.value\n",
    "        formatted_memory = (\n",
    "            f\"Name: {memory_dict.get('user_name', 'Unknown')}\\n\"\n",
    "            f\"Interests: {', '.join(memory_dict.get('interests', []))}\\n\"\n",
    "        )\n",
    "    else:\n",
    "        formatted_memory = None\n",
    "\n",
    "    system_message: str = MODEL_SYSTEM_MESSAGE.format(memory=formatted_memory)\n",
    "    # Respond using memory + chat history\n",
    "    messages = to_openai_messages(\n",
    "        [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    )\n",
    "    response = await get_llm_response(\n",
    "        messages=messages,\n",
    "        response_model=None,\n",
    "        model=ModelEnum.LLAMA_3p2_3B_INSTRUCT_REMOTE.value,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "\n",
    "    return {\"messages\": [AIMessage(content=content)]}\n",
    "\n",
    "\n",
    "async def write_memory(\n",
    "    state: MessageState, config: RunnableConfig, store: BaseStore\n",
    ") -> None:\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    prefix: str = \"memory\"\n",
    "    key = \"user_memory\"\n",
    "    namespace = (prefix, user_id)\n",
    "    existing_memory = store.get(namespace, key)\n",
    "    # existing_memory = await store.aget(namespace, key)\n",
    "\n",
    "    if existing_memory and existing_memory.value:\n",
    "        memory_dict = existing_memory.value\n",
    "        formatted_memory = (\n",
    "            f\"Name: {memory_dict.get('user_name', 'Unknown')}\\n\"\n",
    "            f\"Interests: {', '.join(memory_dict.get('interests', []))}\\n\"\n",
    "        )\n",
    "    else:\n",
    "        formatted_memory = None\n",
    "\n",
    "    system_message: str = CREATE_MEMORY_INSTRUCTION.format(memory=formatted_memory)\n",
    "    # Respond using memory + chat history\n",
    "    messages = to_openai_messages(\n",
    "        [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    )\n",
    "    new_memory = await get_llm_response(\n",
    "        messages=messages,\n",
    "        response_model=UserProfile,\n",
    "        model=ModelEnum.LLAMA_3p2_3B_INSTRUCT_REMOTE.value,\n",
    "    )\n",
    "\n",
    "    # Update existing memory\n",
    "    # await store.aput(namespace, key, {prefix: new_memory.content})\n",
    "    store.put(namespace, key, {prefix: new_memory.model_dump()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "11b8d45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = InMemoryStore()\n",
    "\n",
    "namespace = (\"memory\", \"1\")\n",
    "key: str = \"sample_memory\"\n",
    "value: dict[str, Any] = {\"user_name\": \"Neidu\", \"interests\": [\"LangGraph\"]}\n",
    "\n",
    "mem.put(namespace, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "90baedbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[23:29:44] </span><span style=\"font-weight: bold\">[</span>                                                                                        <a href=\"file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_28846/1390856903.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1390856903.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_28846/1390856903.py#5\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Item</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">namespace</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'memory'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">key</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'sample_memory'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">value</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'user_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Neidu'</span>,    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000\">'interests'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'LangGraph'</span><span style=\"font-weight: bold\">]}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">created_at</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2025-06-15T22:29:43.257251+00:00'</span>,              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #808000; text-decoration-color: #808000\">updated_at</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2025-06-15T22:29:43.257256+00:00'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">score</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">]</span>                                                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[23:29:44]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0m                                                                                        \u001b]8;id=269961;file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_28846/1390856903.py\u001b\\\u001b[2m1390856903.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=41809;file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_28846/1390856903.py#5\u001b\\\u001b[2m5\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m    \u001b[1;35mItem\u001b[0m\u001b[1m(\u001b[0m\u001b[33mnamespace\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'memory'\u001b[0m, \u001b[32m'1'\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mkey\u001b[0m=\u001b[32m'sample_memory'\u001b[0m, \u001b[33mvalue\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'user_name'\u001b[0m: \u001b[32m'Neidu'\u001b[0m,    \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[32m'interests'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'LangGraph'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m, \u001b[33mcreated_at\u001b[0m=\u001b[32m'2025-06-15T22:29:43.257251+00:00'\u001b[0m,              \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[33mupdated_at\u001b[0m=\u001b[32m'2025-06-15T22:29:43.257256+00:00'\u001b[0m, \u001b[33mscore\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m                               \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1m]\u001b[0m                                                                                        \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "namespace = (\"memory\", \"1\")\n",
    "key: str = \"sample_memory\"\n",
    "\n",
    "ex_memory = mem.search(namespace)\n",
    "console.log(ex_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8300654a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user_name': 'Neidu', 'interests': ['LangGraph']}]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[row.value for row in ex_memory]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6d7ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa643e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "86af8a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def call_llm(\n",
    "    state: MessageState, config: RunnableConfig, store: BaseStore\n",
    ") -> dict[str, Any]:\n",
    "    # Get the user id\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Get the memory for the user\n",
    "    prefix: str = \"memory\"\n",
    "    namespace = (prefix, user_id)\n",
    "    existing_memory = store.search(namespace)\n",
    "\n",
    "    formatted_memory = (\n",
    "        \"\\n\".join([f\"- {memory.value['memory']}\" for memory in existing_memory])\n",
    "        if existing_memory\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    system_message: str = MODEL_SYSTEM_MESSAGE.format(memory=formatted_memory)\n",
    "    # Respond using memory + chat history\n",
    "    messages = to_openai_messages(\n",
    "        [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    )\n",
    "    response = await get_llm_response(\n",
    "        messages=messages,\n",
    "        response_model=None,\n",
    "        model=ModelEnum.QWEN_3p0_8B_REMOTE.value,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "\n",
    "    return {\"messages\": [AIMessage(content=content)]}\n",
    "\n",
    "\n",
    "async def write_memory(\n",
    "    state: MessageState, config: RunnableConfig, store: BaseStore\n",
    ") -> None:\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    prefix: str = \"memory\"\n",
    "    key = \"user_memory\"\n",
    "    namespace = (prefix, user_id)\n",
    "    existing_memory = store.search(namespace)\n",
    "\n",
    "    formatted_memory = (\n",
    "        \"\\n\".join([f\"- {memory.value['memory']}\" for memory in existing_memory])\n",
    "        if existing_memory\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    system_message: str = CREATE_MEMORY_INSTRUCTION.format(memory=formatted_memory)\n",
    "    # Respond using memory + chat history\n",
    "    messages = to_openai_messages(\n",
    "        [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    )\n",
    "    new_memory = await get_llm_response(\n",
    "        messages=messages,\n",
    "        response_model=MemoryCollection,\n",
    "        model=ModelEnum.QWEN_3p0_8B_REMOTE.value,\n",
    "    )\n",
    "\n",
    "    # Update existing memory\n",
    "    # await store.aput(namespace, key, {prefix: new_memory.content})\n",
    "    store.put(namespace, key, {prefix: new_memory.model_dump()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "6d7bcb35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graph\n",
    "graph_builder = StateGraph(MessageState)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node(\"call_llm\", call_llm)\n",
    "graph_builder.add_node(\"write_memory\", write_memory)\n",
    "\n",
    "# Add edges\n",
    "graph_builder.add_edge(START, \"call_llm\")\n",
    "graph_builder.add_edge(\"call_llm\", \"write_memory\")\n",
    "graph_builder.add_edge(\"write_memory\", END)\n",
    "\n",
    "# Long-term-memory store (across threads)\n",
    "across_thread_memory = InMemoryStore()\n",
    "# Short-term-memory store (within a thread) checkpointer\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# Compile\n",
    "graph = graph_builder.compile(\n",
    "    checkpointer=within_thread_memory, store=across_thread_memory\n",
    ").with_config(run_name=\"chatbot-with-structured-memory\")\n",
    "\n",
    "# Visualize\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c9df8582",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_chat(inputs: list[str]):\n",
    "    state = {\"messages\": [HumanMessage(content=msg) for msg in inputs]}\n",
    "    config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "    async for event in graph.astream(state, config=config, stream_mode=\"values\"):\n",
    "        for msg in event[\"messages\"]:\n",
    "            msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "5f5eb6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Neidu\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Neidu\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Neidu! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "inputs: list[str] = [\"Hi, I'm Neidu\"]\n",
    "graph_input = {\"messages\": [HumanMessage(content=msg) for msg in inputs]}\n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "async for event in graph.astream(graph_input, config=config, stream_mode=\"values\"):\n",
    "    for msg in event[\"messages\"]:\n",
    "        msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e20eb8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Neidu\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Neidu! How can I assist you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What do you know about LangGraph?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Neidu\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Neidu! How can I assist you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What do you know about LangGraph?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "LangGraph is a framework developed by the LangChain team to help build applications that use large language models (LLMs) in a structured and scalable way. It allows users to define workflows by chaining together different steps or functions, such as data processing, model inference, or decision-making, into a graph-like structure. This makes it easier to manage complex interactions between LLMs and other tools or data sources.\n",
      "\n",
      "Key features of LangGraph include:\n",
      "- **State Management**: Tracking and persisting the state of conversations or processes across multiple steps.\n",
      "- **Node and Edge Definitions**: Creating modular components (nodes) and defining how they connect (edges) to model workflows.\n",
      "- **Integration with LangChain**: Works seamlessly with other LangChain tools like LangSmith for debugging and monitoring.\n",
      "- **Scalability**: Designed to handle large-scale applications by optimizing the flow of data and computations.\n",
      "\n",
      "LangGraph is particularly useful for tasks like building chatbots, automating workflows, or creating multi-step reasoning systems. If you're working on a specific project or need help with a particular use case, feel free to share more details! \n"
     ]
    }
   ],
   "source": [
    "inputs: list[str] = [\"What do you know about LangGraph?\"]\n",
    "graph_input = {\"messages\": [HumanMessage(content=msg) for msg in inputs]}\n",
    "\n",
    "async for event in graph.astream(graph_input, config=config, stream_mode=\"values\"):\n",
    "    for msg in event[\"messages\"]:\n",
    "        msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "27a2977a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Neidu\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Neidu! How can I assist you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What do you know about LangGraph?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "LangGraph is a framework developed by the LangChain team to help build applications that use large language models (LLMs) in a structured and scalable way. It allows users to define workflows by chaining together different steps or functions, such as data processing, model inference, or decision-making, into a graph-like structure. This makes it easier to manage complex interactions between LLMs and other tools or data sources.\n",
      "\n",
      "Key features of LangGraph include:\n",
      "- **State Management**: Tracking and persisting the state of conversations or processes across multiple steps.\n",
      "- **Node and Edge Definitions**: Creating modular components (nodes) and defining how they connect (edges) to model workflows.\n",
      "- **Integration with LangChain**: Works seamlessly with other LangChain tools like LangSmith for debugging and monitoring.\n",
      "- **Scalability**: Designed to handle large-scale applications by optimizing the flow of data and computations.\n",
      "\n",
      "LangGraph is particularly useful for tasks like building chatbots, automating workflows, or creating multi-step reasoning systems. If you're working on a specific project or need help with a particular use case, feel free to share more details! \n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How can I create AI agentic workflows?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Neidu\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Neidu! How can I assist you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What do you know about LangGraph?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "LangGraph is a framework developed by the LangChain team to help build applications that use large language models (LLMs) in a structured and scalable way. It allows users to define workflows by chaining together different steps or functions, such as data processing, model inference, or decision-making, into a graph-like structure. This makes it easier to manage complex interactions between LLMs and other tools or data sources.\n",
      "\n",
      "Key features of LangGraph include:\n",
      "- **State Management**: Tracking and persisting the state of conversations or processes across multiple steps.\n",
      "- **Node and Edge Definitions**: Creating modular components (nodes) and defining how they connect (edges) to model workflows.\n",
      "- **Integration with LangChain**: Works seamlessly with other LangChain tools like LangSmith for debugging and monitoring.\n",
      "- **Scalability**: Designed to handle large-scale applications by optimizing the flow of data and computations.\n",
      "\n",
      "LangGraph is particularly useful for tasks like building chatbots, automating workflows, or creating multi-step reasoning systems. If you're working on a specific project or need help with a particular use case, feel free to share more details! \n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How can I create AI agentic workflows?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Creating AI agentic workflows with **LangGraph** involves designing systems where AI agents can autonomously make decisions, interact with tools, and collaborate with each other. Here's a structured approach to building such workflows:\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Define the Agents**\n",
      "- **Agent Roles**: Identify the tasks each agent needs to perform (e.g., data retrieval, decision-making, task execution).\n",
      "- **LLM Integration**: Use a large language model (LLM) as the core of each agent, enabling it to reason, plan, and act based on input.\n",
      "- **Tool Usage**: Equip agents with tools (e.g., APIs, databases, code execution) to perform specific actions. For example, a \"Customer Support Agent\" might use a database tool to fetch user data or a search API to find answers.\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Structure the Workflow**\n",
      "- **Nodes and Edges**: \n",
      "  - **Nodes** represent individual agents or steps (e.g., \"Answer Question,\" \"Fetch Data\").\n",
      "  - **Edges** define how agents transition between steps based on conditions or outcomes (e.g., \"If the answer is incomplete, redirect to Research Agent\").\n",
      "- **Graph Design**: Use LangGraph's graph API to connect nodes logically. For example:\n",
      "  ```python\n",
      "  from langgraph.graph import Graph\n",
      "  graph = Graph()\n",
      "  graph.add_node(\"Answer Question\", answer_question)\n",
      "  graph.add_node(\"Research\", research_tool)\n",
      "  graph.add_edge(\"Answer Question\", \"Research\")\n",
      "  ```\n",
      "\n",
      "---\n",
      "\n",
      "### **3. Implement State Management**\n",
      "- **Track Context**: Use LangGraph's state management to persist information across steps (e.g., user queries, intermediate results, or agent decisions).\n",
      "- **Example**: A customer support agent might store user history in the state to provide personalized responses.\n",
      "\n",
      "---\n",
      "\n",
      "### **4. Add Conditional Logic**\n",
      "- **Decide Flow**: Use conditional edges to route workflows dynamically. For instance:\n",
      "  - If an agent's response is uncertain, send it to a \"Verify\" node.\n",
      "  - If a task fails, trigger a \"Retry\" or \"Fallback\" agent.\n",
      "\n",
      "---\n",
      "\n",
      "### **5. Integrate Tools and External Systems**\n",
      "- **Tool Calling**: Allow agents to call external tools (e.g., APIs, databases) to perform actions. LangGraph supports this via its `tool_call` and `tool_response` mechanisms.\n",
      "- **Example**: A \"Data Analyst Agent\" might call a SQL database to retrieve datasets before generating insights.\n",
      "\n",
      "---\n",
      "\n",
      "### **6. Test and Debug**\n",
      "- **LangSmith Integration**: Use LangChain's **LangSmith** for testing, logging, and debugging workflows. It helps trace agent decisions and tool interactions.\n",
      "- **Simulation**: Test workflows in isolation to ensure agents handle edge cases (e.g., ambiguous inputs, errors).\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "inputs: list[str] = [\"How can I create AI agentic workflows?\"]\n",
    "graph_input = {\"messages\": [HumanMessage(content=msg) for msg in inputs]}\n",
    "\n",
    "async for event in graph.astream(graph_input, config=config, stream_mode=\"values\"):\n",
    "    for msg in event[\"messages\"]:\n",
    "        msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ed64ea51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Neidu\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Neidu! How can I assist you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What do you know about LangGraph?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "LangGraph is a framework developed by the LangChain team to help build applications that use large language models (LLMs) in a structured and scalable way. It allows users to define workflows by chaining together different steps or functions, such as data processing, model inference, or decision-making, into a graph-like structure. This makes it easier to manage complex interactions between LLMs and other tools or data sources.\n",
      "\n",
      "Key features of LangGraph include:\n",
      "- **State Management**: Tracking and persisting the state of conversations or processes across multiple steps.\n",
      "- **Node and Edge Definitions**: Creating modular components (nodes) and defining how they connect (edges) to model workflows.\n",
      "- **Integration with LangChain**: Works seamlessly with other LangChain tools like LangSmith for debugging and monitoring.\n",
      "- **Scalability**: Designed to handle large-scale applications by optimizing the flow of data and computations.\n",
      "\n",
      "LangGraph is particularly useful for tasks like building chatbots, automating workflows, or creating multi-step reasoning systems. If you're working on a specific project or need help with a particular use case, feel free to share more details! \n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How can I create AI agentic workflows?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Creating AI agentic workflows with **LangGraph** involves designing systems where AI agents can autonomously make decisions, interact with tools, and collaborate with each other. Here's a structured approach to building such workflows:\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Define the Agents**\n",
      "- **Agent Roles**: Identify the tasks each agent needs to perform (e.g., data retrieval, decision-making, task execution).\n",
      "- **LLM Integration**: Use a large language model (LLM) as the core of each agent, enabling it to reason, plan, and act based on input.\n",
      "- **Tool Usage**: Equip agents with tools (e.g., APIs, databases, code execution) to perform specific actions. For example, a \"Customer Support Agent\" might use a database tool to fetch user data or a search API to find answers.\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Structure the Workflow**\n",
      "- **Nodes and Edges**: \n",
      "  - **Nodes** represent individual agents or steps (e.g., \"Answer Question,\" \"Fetch Data\").\n",
      "  - **Edges** define how agents transition between steps based on conditions or outcomes (e.g., \"If the answer is incomplete, redirect to Research Agent\").\n",
      "- **Graph Design**: Use LangGraph's graph API to connect nodes logically. For example:\n",
      "  ```python\n",
      "  from langgraph.graph import Graph\n",
      "  graph = Graph()\n",
      "  graph.add_node(\"Answer Question\", answer_question)\n",
      "  graph.add_node(\"Research\", research_tool)\n",
      "  graph.add_edge(\"Answer Question\", \"Research\")\n",
      "  ```\n",
      "\n",
      "---\n",
      "\n",
      "### **3. Implement State Management**\n",
      "- **Track Context**: Use LangGraph's state management to persist information across steps (e.g., user queries, intermediate results, or agent decisions).\n",
      "- **Example**: A customer support agent might store user history in the state to provide personalized responses.\n",
      "\n",
      "---\n",
      "\n",
      "### **4. Add Conditional Logic**\n",
      "- **Decide Flow**: Use conditional edges to route workflows dynamically. For instance:\n",
      "  - If an agent's response is uncertain, send it to a \"Verify\" node.\n",
      "  - If a task fails, trigger a \"Retry\" or \"Fallback\" agent.\n",
      "\n",
      "---\n",
      "\n",
      "### **5. Integrate Tools and External Systems**\n",
      "- **Tool Calling**: Allow agents to call external tools (e.g., APIs, databases) to perform actions. LangGraph supports this via its `tool_call` and `tool_response` mechanisms.\n",
      "- **Example**: A \"Data Analyst Agent\" might call a SQL database to retrieve datasets before generating insights.\n",
      "\n",
      "---\n",
      "\n",
      "### **6. Test and Debug**\n",
      "- **LangSmith Integration**: Use LangChain's **LangSmith** for testing, logging, and debugging workflows. It helps trace agent decisions and tool interactions.\n",
      "- **Simulation**: Test workflows in isolation to ensure agents handle edge cases (e.g., ambiguous inputs, errors).\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Chat history\n",
    "state = graph.get_state(config=config).values\n",
    "\n",
    "for m in state[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[00:25:06] </span><span style=\"font-weight: bold\">[</span>                                                                                        <a href=\"file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_28846/1731364959.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1731364959.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_28846/1731364959.py#6\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Item</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">namespace</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'memory'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">key</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'user_memory'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">value</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'memory'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'memories'</span>:     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Hi, I'm Neidu\"</span><span style=\"font-weight: bold\">}</span>, <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'What do you know about LangGraph?'</span><span style=\"font-weight: bold\">}]}}</span>,      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #808000; text-decoration-color: #808000\">created_at</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2025-06-15T23:24:32.164360+00:00'</span>,                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #808000; text-decoration-color: #808000\">updated_at</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2025-06-15T23:24:32.164366+00:00'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">score</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">]</span>                                                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[00:25:06]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0m                                                                                        \u001b]8;id=521469;file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_28846/1731364959.py\u001b\\\u001b[2m1731364959.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=538980;file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_28846/1731364959.py#6\u001b\\\u001b[2m6\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m    \u001b[1;35mItem\u001b[0m\u001b[1m(\u001b[0m\u001b[33mnamespace\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'memory'\u001b[0m, \u001b[32m'1'\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mkey\u001b[0m=\u001b[32m'user_memory'\u001b[0m, \u001b[33mvalue\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'memory'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'memories'\u001b[0m:     \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'content'\u001b[0m: \u001b[32m\"Hi, I'm Neidu\"\u001b[0m\u001b[1m}\u001b[0m, \u001b[1m{\u001b[0m\u001b[32m'content'\u001b[0m: \u001b[32m'What do you know about LangGraph?'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,      \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[33mcreated_at\u001b[0m=\u001b[32m'2025-06-15T23:24:32.164360+00:00'\u001b[0m,                                           \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[33mupdated_at\u001b[0m=\u001b[32m'2025-06-15T23:24:32.164366+00:00'\u001b[0m, \u001b[33mscore\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m                               \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1m]\u001b[0m                                                                                        \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# namespace for the memory to save\n",
    "user_id: str = \"1\"\n",
    "prefix: str = \"memory\"\n",
    "namespace = (prefix, user_id)\n",
    "existing_memory = across_thread_memory.search(namespace)\n",
    "console.log(existing_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23e995d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Neidu\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Neidu! How can I assist you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What do you know about LangGraph?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "LangGraph is a framework developed by the LangChain team to help build applications that use large language models (LLMs) in a structured and scalable way. It allows users to define workflows by chaining together different steps or functions, such as data processing, model inference, or decision-making, into a graph-like structure. This makes it easier to manage complex interactions between LLMs and other tools or data sources.\n",
      "\n",
      "Key features of LangGraph include:\n",
      "- **State Management**: Tracking and persisting the state of conversations or processes across multiple steps.\n",
      "- **Node and Edge Definitions**: Creating modular components (nodes) and defining how they connect (edges) to model workflows.\n",
      "- **Integration with LangChain**: Works seamlessly with other LangChain tools like LangSmith for debugging and monitoring.\n",
      "- **Scalability**: Designed to handle large-scale applications by optimizing the flow of data and computations.\n",
      "\n",
      "LangGraph is particularly useful for tasks like building chatbots, automating workflows, or creating multi-step reasoning systems. If you're working on a specific project or need help with a particular use case, feel free to share more details! \n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How can I create AI agentic workflows?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Creating AI agentic workflows with **LangGraph** involves designing systems where AI agents can autonomously make decisions, interact with tools, and collaborate with each other. Here's a structured approach to building such workflows:\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Define the Agents**\n",
      "- **Agent Roles**: Identify the tasks each agent needs to perform (e.g., data retrieval, decision-making, task execution).\n",
      "- **LLM Integration**: Use a large language model (LLM) as the core of each agent, enabling it to reason, plan, and act based on input.\n",
      "- **Tool Usage**: Equip agents with tools (e.g., APIs, databases, code execution) to perform specific actions. For example, a \"Customer Support Agent\" might use a database tool to fetch user data or a search API to find answers.\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Structure the Workflow**\n",
      "- **Nodes and Edges**: \n",
      "  - **Nodes** represent individual agents or steps (e.g., \"Answer Question,\" \"Fetch Data\").\n",
      "  - **Edges** define how agents transition between steps based on conditions or outcomes (e.g., \"If the answer is incomplete, redirect to Research Agent\").\n",
      "- **Graph Design**: Use LangGraph's graph API to connect nodes logically. For example:\n",
      "  ```python\n",
      "  from langgraph.graph import Graph\n",
      "  graph = Graph()\n",
      "  graph.add_node(\"Answer Question\", answer_question)\n",
      "  graph.add_node(\"Research\", research_tool)\n",
      "  graph.add_edge(\"Answer Question\", \"Research\")\n",
      "  ```\n",
      "\n",
      "---\n",
      "\n",
      "### **3. Implement State Management**\n",
      "- **Track Context**: Use LangGraph's state management to persist information across steps (e.g., user queries, intermediate results, or agent decisions).\n",
      "- **Example**: A customer support agent might store user history in the state to provide personalized responses.\n",
      "\n",
      "---\n",
      "\n",
      "### **4. Add Conditional Logic**\n",
      "- **Decide Flow**: Use conditional edges to route workflows dynamically. For instance:\n",
      "  - If an agent's response is uncertain, send it to a \"Verify\" node.\n",
      "  - If a task fails, trigger a \"Retry\" or \"Fallback\" agent.\n",
      "\n",
      "---\n",
      "\n",
      "### **5. Integrate Tools and External Systems**\n",
      "- **Tool Calling**: Allow agents to call external tools (e.g., APIs, databases) to perform actions. LangGraph supports this via its `tool_call` and `tool_response` mechanisms.\n",
      "- **Example**: A \"Data Analyst Agent\" might call a SQL database to retrieve datasets before generating insights.\n",
      "\n",
      "---\n",
      "\n",
      "### **6. Test and Debug**\n",
      "- **LangSmith Integration**: Use LangChain's **LangSmith** for testing, logging, and debugging workflows. It helps trace agent decisions and tool interactions.\n",
      "- **Simulation**: Test workflows in isolation to ensure agents handle edge cases (e.g., ambiguous inputs, errors).\n",
      "\n",
      "---\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What was my first question?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your first question was \"Hi, I'm Neidu.\" \n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Summarize our conversation thus far\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Neidu\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Neidu! How can I assist you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What do you know about LangGraph?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "LangGraph is a framework developed by the LangChain team to help build applications that use large language models (LLMs) in a structured and scalable way. It allows users to define workflows by chaining together different steps or functions, such as data processing, model inference, or decision-making, into a graph-like structure. This makes it easier to manage complex interactions between LLMs and other tools or data sources.\n",
      "\n",
      "Key features of LangGraph include:\n",
      "- **State Management**: Tracking and persisting the state of conversations or processes across multiple steps.\n",
      "- **Node and Edge Definitions**: Creating modular components (nodes) and defining how they connect (edges) to model workflows.\n",
      "- **Integration with LangChain**: Works seamlessly with other LangChain tools like LangSmith for debugging and monitoring.\n",
      "- **Scalability**: Designed to handle large-scale applications by optimizing the flow of data and computations.\n",
      "\n",
      "LangGraph is particularly useful for tasks like building chatbots, automating workflows, or creating multi-step reasoning systems. If you're working on a specific project or need help with a particular use case, feel free to share more details! \n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How can I create AI agentic workflows?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Creating AI agentic workflows with **LangGraph** involves designing systems where AI agents can autonomously make decisions, interact with tools, and collaborate with each other. Here's a structured approach to building such workflows:\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Define the Agents**\n",
      "- **Agent Roles**: Identify the tasks each agent needs to perform (e.g., data retrieval, decision-making, task execution).\n",
      "- **LLM Integration**: Use a large language model (LLM) as the core of each agent, enabling it to reason, plan, and act based on input.\n",
      "- **Tool Usage**: Equip agents with tools (e.g., APIs, databases, code execution) to perform specific actions. For example, a \"Customer Support Agent\" might use a database tool to fetch user data or a search API to find answers.\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Structure the Workflow**\n",
      "- **Nodes and Edges**: \n",
      "  - **Nodes** represent individual agents or steps (e.g., \"Answer Question,\" \"Fetch Data\").\n",
      "  - **Edges** define how agents transition between steps based on conditions or outcomes (e.g., \"If the answer is incomplete, redirect to Research Agent\").\n",
      "- **Graph Design**: Use LangGraph's graph API to connect nodes logically. For example:\n",
      "  ```python\n",
      "  from langgraph.graph import Graph\n",
      "  graph = Graph()\n",
      "  graph.add_node(\"Answer Question\", answer_question)\n",
      "  graph.add_node(\"Research\", research_tool)\n",
      "  graph.add_edge(\"Answer Question\", \"Research\")\n",
      "  ```\n",
      "\n",
      "---\n",
      "\n",
      "### **3. Implement State Management**\n",
      "- **Track Context**: Use LangGraph's state management to persist information across steps (e.g., user queries, intermediate results, or agent decisions).\n",
      "- **Example**: A customer support agent might store user history in the state to provide personalized responses.\n",
      "\n",
      "---\n",
      "\n",
      "### **4. Add Conditional Logic**\n",
      "- **Decide Flow**: Use conditional edges to route workflows dynamically. For instance:\n",
      "  - If an agent's response is uncertain, send it to a \"Verify\" node.\n",
      "  - If a task fails, trigger a \"Retry\" or \"Fallback\" agent.\n",
      "\n",
      "---\n",
      "\n",
      "### **5. Integrate Tools and External Systems**\n",
      "- **Tool Calling**: Allow agents to call external tools (e.g., APIs, databases) to perform actions. LangGraph supports this via its `tool_call` and `tool_response` mechanisms.\n",
      "- **Example**: A \"Data Analyst Agent\" might call a SQL database to retrieve datasets before generating insights.\n",
      "\n",
      "---\n",
      "\n",
      "### **6. Test and Debug**\n",
      "- **LangSmith Integration**: Use LangChain's **LangSmith** for testing, logging, and debugging workflows. It helps trace agent decisions and tool interactions.\n",
      "- **Simulation**: Test workflows in isolation to ensure agents handle edge cases (e.g., ambiguous inputs, errors).\n",
      "\n",
      "---\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What was my first question?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your first question was \"Hi, I'm Neidu.\" \n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Summarize our conversation thus far\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Our conversation so far has covered:  \n",
      "1. **Introduction**: You introduced yourself as \"Neidu,\" and I greeted you.  \n",
      "2. **LangGraph Overview**: You asked about LangGraph, and I explained it as a framework for building structured, scalable AI workflows with LLMs, highlighting features like state management, node/edge definitions, and integration with LangChain tools.  \n",
      "3. **AI Agentic Workflows**: You inquired about creating such workflows, and I outlined steps like defining agents, structuring graphs, managing state, adding conditions, integrating tools, and testing with LangSmith.  \n",
      "\n",
      "Let me know if you'd like to dive deeper into any of these topics! \n"
     ]
    }
   ],
   "source": [
    "inputs: list[str] = [\"What was my first question?\"]\n",
    "graph_input = {\"messages\": [HumanMessage(content=msg) for msg in inputs]}\n",
    "\n",
    "async for event in graph.astream(graph_input, config=config, stream_mode=\"values\"):\n",
    "    for msg in event[\"messages\"]:\n",
    "        msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "bf16c412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Neidu\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Neidu! How can I assist you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What do you know about LangGraph?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "LangGraph is a framework developed by the LangChain team to help build applications that use large language models (LLMs) in a structured and scalable way. It allows users to define workflows by chaining together different steps or functions, such as data processing, model inference, or decision-making, into a graph-like structure. This makes it easier to manage complex interactions between LLMs and other tools or data sources.\n",
      "\n",
      "Key features of LangGraph include:\n",
      "- **State Management**: Tracking and persisting the state of conversations or processes across multiple steps.\n",
      "- **Node and Edge Definitions**: Creating modular components (nodes) and defining how they connect (edges) to model workflows.\n",
      "- **Integration with LangChain**: Works seamlessly with other LangChain tools like LangSmith for debugging and monitoring.\n",
      "- **Scalability**: Designed to handle large-scale applications by optimizing the flow of data and computations.\n",
      "\n",
      "LangGraph is particularly useful for tasks like building chatbots, automating workflows, or creating multi-step reasoning systems. If you're working on a specific project or need help with a particular use case, feel free to share more details! \n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How can I create AI agentic workflows?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Creating AI agentic workflows with **LangGraph** involves designing systems where AI agents can autonomously make decisions, interact with tools, and collaborate with each other. Here's a structured approach to building such workflows:\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Define the Agents**\n",
      "- **Agent Roles**: Identify the tasks each agent needs to perform (e.g., data retrieval, decision-making, task execution).\n",
      "- **LLM Integration**: Use a large language model (LLM) as the core of each agent, enabling it to reason, plan, and act based on input.\n",
      "- **Tool Usage**: Equip agents with tools (e.g., APIs, databases, code execution) to perform specific actions. For example, a \"Customer Support Agent\" might use a database tool to fetch user data or a search API to find answers.\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Structure the Workflow**\n",
      "- **Nodes and Edges**: \n",
      "  - **Nodes** represent individual agents or steps (e.g., \"Answer Question,\" \"Fetch Data\").\n",
      "  - **Edges** define how agents transition between steps based on conditions or outcomes (e.g., \"If the answer is incomplete, redirect to Research Agent\").\n",
      "- **Graph Design**: Use LangGraph's graph API to connect nodes logically. For example:\n",
      "  ```python\n",
      "  from langgraph.graph import Graph\n",
      "  graph = Graph()\n",
      "  graph.add_node(\"Answer Question\", answer_question)\n",
      "  graph.add_node(\"Research\", research_tool)\n",
      "  graph.add_edge(\"Answer Question\", \"Research\")\n",
      "  ```\n",
      "\n",
      "---\n",
      "\n",
      "### **3. Implement State Management**\n",
      "- **Track Context**: Use LangGraph's state management to persist information across steps (e.g., user queries, intermediate results, or agent decisions).\n",
      "- **Example**: A customer support agent might store user history in the state to provide personalized responses.\n",
      "\n",
      "---\n",
      "\n",
      "### **4. Add Conditional Logic**\n",
      "- **Decide Flow**: Use conditional edges to route workflows dynamically. For instance:\n",
      "  - If an agent's response is uncertain, send it to a \"Verify\" node.\n",
      "  - If a task fails, trigger a \"Retry\" or \"Fallback\" agent.\n",
      "\n",
      "---\n",
      "\n",
      "### **5. Integrate Tools and External Systems**\n",
      "- **Tool Calling**: Allow agents to call external tools (e.g., APIs, databases) to perform actions. LangGraph supports this via its `tool_call` and `tool_response` mechanisms.\n",
      "- **Example**: A \"Data Analyst Agent\" might call a SQL database to retrieve datasets before generating insights.\n",
      "\n",
      "---\n",
      "\n",
      "### **6. Test and Debug**\n",
      "- **LangSmith Integration**: Use LangChain's **LangSmith** for testing, logging, and debugging workflows. It helps trace agent decisions and tool interactions.\n",
      "- **Simulation**: Test workflows in isolation to ensure agents handle edge cases (e.g., ambiguous inputs, errors).\n",
      "\n",
      "---\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What was my first question?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your first question was \"Hi, I'm Neidu.\" \n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Summarize our conversation thus far\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Our conversation so far has covered:  \n",
      "1. **Introduction**: You introduced yourself as \"Neidu,\" and I greeted you.  \n",
      "2. **LangGraph Overview**: You asked about LangGraph, and I explained it as a framework for building structured, scalable AI workflows with LLMs, highlighting features like state management, node/edge definitions, and integration with LangChain tools.  \n",
      "3. **AI Agentic Workflows**: You inquired about creating such workflows, and I outlined steps like defining agents, structuring graphs, managing state, adding conditions, integrating tools, and testing with LangSmith.  \n",
      "\n",
      "Let me know if you'd like to dive deeper into any of these topics! \n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What do you know about me?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Neidu\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Neidu! How can I assist you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What do you know about LangGraph?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "LangGraph is a framework developed by the LangChain team to help build applications that use large language models (LLMs) in a structured and scalable way. It allows users to define workflows by chaining together different steps or functions, such as data processing, model inference, or decision-making, into a graph-like structure. This makes it easier to manage complex interactions between LLMs and other tools or data sources.\n",
      "\n",
      "Key features of LangGraph include:\n",
      "- **State Management**: Tracking and persisting the state of conversations or processes across multiple steps.\n",
      "- **Node and Edge Definitions**: Creating modular components (nodes) and defining how they connect (edges) to model workflows.\n",
      "- **Integration with LangChain**: Works seamlessly with other LangChain tools like LangSmith for debugging and monitoring.\n",
      "- **Scalability**: Designed to handle large-scale applications by optimizing the flow of data and computations.\n",
      "\n",
      "LangGraph is particularly useful for tasks like building chatbots, automating workflows, or creating multi-step reasoning systems. If you're working on a specific project or need help with a particular use case, feel free to share more details! \n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How can I create AI agentic workflows?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Creating AI agentic workflows with **LangGraph** involves designing systems where AI agents can autonomously make decisions, interact with tools, and collaborate with each other. Here's a structured approach to building such workflows:\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Define the Agents**\n",
      "- **Agent Roles**: Identify the tasks each agent needs to perform (e.g., data retrieval, decision-making, task execution).\n",
      "- **LLM Integration**: Use a large language model (LLM) as the core of each agent, enabling it to reason, plan, and act based on input.\n",
      "- **Tool Usage**: Equip agents with tools (e.g., APIs, databases, code execution) to perform specific actions. For example, a \"Customer Support Agent\" might use a database tool to fetch user data or a search API to find answers.\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Structure the Workflow**\n",
      "- **Nodes and Edges**: \n",
      "  - **Nodes** represent individual agents or steps (e.g., \"Answer Question,\" \"Fetch Data\").\n",
      "  - **Edges** define how agents transition between steps based on conditions or outcomes (e.g., \"If the answer is incomplete, redirect to Research Agent\").\n",
      "- **Graph Design**: Use LangGraph's graph API to connect nodes logically. For example:\n",
      "  ```python\n",
      "  from langgraph.graph import Graph\n",
      "  graph = Graph()\n",
      "  graph.add_node(\"Answer Question\", answer_question)\n",
      "  graph.add_node(\"Research\", research_tool)\n",
      "  graph.add_edge(\"Answer Question\", \"Research\")\n",
      "  ```\n",
      "\n",
      "---\n",
      "\n",
      "### **3. Implement State Management**\n",
      "- **Track Context**: Use LangGraph's state management to persist information across steps (e.g., user queries, intermediate results, or agent decisions).\n",
      "- **Example**: A customer support agent might store user history in the state to provide personalized responses.\n",
      "\n",
      "---\n",
      "\n",
      "### **4. Add Conditional Logic**\n",
      "- **Decide Flow**: Use conditional edges to route workflows dynamically. For instance:\n",
      "  - If an agent's response is uncertain, send it to a \"Verify\" node.\n",
      "  - If a task fails, trigger a \"Retry\" or \"Fallback\" agent.\n",
      "\n",
      "---\n",
      "\n",
      "### **5. Integrate Tools and External Systems**\n",
      "- **Tool Calling**: Allow agents to call external tools (e.g., APIs, databases) to perform actions. LangGraph supports this via its `tool_call` and `tool_response` mechanisms.\n",
      "- **Example**: A \"Data Analyst Agent\" might call a SQL database to retrieve datasets before generating insights.\n",
      "\n",
      "---\n",
      "\n",
      "### **6. Test and Debug**\n",
      "- **LangSmith Integration**: Use LangChain's **LangSmith** for testing, logging, and debugging workflows. It helps trace agent decisions and tool interactions.\n",
      "- **Simulation**: Test workflows in isolation to ensure agents handle edge cases (e.g., ambiguous inputs, errors).\n",
      "\n",
      "---\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What was my first question?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your first question was \"Hi, I'm Neidu.\" \n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Summarize our conversation thus far\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Our conversation so far has covered:  \n",
      "1. **Introduction**: You introduced yourself as \"Neidu,\" and I greeted you.  \n",
      "2. **LangGraph Overview**: You asked about LangGraph, and I explained it as a framework for building structured, scalable AI workflows with LLMs, highlighting features like state management, node/edge definitions, and integration with LangChain tools.  \n",
      "3. **AI Agentic Workflows**: You inquired about creating such workflows, and I outlined steps like defining agents, structuring graphs, managing state, adding conditions, integrating tools, and testing with LangSmith.  \n",
      "\n",
      "Let me know if you'd like to dive deeper into any of these topics! \n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What do you know about me?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I dont have access to personal information about you beyond our conversation so far. From what weve discussed, you introduced yourself as \"Neidu\" and asked about **LangGraph** and how to create AI agentic workflows. If youd like to share more about your goals or projects, Id be happy to help! \n"
     ]
    }
   ],
   "source": [
    "inputs: list[str] = [\"What do you know about me?\"]\n",
    "graph_input = {\"messages\": [HumanMessage(content=msg) for msg in inputs]}\n",
    "config_2 = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"2\"}}\n",
    "\n",
    "async for event in graph.astream(graph_input, config=config_2, stream_mode=\"values\"):\n",
    "    for msg in event[\"messages\"]:\n",
    "        msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[00:30:20] </span><span style=\"font-weight: bold\">[</span>                                                                                        <a href=\"file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_28846/1731364959.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1731364959.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_28846/1731364959.py#6\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Item</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">namespace</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'memory'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">key</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'user_memory'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">value</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'memory'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'memories'</span>:     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Hi, I'm Neidu\"</span><span style=\"font-weight: bold\">}</span>, <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'What do you know about LangGraph?'</span><span style=\"font-weight: bold\">}</span>,         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'How can I create AI agentic workflows?'</span><span style=\"font-weight: bold\">}</span>, <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'What was my first </span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000\">question?'</span><span style=\"font-weight: bold\">}]}}</span>, <span style=\"color: #808000; text-decoration-color: #808000\">created_at</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2025-06-15T23:26:23.533687+00:00'</span>,                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #808000; text-decoration-color: #808000\">updated_at</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2025-06-15T23:26:23.533692+00:00'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">score</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">]</span>                                                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[00:30:20]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0m                                                                                        \u001b]8;id=710199;file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_28846/1731364959.py\u001b\\\u001b[2m1731364959.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=890171;file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_28846/1731364959.py#6\u001b\\\u001b[2m6\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m    \u001b[1;35mItem\u001b[0m\u001b[1m(\u001b[0m\u001b[33mnamespace\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'memory'\u001b[0m, \u001b[32m'1'\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mkey\u001b[0m=\u001b[32m'user_memory'\u001b[0m, \u001b[33mvalue\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'memory'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'memories'\u001b[0m:     \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'content'\u001b[0m: \u001b[32m\"Hi, I'm Neidu\"\u001b[0m\u001b[1m}\u001b[0m, \u001b[1m{\u001b[0m\u001b[32m'content'\u001b[0m: \u001b[32m'What do you know about LangGraph?'\u001b[0m\u001b[1m}\u001b[0m,         \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1m{\u001b[0m\u001b[32m'content'\u001b[0m: \u001b[32m'How can I create AI agentic workflows?'\u001b[0m\u001b[1m}\u001b[0m, \u001b[1m{\u001b[0m\u001b[32m'content'\u001b[0m: \u001b[32m'What was my first \u001b[0m   \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[32mquestion?'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m, \u001b[33mcreated_at\u001b[0m=\u001b[32m'2025-06-15T23:26:23.533687+00:00'\u001b[0m,                           \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[33mupdated_at\u001b[0m=\u001b[32m'2025-06-15T23:26:23.533692+00:00'\u001b[0m, \u001b[33mscore\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m                               \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1m]\u001b[0m                                                                                        \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# namespace for the memory to save\n",
    "user_id: str = \"1\"\n",
    "prefix: str = \"memory\"\n",
    "namespace = (prefix, user_id)\n",
    "existing_memory = across_thread_memory.search(namespace)\n",
    "console.log(existing_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a72976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bc140c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da40950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
