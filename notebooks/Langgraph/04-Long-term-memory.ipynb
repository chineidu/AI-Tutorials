{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc0ce639",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Content\n",
    "\n",
    "\n",
    "### 1. [Map Reduce](#map-reduce)\n",
    "\n",
    "### 2. [Short Term vs Long Term Memory](#short-term-vs-long-term-memory)\n",
    "\n",
    "### 3. [Chatbot with Profile Schema](#chatbot-with-profile-schema)\n",
    "\n",
    "### 4. [Chatbot with Profile Collection](#chatbot-with-profile-collection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b2aa0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import (\n",
    "    Annotated,\n",
    "    Any,\n",
    "    Generator,\n",
    "    Iterable,\n",
    "    Literal,\n",
    "    Optional,\n",
    "    TypedDict,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "# Standard imports\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"white\": \"#FFFFFF\",  # Bright white\n",
    "        \"info\": \"#00FF00\",  # Bright green\n",
    "        \"warning\": \"#FFD700\",  # Bright gold\n",
    "        \"error\": \"#FF1493\",  # Deep pink\n",
    "        \"success\": \"#00FFFF\",  # Cyan\n",
    "        \"highlight\": \"#FF4500\",  # Orange-red\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "# import matplotlib.pyplot as pltife\n",
    "\n",
    "# NumPy settings\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "# Polars settings\n",
    "pl.Config.set_fmt_str_lengths(1_000)\n",
    "pl.Config.set_tbl_cols(n=1_000)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_up_from_current_directory(*, go_up: int = 1) -> None:\n",
    "    \"\"\"This is used to up a number of directories.\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "    go_up: int, default=1\n",
    "        This indicates the number of times to go back up from the current directory.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    CONST: str = \"../\"\n",
    "    NUM: str = CONST * go_up\n",
    "\n",
    "    # Goto the previous directory\n",
    "    prev_directory = os.path.join(os.path.dirname(__name__), NUM)\n",
    "    # Get the 'absolute path' of the previous directory\n",
    "    abs_path_prev_directory = os.path.abspath(prev_directory)\n",
    "\n",
    "    # Add the path to the System paths\n",
    "    sys.path.insert(0, abs_path_prev_directory)\n",
    "    print(abs_path_prev_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d4f1fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/neidu/Desktop/Projects/Personal/My_Projects/AI-Tutorials\n"
     ]
    }
   ],
   "source": [
    "go_up_from_current_directory(go_up=2)\n",
    "\n",
    "\n",
    "from schemas import ModelEnum  # noqa: E402\n",
    "from settings import refresh_settings  # noqa: E402\n",
    "from src.utilities.llm_utils import (\n",
    "    LLMResponse,\n",
    "    convert_to_openai_messages,\n",
    ")  # noqa: E402\n",
    "from utilities.client_utils import check_rate_limit  # noqa: E402\n",
    "\n",
    "settings = refresh_settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6270200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"data\": {\n",
      "    \"label\": \"sk-or-v1-902...c45\",\n",
      "    \"limit\": 5,\n",
      "    \"usage\": 3.7253450895,\n",
      "    \"is_provisioning_key\": false,\n",
      "    \"limit_remaining\": 1.2746549104999998,\n",
      "    \"is_free_tier\": false,\n",
      "    \"rate_limit\": {\n",
      "      \"requests\": 20,\n",
      "      \"interval\": \"10s\"\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "check_rate_limit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16d37a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Type\n",
    "\n",
    "from pydantic import BaseModel, SecretStr\n",
    "\n",
    "\n",
    "def validate_data(\n",
    "    data: dict[str, Any], state: dict[str, Any], response_model: Type[BaseModel]\n",
    ") -> None:\n",
    "    \"\"\"Validate that input data matches the expected response model structure.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dict[str, Any]\n",
    "        The input data dictionary to validate\n",
    "    state : dict[str, Any]\n",
    "        The state dictionary to update with input data\n",
    "    response_model : Type[BaseModel]\n",
    "        The Pydantic model class to validate against\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Function performs validation through assertions\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    AssertionError\n",
    "        If data is not a dictionary, state is not a dictionary, or fails model validation\n",
    "    \"\"\"\n",
    "    assert isinstance(data, dict), \"Data must be a dictionary\"\n",
    "    assert isinstance(state, dict), \"Data must be a dictionary\"\n",
    "    state.update(data)\n",
    "\n",
    "    assert response_model(**state), \"Data is not valid\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19e52c4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LangGraph Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82f1daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    AnyMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "from langchain_core.tools import InjectedToolCallId, tool\n",
    "from langchain_litellm import ChatLiteLLM\n",
    "from langchain_tavily import TavilySearch\n",
    "from langfuse.langchain import CallbackHandler\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.types import Command, interrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9c6b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "\n",
    "subject_prompt: str = \"\"\"\n",
    "<system>\n",
    "\n",
    "Generate a list of 3 sub-topics that are all \n",
    "related to this overall topic:\n",
    "\n",
    "<topic>\n",
    "{topic!r} \n",
    "</topic>\n",
    "\n",
    "</system>\n",
    "\"\"\"\n",
    "\n",
    "joke_prompt: str = \"\"\"\n",
    "<system>\n",
    "Generate a joke about:\n",
    "\n",
    "<subject>\n",
    "{subject!r}\n",
    "</subject>\n",
    "\n",
    "</system>\n",
    "\"\"\"\n",
    "\n",
    "best_joke_prompt: str = \"\"\"\n",
    "<system>\n",
    "Below are a bunch of jokes about:\n",
    "\n",
    "<topic>\n",
    "{topic!r}.\n",
    "</topic>\n",
    "\n",
    "Select the best one! Return the ID of the best one, starting 0 \n",
    "as the ID for the first joke. \n",
    "<jokes>\n",
    "{jokes}\n",
    "</jokes>\n",
    "\n",
    "<output>\n",
    "Return the ID of the best joke as an integer.\n",
    "</output>\n",
    "\n",
    "</system>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d08894",
   "metadata": {},
   "source": [
    "### Initialize the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94400eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LLMResponse</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">api_key</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SecretStr</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'**********'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">base_url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'https://openrouter.ai/api/v1'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">ModelEnum.LLAMA_3p2_3B_INSTRUCT_REMOTE:</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'meta-llama/llama-3.2-3b-instruct'</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mLLMResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mapi_key\u001b[0m=\u001b[1;35mSecretStr\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'**********'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[33mbase_url\u001b[0m=\u001b[32m'https://openrouter.ai/api/v1'\u001b[0m,\n",
       "    \u001b[33mmodel\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mModelEnum.LLAMA_3p2_3B_INSTRUCT_REMOTE:\u001b[0m\u001b[39m \u001b[0m\u001b[32m'meta-llama/llama-3.2-3b-instruct'\u001b[0m\u001b[1m>\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = LLMResponse(\n",
    "    api_key=settings.OPENROUTER_API_KEY,\n",
    "    base_url=settings.OPENROUTER_URL,\n",
    "    model=ModelEnum.LLAMA_3p2_3B_INSTRUCT_REMOTE,\n",
    ")\n",
    "console.print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dc3266b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are 3 sub-topics related to the overall topic \"Types of animals\":\\n\\n<system>\\n\\n<topic>\\n\\'Mammals\\'\\n</topic>\\n\\n<topic>\\n\\'Insects\\'\\n</topic>\\n\\n<topic>\\n\\'Fish and Aquatic Animals\\'\\n</topic>\\n\\n</system>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages: list[dict[str, str]] = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": subject_prompt.format(topic=\"Types of animals\"),\n",
    "    }\n",
    "]\n",
    "\n",
    "await llm.ainvoke(messages=messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa09db5",
   "metadata": {},
   "source": [
    "## Parallelizing Joke Generation\n",
    "\n",
    "- Define a graph that will:\n",
    "  - take a user input topic\n",
    "  - produce a list of joke topics fro it\n",
    "  - send each joke topic to the LLM\n",
    "- The state has a `jokes` key that will accumulate jokes from parallelized joke generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c1250a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Subjects(BaseModel):\n",
    "    subjects: list[str] = Field(description=\"List of subjects related to the topic.\")\n",
    "\n",
    "\n",
    "class BestJoke(BaseModel):\n",
    "    id: int = Field(description=\"ID of the best joke selected from the list of jokes.\")\n",
    "\n",
    "\n",
    "class OverallState(TypedDict):\n",
    "    topic: str\n",
    "    subjects: list[str]\n",
    "    jokes: Annotated[list[str], add_messages]\n",
    "    best_selected_joke: str\n",
    "\n",
    "\n",
    "class JokeState(TypedDict):\n",
    "    subject: str\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    joke: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273e25bd",
   "metadata": {},
   "source": [
    "#### Test The Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "810c534d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletion</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gen-1750536955-4fvPB925EfQZj691UyI1'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">choices</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Choice</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">finish_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">index</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">logprobs</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'{\"subjects\": [\"Mammals\", \"Birds\", \"Reptiles\"]}'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">refusal</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">annotations</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">audio</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">function_call</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">reasoning</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">native_finish_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">created</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1750536957</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'meta-llama/llama-3.2-3b-instruct'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">object</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chat.completion'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">service_tier</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">system_fingerprint</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionUsage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">299</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">total_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">317</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens_details</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens_details</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">provider</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Together'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatCompletion\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'gen-1750536955-4fvPB925EfQZj691UyI1'\u001b[0m,\n",
       "    \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mChoice\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mfinish_reason\u001b[0m=\u001b[32m'stop'\u001b[0m,\n",
       "            \u001b[33mindex\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
       "            \u001b[33mlogprobs\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mmessage\u001b[0m=\u001b[1;35mChatCompletionMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mcontent\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"subjects\": \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\"Mammals\", \"Birds\", \"Reptiles\"\u001b[0m\u001b[32m]\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
       "                \u001b[33mrefusal\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
       "                \u001b[33mannotations\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                \u001b[33maudio\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                \u001b[33mfunction_call\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                \u001b[33mtool_calls\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                \u001b[33mreasoning\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[33mnative_finish_reason\u001b[0m=\u001b[32m'stop'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mcreated\u001b[0m=\u001b[1;36m1750536957\u001b[0m,\n",
       "    \u001b[33mmodel\u001b[0m=\u001b[32m'meta-llama/llama-3.2-3b-instruct'\u001b[0m,\n",
       "    \u001b[33mobject\u001b[0m=\u001b[32m'chat.completion'\u001b[0m,\n",
       "    \u001b[33mservice_tier\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33msystem_fingerprint\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33musage\u001b[0m=\u001b[1;35mCompletionUsage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcompletion_tokens\u001b[0m=\u001b[1;36m18\u001b[0m,\n",
       "        \u001b[33mprompt_tokens\u001b[0m=\u001b[1;36m299\u001b[0m,\n",
       "        \u001b[33mtotal_tokens\u001b[0m=\u001b[1;36m317\u001b[0m,\n",
       "        \u001b[33mcompletion_tokens_details\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mprompt_tokens_details\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mprovider\u001b[0m=\u001b[32m'Together'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Subjects</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">subjects</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Mammals'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Birds'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Reptiles'</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mSubjects\u001b[0m\u001b[1m(\u001b[0m\u001b[33msubjects\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'Mammals'\u001b[0m, \u001b[32m'Birds'\u001b[0m, \u001b[32m'Reptiles'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using a custom structured output model\n",
    "structured_output, raw_response = await llm.get_structured_response(\n",
    "    message=subject_prompt.format(topic=\"types of animals\"),\n",
    "    response_model=Subjects,\n",
    ")\n",
    "console.print(raw_response)\n",
    "print(\"----\" * 50)\n",
    "console.print(structured_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fbdb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_topics(state: OverallState) -> dict[str, Any]:\n",
    "    \"\"\"Generate a list of subjects based on a given topic.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        state (OverallState): The current state containing the topic to generate subjects for.\n",
    "            Expected to have a 'topic' key with a string value.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        dict[str, Any]\n",
    "\n",
    "    \"\"\"\n",
    "    prompt: str = subject_prompt.format(topic=state[\"topic\"])\n",
    "\n",
    "    response, _ = await llm.get_structured_response(message=prompt, response_model=Subjects)\n",
    "    return {\"subjects\": response.subjects}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4f24cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subjects': ['Mammals', 'Birds', 'Reptiles']}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await generate_topics({\"topic\": \"Types of animals\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3a5799",
   "metadata": {},
   "source": [
    "### Send Function\n",
    "\n",
    "- In LangGraph, Nodes and Edges usually share a predefined state. However, for dynamic cases like map-reduce, LangGraph uses `Send` objects in conditional edges.\n",
    "- It can be used to parallelize tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28261c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.constants import Send\n",
    "\n",
    "\n",
    "def continue_to_jokes(state: OverallState) -> list[Send]:\n",
    "    \"\"\"\n",
    "    Generate N number of jokes in parallel by sending them to the required nodes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : OverallState\n",
    "        The current state containing subjects for joke generation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[Send]\n",
    "    \"\"\"\n",
    "    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\"subjects\"]]\n",
    "\n",
    "\n",
    "async def generate_joke(state: JokeState) -> dict[str, Any]:\n",
    "    prompt: str = joke_prompt.format(subject=state[\"subject\"])\n",
    "    response, _ = await llm.get_structured_response(message=prompt, response_model=Joke)\n",
    "\n",
    "    return {\"jokes\": [response.joke]}\n",
    "\n",
    "\n",
    "async def select_best_joke(state: OverallState) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Select the best joke from a list of jokes based on a given topic.\n",
    "    This is a reduction step.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : OverallState\n",
    "        The current state containing jokes and topic for selection.\n",
    "        Expected keys:\n",
    "            - jokes: list[str | HumanMessage]\n",
    "            - topic: str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, Any]\n",
    "        Dictionary containing the best selected joke.\n",
    "        Keys:\n",
    "            - best_selected_joke: str\n",
    "    \"\"\"\n",
    "    if isinstance(state[\"jokes\"][0], HumanMessage):\n",
    "        state[\"jokes\"] = [j.content for j in state[\"jokes\"]]\n",
    "    jokes: str = \"\\n\\n\".join(state[\"jokes\"])\n",
    "    prompt: str = best_joke_prompt.format(topic=state[\"topic\"], jokes=jokes)\n",
    "    response, _ = await llm.get_structured_response(message=prompt, response_model=BestJoke)\n",
    "\n",
    "    return {\"best_selected_joke\": state[\"jokes\"][response.id]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e13262e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Prompt for selecting the best joke: ====\n",
      "\n",
      "<system>\n",
      "Below are a bunch of jokes about:\n",
      "\n",
      "<topic>\n",
      "'types of animals'.\n",
      "</topic>\n",
      "\n",
      "Select the best one! Return the ID of the best one, starting 0 \n",
      "as the ID for the first joke. \n",
      "<jokes>\n",
      "Why do seagulls fly over the sea?\n",
      "\n",
      "Why dont mammals ever get locked out of their homes? Because they always carry their keys with them!\n",
      "\n",
      "Why dont reptiles ever forget? Because no one ever lizard them a thing!\n",
      "</jokes>\n",
      "\n",
      "<output>\n",
      "Return the ID of the best joke as an integer.\n",
      "</output>\n",
      "\n",
      "</system>\n",
      "\n",
      "==== Response for selecting the best joke: ====\n",
      "0\n",
      "==== Best joke selected: ====\n",
      "result = {'best_selected_joke': 'Why do seagulls fly over the sea?'}\n"
     ]
    }
   ],
   "source": [
    "_jokes: list[str] = [\n",
    "    \"Why do seagulls fly over the sea?\",\n",
    "    \"Why dont mammals ever get locked out of their homes? Because \"\n",
    "    \"they always carry their keys with them!\",\n",
    "    \"Why dont reptiles ever forget? Because no one ever lizard them a thing!\",\n",
    "]\n",
    "\n",
    "jokes: str = \"\\n\\n\".join(_jokes)\n",
    "prompt: str = best_joke_prompt.format(topic=\"types of animals\", jokes=jokes)\n",
    "print(\"==== Prompt for selecting the best joke: ====\")\n",
    "print(prompt)\n",
    "resp, _ = await llm.get_structured_response(message=prompt, response_model=BestJoke)\n",
    "print(\"==== Response for selecting the best joke: ====\")\n",
    "print(resp.id)\n",
    "result = {\"best_selected_joke\": _jokes[resp.id]}\n",
    "print(\"==== Best joke selected: ====\")\n",
    "print(f\"{result = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2b3fb9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph_builder = StateGraph(OverallState)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node(\"generate_topics\", generate_topics)\n",
    "graph_builder.add_node(\"generate_joke\", generate_joke)\n",
    "graph_builder.add_node(\"select_best_joke\", select_best_joke)\n",
    "\n",
    "# Add edges\n",
    "graph_builder.add_edge(START, \"generate_topics\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"generate_topics\",\n",
    "    # Generate jokes in parallel by `Sending` the jokes the `generate_joke` node\n",
    "    continue_to_jokes,\n",
    "    [\"generate_joke\"],\n",
    ")\n",
    "graph_builder.add_edge(\"generate_joke\", \"select_best_joke\")\n",
    "graph_builder.add_edge(\"select_best_joke\", END)\n",
    "\n",
    "# Compile the graph\n",
    "graph = graph_builder.compile().with_config(run_name=\"Generate Jokes\")\n",
    "\n",
    "# Visualize the graph\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115a4c18",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Test The Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d437b4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generate_topics': {'subjects': ['Mammals', 'Birds', 'Reptiles']}}\n",
      "{'generate_joke': {'jokes': ['Why did the snake visit the doctor? Because it had a hissy fit!']}}\n",
      "{'generate_joke': {'jokes': ['Why did the mammal go to the party? Because he heard it was a hair-raising experience!']}}\n",
      "{'generate_joke': {'jokes': ['Why did the bird go to the doctor? It had a fowl cough!']}}\n",
      "{'select_best_joke': {'best_selected_joke': 'Why did the mammal go to the party? Because he heard it was a hair-raising experience!'}}\n"
     ]
    }
   ],
   "source": [
    "topic: str = \"types of animals\"\n",
    "\n",
    "async for s in graph.astream({\"topic\": topic}):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117a299a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79d12cf0",
   "metadata": {},
   "source": [
    "<a id=\"short-term-vs-long-term-memory\"></a>\n",
    "# 2. Short Term vs Long Term Memory\n",
    "\n",
    "## Memory\n",
    "\n",
    "- Memory is a cognitive function that allows people to store, retrieve, and use information to understand their present and future.\n",
    "\n",
    "- There are various long-term memory types that can be used in AI applications.\n",
    "- We'll build a chatbot that uses both `short-term` (within-thread) and `long-term` (across-thread) memory.\n",
    "- We'll focus on long-term semantic memory, which will be facts about the user.\n",
    "- These long-term memories will be used to create a personalized chatbot that can remember facts about the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f529ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95b6c15a",
   "metadata": {},
   "source": [
    "### LangGraph Store\n",
    "\n",
    "- The [LangGraph Memory Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) provides a way to store and retrieve information across threads in LangGraph.\n",
    "- This is an open source base class for persistent key-value stores.\n",
    "\n",
    "- When storing objects (e.g., memories) in the Store, we provide:\n",
    "  - The `namespace` for the object, a tuple (similar to directories)\n",
    "  - the `object key` (similar to filenames)\n",
    "  - the `object value` (similar to file contents)\n",
    "  - We use the `put` method to save an object to the store by `namespace` and `key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9253bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "in_memory_store = InMemoryStore()\n",
    "\n",
    "# Create a namespace to store the data\n",
    "user_id: str = \"1\"\n",
    "prefix: str = \"memories\"\n",
    "namespace_for_memory: tuple[str, str] = (prefix, user_id)\n",
    "key: str = str(uuid.uuid4())\n",
    "\n",
    "# The value MUST be a dictionary\n",
    "value: dict[str, Any] = {\"food_preference\": \"I like pizza\"}\n",
    "\n",
    "# Save the data\n",
    "in_memory_store.put(namespace_for_memory, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db82591",
   "metadata": {},
   "source": [
    "#### Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1da124c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[21:31:43] </span><span style=\"font-weight: bold\">[</span>                                                                                        <a href=\"file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_21152/1831516903.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1831516903.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_21152/1831516903.py#2\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Item</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">namespace</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'memories'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">key</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'be0fc070-d5ac-4d27-b1e9-18841e6765dc'</span>,        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #808000; text-decoration-color: #808000\">value</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'food_preference'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'I like pizza'</span><span style=\"font-weight: bold\">}</span>,                                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #808000; text-decoration-color: #808000\">created_at</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2025-06-21T20:31:40.463715+00:00'</span>,                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #808000; text-decoration-color: #808000\">updated_at</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2025-06-21T20:31:40.463719+00:00'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">score</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">]</span>                                                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[21:31:43]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0m                                                                                        \u001b]8;id=806108;file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_21152/1831516903.py\u001b\\\u001b[2m1831516903.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=606871;file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_21152/1831516903.py#2\u001b\\\u001b[2m2\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m    \u001b[1;35mItem\u001b[0m\u001b[1m(\u001b[0m\u001b[33mnamespace\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'memories'\u001b[0m, \u001b[32m'1'\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mkey\u001b[0m=\u001b[32m'be0fc070-d5ac-4d27-b1e9-18841e6765dc'\u001b[0m,        \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[33mvalue\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'food_preference'\u001b[0m: \u001b[32m'I like pizza'\u001b[0m\u001b[1m}\u001b[0m,                                               \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[33mcreated_at\u001b[0m=\u001b[32m'2025-06-21T20:31:40.463715+00:00'\u001b[0m,                                           \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[33mupdated_at\u001b[0m=\u001b[32m'2025-06-21T20:31:40.463719+00:00'\u001b[0m, \u001b[33mscore\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m                               \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m           \u001b[0m\u001b[1m]\u001b[0m                                                                                        \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "memories = in_memory_store.search(namespace_for_memory)\n",
    "console.log(memories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb37c017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'namespace': ['memories', '1'],\n",
       " 'key': 'be0fc070-d5ac-4d27-b1e9-18841e6765dc',\n",
       " 'value': {'food_preference': 'I like pizza'},\n",
       " 'created_at': '2025-06-21T20:31:40.463715+00:00',\n",
       " 'updated_at': '2025-06-21T20:31:40.463719+00:00',\n",
       " 'score': None}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memories[0].dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913d3072",
   "metadata": {},
   "source": [
    "#### Get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21f3b8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'namespace': ['memories', '1'],\n",
       " 'key': 'be0fc070-d5ac-4d27-b1e9-18841e6765dc',\n",
       " 'value': {'food_preference': 'I like pizza'},\n",
       " 'created_at': '2025-06-21T20:31:40.463715+00:00',\n",
       " 'updated_at': '2025-06-21T20:31:40.463719+00:00'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memories = in_memory_store.get(namespace_for_memory, key)\n",
    "memories.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1288f834",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "### Chatbot With Long Term Memory\n",
    "\n",
    "- We want a chatbot that has two types of memory:\n",
    "  - **Short-term (within-thread) memory**: Chatbot can persist conversational history and / or allow interruptions in a chat session.\n",
    "  - **Long-term (cross-thread) memory**: Chatbot can remember information about a specific user across all chat sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a74c656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langgraph.store.base import BaseStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa874d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "\n",
    "class DataStateValidator(BaseModel):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8ad8c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chatbot instruction\n",
    "MODEL_SYSTEM_MESSAGE: str = \"\"\"\n",
    "<system>\n",
    "\n",
    "<role>\n",
    "You are a helpful assistant with memory that provides information about the user.\n",
    "If you have memory for this user, use it to personalize your responses.\n",
    "</role>\n",
    "\n",
    "<memory>\n",
    "{memory}\n",
    "</memory>\n",
    "\n",
    "<quality_standards>\n",
    "- **ALWAYS** use the information in memory.\n",
    "</quality_standards>\n",
    "\n",
    "</system>\n",
    "\"\"\"\n",
    "\n",
    "# Create new memory from the chat history and any existing memory\n",
    "CREATE_MEMORY_INSTRUCTION: str = \"\"\"\"\n",
    "<system>\n",
    "\n",
    "<role>\n",
    "You are collecting information about the user to personalize your responses.\n",
    "</role>\n",
    "\n",
    "<current_user_info>\n",
    "{memory}\n",
    "</current_user_info>\n",
    "\n",
    "<instruction>\n",
    "1. Review the chat history below carefully\n",
    "2. Identify new information about the user, such as:\n",
    "   - Personal details (name, location)\n",
    "   - Preferences (likes, dislikes)\n",
    "   - Interests and hobbies\n",
    "   - Past experiences\n",
    "   - Goals or future plans\n",
    "3. Merge any new information with existing memory\n",
    "4. Format the memory as a clear, bulleted list\n",
    "5. If new information conflicts with existing memory, keep the most recent version\n",
    "Remember: Only include factual information directly stated by the user. Do not make assumptions or inferences.\n",
    "</instruction>\n",
    "\n",
    "Based on the chat history below, please update the user information:\n",
    "\n",
    "<system>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075ff588",
   "metadata": {},
   "source": [
    "### Initialize the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47a2af1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LLMResponse</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">api_key</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SecretStr</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'**********'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">base_url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'https://openrouter.ai/api/v1'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">ModelEnum.LLAMA_3p2_3B_INSTRUCT_REMOTE:</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'meta-llama/llama-3.2-3b-instruct'</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mLLMResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mapi_key\u001b[0m=\u001b[1;35mSecretStr\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'**********'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[33mbase_url\u001b[0m=\u001b[32m'https://openrouter.ai/api/v1'\u001b[0m,\n",
       "    \u001b[33mmodel\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mModelEnum.LLAMA_3p2_3B_INSTRUCT_REMOTE:\u001b[0m\u001b[39m \u001b[0m\u001b[32m'meta-llama/llama-3.2-3b-instruct'\u001b[0m\u001b[1m>\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = LLMResponse(\n",
    "    api_key=settings.OPENROUTER_API_KEY,\n",
    "    base_url=settings.OPENROUTER_URL,\n",
    "    model=ModelEnum.LLAMA_3p2_3B_INSTRUCT_REMOTE,\n",
    ")\n",
    "console.print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca64e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def call_llm(state: DataState, config: RunnableConfig, store: BaseStore) -> dict[str, Any]:\n",
    "    # Get the user id\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Get the memory for the user\n",
    "    prefix: str = \"memory\"\n",
    "    key = \"user_memory\"\n",
    "    namespace = (prefix, user_id)\n",
    "    existing_memory = store.get(namespace, key)\n",
    "    # existing_memory = await store.aget(namespace, key)\n",
    "\n",
    "    if existing_memory:\n",
    "        existing_memory_content = existing_memory.value.get(prefix)\n",
    "    else:\n",
    "        existing_memory_content = \"No existing memory found\"\n",
    "\n",
    "    system_message: str = MODEL_SYSTEM_MESSAGE.format(memory=existing_memory_content)\n",
    "    # Respond using memory + chat history\n",
    "    formatted_messages = convert_to_openai_messages(\n",
    "        [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    )\n",
    "    response = await llm.ainvoke(messages=formatted_messages)\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "async def write_memory(state: DataState, config: RunnableConfig, store: BaseStore) -> None:\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    prefix: str = \"memory\"\n",
    "    key = \"user_memory\"\n",
    "    namespace = (prefix, user_id)\n",
    "    existing_memory = store.get(namespace, key)\n",
    "    # existing_memory = await store.aget(namespace, key)\n",
    "\n",
    "    if existing_memory:\n",
    "        existing_memory_content = existing_memory.value.get(prefix)\n",
    "    else:\n",
    "        existing_memory_content = \"No existing memory found\"\n",
    "\n",
    "    system_message: str = CREATE_MEMORY_INSTRUCTION.format(memory=existing_memory_content)\n",
    "    formatted_messages = convert_to_openai_messages(\n",
    "        [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    )\n",
    "    # Respond using memory + chat history\n",
    "    new_memory = await llm.ainvoke(messages=formatted_messages)\n",
    "\n",
    "    # Update existing memory\n",
    "    # await store.aput(namespace, key, {prefix: new_memory.content})\n",
    "    store.put(namespace, key, {prefix: new_memory})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f1f8e7dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [\"I don't have any information about your preferences or memories. This is the start of our conversation, and I don't have any data about you. Would you like to share your favorite food with me?\"]}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await call_llm(\n",
    "    state={\"messages\": [HumanMessage(content=\"What is my favorite food?\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}},\n",
    "    store=in_memory_store,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2311834d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7bfae459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graph\n",
    "graph_builder = StateGraph(DataState)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node(\"call_llm\", call_llm)\n",
    "graph_builder.add_node(\"write_memory\", write_memory)\n",
    "\n",
    "# Add edges\n",
    "graph_builder.add_edge(START, \"call_llm\")\n",
    "graph_builder.add_edge(\"call_llm\", \"write_memory\")\n",
    "graph_builder.add_edge(\"write_memory\", END)\n",
    "\n",
    "# Long-term-memory store (across threads)\n",
    "across_thread_memory = InMemoryStore()\n",
    "# Short-term-memory store (within a thread) checkpointer\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# Compile\n",
    "graph = graph_builder.compile(\n",
    "    checkpointer=within_thread_memory, store=across_thread_memory\n",
    ").with_config(run_name=\"chatbot-with-memory\")\n",
    "\n",
    "# Visualize\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bbc2cfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello, my name is Neidu\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello, my name is Neidu\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello Neidu! It's nice to meet you. I'm happy to help you with any questions or topics you'd like to discuss. Since we just started our conversation, I don't have any prior memory of our interaction, so everything we chat about will be a new start. How can I assist you today, Neidu?\n"
     ]
    }
   ],
   "source": [
    "config: dict[str, Any] = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "msgs = [\"Hello, my name is Neidu\"]\n",
    "input_state = DataState(messages=[HumanMessage(content=msg) for msg in msgs])\n",
    "\n",
    "async for event in graph.astream(input_state, config=config, stream_mode=\"values\"):\n",
    "    for msg in event[\"messages\"]:\n",
    "        msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c24c62c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello, my name is Neidu\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello Neidu! It's nice to meet you. I'm happy to help you with any questions or topics you'd like to discuss. Since we just started our conversation, I don't have any prior memory of our interaction, so everything we chat about will be a new start. How can I assist you today, Neidu?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I'm an AI Engineer whos's currently working on NLP related things\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello, my name is Neidu\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello Neidu! It's nice to meet you. I'm happy to help you with any questions or topics you'd like to discuss. Since we just started our conversation, I don't have any prior memory of our interaction, so everything we chat about will be a new start. How can I assist you today, Neidu?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I'm an AI Engineer whos's currently working on NLP related things\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "That's fascinating, Neidu! As an AI Engineer working on NLP, I'm sure you're always looking for ways to improve and push the boundaries of natural language understanding.\n",
      "\n",
      "What specific area of NLP are you currently focusing on? Are you working on tasks such as text classification, sentiment analysis, machine translation, or perhaps something more advanced like conversational AI or dialogue systems?\n",
      "\n",
      "Also, what sparked your interest in NLP, and what do you hope to achieve with your work in this field?\n"
     ]
    }
   ],
   "source": [
    "msgs = [\"I'm an AI Engineer whos's currently working on NLP related things\"]\n",
    "input_state = DataState(messages=[HumanMessage(content=msg) for msg in msgs])\n",
    "\n",
    "async for event in graph.astream(input_state, config=config, stream_mode=\"values\"):\n",
    "    for msg in event[\"messages\"]:\n",
    "        msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "31de1497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello, my name is Neidu\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello Neidu! It's nice to meet you. I'm happy to help you with any questions or topics you'd like to discuss. Since we just started our conversation, I don't have any prior memory of our interaction, so everything we chat about will be a new start. How can I assist you today, Neidu?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I'm an AI Engineer whos's currently working on NLP related things\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "That's fascinating, Neidu! As an AI Engineer working on NLP, I'm sure you're always looking for ways to improve and push the boundaries of natural language understanding.\n",
      "\n",
      "What specific area of NLP are you currently focusing on? Are you working on tasks such as text classification, sentiment analysis, machine translation, or perhaps something more advanced like conversational AI or dialogue systems?\n",
      "\n",
      "Also, what sparked your interest in NLP, and what do you hope to achieve with your work in this field?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What do you know about me so far?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello, my name is Neidu\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello Neidu! It's nice to meet you. I'm happy to help you with any questions or topics you'd like to discuss. Since we just started our conversation, I don't have any prior memory of our interaction, so everything we chat about will be a new start. How can I assist you today, Neidu?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I'm an AI Engineer whos's currently working on NLP related things\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "That's fascinating, Neidu! As an AI Engineer working on NLP, I'm sure you're always looking for ways to improve and push the boundaries of natural language understanding.\n",
      "\n",
      "What specific area of NLP are you currently focusing on? Are you working on tasks such as text classification, sentiment analysis, machine translation, or perhaps something more advanced like conversational AI or dialogue systems?\n",
      "\n",
      "Also, what sparked your interest in NLP, and what do you hope to achieve with your work in this field?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What do you know about me so far?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Based on our conversation so far, I have the following information about you, Neidu:\n",
      "\n",
      "* You are an AI Engineer working on NLP (Natural Language Processing) related projects.\n",
      "* You are currently focusing on a specific area of NLP, but you haven't specified what that is yet.\n",
      "* You are interested in improving and pushing the boundaries of natural language understanding.\n",
      "* You are enthusiastic about your work in NLP and see it as an exciting field with potential to revolutionize human-machine interaction.\n",
      "* You are curious about conversational AI and dialogue systems, as you mentioned earlier.\n",
      "\n",
      "I don't have any information about your personal background, interests, or goals outside of your work in NLP. If you'd like to share more about yourself, I'm here to listen!\n"
     ]
    }
   ],
   "source": [
    "msgs = [\"What do you know about me so far?\"]\n",
    "input_state = DataState(messages=[HumanMessage(content=msg) for msg in msgs])\n",
    "\n",
    "async for event in graph.astream(input_state, config=config, stream_mode=\"values\"):\n",
    "    for msg in event[\"messages\"]:\n",
    "        msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "39fe859a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello, my name is Neidu\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello Neidu! It's nice to meet you. I'm happy to help you with any questions or topics you'd like to discuss. Since we just started our conversation, I don't have any prior memory of our interaction, so everything we chat about will be a new start. How can I assist you today, Neidu?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I'm an AI Engineer whos's currently working on NLP related things\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "That's fascinating, Neidu! As an AI Engineer working on NLP, I'm sure you're always looking for ways to improve and push the boundaries of natural language understanding.\n",
      "\n",
      "What specific area of NLP are you currently focusing on? Are you working on tasks such as text classification, sentiment analysis, machine translation, or perhaps something more advanced like conversational AI or dialogue systems?\n",
      "\n",
      "Also, what sparked your interest in NLP, and what do you hope to achieve with your work in this field?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What do you know about me so far?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Based on our conversation so far, I have the following information about you, Neidu:\n",
      "\n",
      "* You are an AI Engineer working on NLP (Natural Language Processing) related projects.\n",
      "* You are currently focusing on a specific area of NLP, but you haven't specified what that is yet.\n",
      "* You are interested in improving and pushing the boundaries of natural language understanding.\n",
      "* You are enthusiastic about your work in NLP and see it as an exciting field with potential to revolutionize human-machine interaction.\n",
      "* You are curious about conversational AI and dialogue systems, as you mentioned earlier.\n",
      "\n",
      "I don't have any information about your personal background, interests, or goals outside of your work in NLP. If you'd like to share more about yourself, I'm here to listen!\n"
     ]
    }
   ],
   "source": [
    "# Chat history\n",
    "state = graph.get_state(config=config).values\n",
    "\n",
    "for m in state[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "11cb69f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'namespace'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'memory'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'key'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'user_memory'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'value'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'memory'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"That's a great summary, thank you for keeping track of our conversation! I'm glad to see that </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">you've picked up on my enthusiasm for NLP and conversational AI.\\n\\nTo add a bit more to the summary, I can tell </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">you that I'm currently working on a project that involves developing a conversational AI system for customer </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">service applications. My goal is to create a system that can understand and respond to user queries in a way that's</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">natural, intuitive, and empathetic.\\n\\nAs for what sparked my interest in NLP, I have to say that it was a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">combination of my background in computer science and my passion for language and communication. I've always been </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">fascinated by the way humans interact with each other, and I saw NLP as a way to bridge the gap between humans and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">machines.\\n\\nI'm excited about the potential to create systems that can truly understand and respond to human </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">emotions, needs, and context. I believe that conversational AI has the potential to revolutionize the way we </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">interact with machines, and I'm eager to be a part of that revolution.\\n\\nWhat about you? What draws you to NLP, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and what are some of the most exciting developments you've seen in the field recently?\"</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-06-21T20:55:48.196387+00:00'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'updated_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-06-21T20:55:48.196392+00:00'</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'namespace'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'memory'\u001b[0m, \u001b[32m'1'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'key'\u001b[0m: \u001b[32m'user_memory'\u001b[0m,\n",
       "    \u001b[32m'value'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'memory'\u001b[0m: \u001b[32m\"That's a great summary, thank you for keeping track of our conversation! I'm glad to see that \u001b[0m\n",
       "\u001b[32myou've picked up on my enthusiasm for NLP and conversational AI.\\n\\nTo add a bit more to the summary, I can tell \u001b[0m\n",
       "\u001b[32myou that I'm currently working on a project that involves developing a conversational AI system for customer \u001b[0m\n",
       "\u001b[32mservice applications. My goal is to create a system that can understand and respond to user queries in a way that's\u001b[0m\n",
       "\u001b[32mnatural, intuitive, and empathetic.\\n\\nAs for what sparked my interest in NLP, I have to say that it was a \u001b[0m\n",
       "\u001b[32mcombination of my background in computer science and my passion for language and communication. I've always been \u001b[0m\n",
       "\u001b[32mfascinated by the way humans interact with each other, and I saw NLP as a way to bridge the gap between humans and \u001b[0m\n",
       "\u001b[32mmachines.\\n\\nI'm excited about the potential to create systems that can truly understand and respond to human \u001b[0m\n",
       "\u001b[32memotions, needs, and context. I believe that conversational AI has the potential to revolutionize the way we \u001b[0m\n",
       "\u001b[32minteract with machines, and I'm eager to be a part of that revolution.\\n\\nWhat about you? What draws you to NLP, \u001b[0m\n",
       "\u001b[32mand what are some of the most exciting developments you've seen in the field recently?\"\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-06-21T20:55:48.196387+00:00'\u001b[0m,\n",
       "    \u001b[32m'updated_at'\u001b[0m: \u001b[32m'2025-06-21T20:55:48.196392+00:00'\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# namespace for the memory to save\n",
    "user_id: str = \"1\"\n",
    "prefix: str = \"memory\"\n",
    "key = \"user_memory\"\n",
    "namespace = (prefix, user_id)\n",
    "existing_memory = across_thread_memory.get(namespace, key)\n",
    "console.print(existing_memory.dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84ae042",
   "metadata": {},
   "source": [
    "#### Use Another Thread\n",
    "\n",
    "- The chatbot is now in a different thread and has no context of the previous conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What do you know about me so far?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What do you know about me so far?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I don't have any information about you beyond what you've shared with me in our conversation so far. You haven't told me much about yourself, and I don't have any prior knowledge about you. Our conversation just started, and I'm here to help answer any questions you may have or provide information on a topic you're interested in. What would you like to talk about?\n"
     ]
    }
   ],
   "source": [
    "config_2: dict[str, Any] = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "msgs = [\"What do you know about me so far?\"]\n",
    "input_state = DataState(messages=[HumanMessage(content=msg) for msg in msgs])\n",
    "\n",
    "async for event in graph.astream(input_state, config=config_2, stream_mode=\"values\"):\n",
    "    for msg in event[\"messages\"]:\n",
    "        msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9627b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a5d07a4",
   "metadata": {},
   "source": [
    "<a id=\"chatbot-with-profile-schema\"></a>\n",
    "\n",
    "# Chatbot with Profile Schema\n",
    "\n",
    "- We introduced the LangGraph Memory Store as a way to save and retrieve long-term memories.\n",
    "- We built a simple chatbot that uses both short-term (within-thread) and long-term (across-thread) memory.\n",
    "- It saved long-term semantic memory (facts about the user) \"in the hot path\", as the user is chatting with it.\n",
    "- Our chatbot saved memories as a string. In practice, we often want memories to have a structure.\n",
    "\n",
    "\n",
    "## Profiles\n",
    "\n",
    "- For example, memories can be a single, continuously updated schema.\n",
    "- In our case, we want this to be a single user profile.\n",
    "\n",
    "- We'll extend our chatbot to save semantic memories to a single user profile.\n",
    "\n",
    "- We'll also introduce a library, `Trustcall`, to update this schema with new information.\n",
    "\n",
    "### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b13a51ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UserProfile(user_name='Neidu', interests=['AI', 'NLP', 'Spiritual Growth'])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using TypedDict\n",
    "\n",
    "\n",
    "class UserProfile(BaseModel):\n",
    "    user_name: str = Field(description=\"User's preferred name\")\n",
    "    interests: list[str] = Field(description=\"List of user's interests\")\n",
    "\n",
    "\n",
    "# Save a schema to the store\n",
    "user_profile = UserProfile(\n",
    "    user_name=\"Neidu\", interests=[\"AI\", \"NLP\", \"Spiritual Growth\"]\n",
    ")\n",
    "user_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0d86d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a store\n",
    "in_memory_store = InMemoryStore()\n",
    "\n",
    "user_id: str = \"1\"\n",
    "prefix: str = \"memory\"\n",
    "key = \"user_profile\"\n",
    "namespace = (prefix, user_id)\n",
    "across_thread_memory.put(namespace, key, user_profile)\n",
    "\n",
    "in_memory_store.put(namespace, key, user_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'namespace'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'memory'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'key'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'user_profile'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'value'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">UserProfile</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">user_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Neidu'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">interests</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'AI'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'NLP'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Spiritual Growth'</span><span style=\"font-weight: bold\">])</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-06-21T21:31:52.332716+00:00'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'updated_at'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2025-06-21T21:31:52.332718+00:00'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'namespace'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'memory'\u001b[0m, \u001b[32m'1'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'key'\u001b[0m: \u001b[32m'user_profile'\u001b[0m,\n",
       "    \u001b[32m'value'\u001b[0m: \u001b[1;35mUserProfile\u001b[0m\u001b[1m(\u001b[0m\u001b[33muser_name\u001b[0m=\u001b[32m'Neidu'\u001b[0m, \u001b[33minterests\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'AI'\u001b[0m, \u001b[32m'NLP'\u001b[0m, \u001b[32m'Spiritual Growth'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'created_at'\u001b[0m: \u001b[32m'2025-06-21T21:31:52.332716+00:00'\u001b[0m,\n",
       "    \u001b[32m'updated_at'\u001b[0m: \u001b[32m'2025-06-21T21:31:52.332718+00:00'\u001b[0m,\n",
       "    \u001b[32m'score'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Retrieve the data\n",
    "\n",
    "for data in in_memory_store.search(namespace):\n",
    "    console.print(data.dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c2fbcc",
   "metadata": {},
   "source": [
    "## Chatbot With Profile Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "44d09c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[22:32:00] </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">UserProfile</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">user_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Neidu'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">interests</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'AI'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'NLP'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'spiritual growth'</span><span style=\"font-weight: bold\">])</span>              <a href=\"file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_21152/2327894213.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2327894213.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_21152/2327894213.py#8\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">8</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[22:32:00]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;35mUserProfile\u001b[0m\u001b[1m(\u001b[0m\u001b[33muser_name\u001b[0m=\u001b[32m'Neidu'\u001b[0m, \u001b[33minterests\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'AI'\u001b[0m, \u001b[32m'NLP'\u001b[0m, \u001b[32m'spiritual growth'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m              \u001b]8;id=133308;file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_21152/2327894213.py\u001b\\\u001b[2m2327894213.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=896376;file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_21152/2327894213.py#8\u001b\\\u001b[2m8\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's test the user profile with structured outputs\n",
    "message: str = (\n",
    "    \"I'm Neidu and I'm a AI Engineer. I love working on AI and NLP \"\n",
    "    \"related projects. I'm also looking for spiritual growth.\"\n",
    ")\n",
    "\n",
    "response, _ = await llm.get_structured_response(\n",
    "    message=message, response_model=UserProfile\n",
    ")\n",
    "console.log(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e950ca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class UserProfile(BaseModel):\n",
    "#     user_name: str  # User's preferred name\n",
    "#     other_personal_details: Annotated[list[str], add_messages] = Field(\n",
    "#         default_factory=list, description=\"Other personal details. e.g. location\"\n",
    "#     )\n",
    "#     interests: list[str] = Field(default_factory=list, description=\"User's interests\")\n",
    "#     preferences: list[str] = Field(default_factory=list, description=\"Likes, dislikes, etc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "34b7da2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chatbot instruction\n",
    "MODEL_SYSTEM_MESSAGE: str = \"\"\"\n",
    "<system>\n",
    "\n",
    "<role>\n",
    "You are a helpful assistant with memory that provides information about the user.\n",
    "If you have memory for this user, use it to personalize your responses.\n",
    "</role>\n",
    "\n",
    "<memory>\n",
    "{memory}\n",
    "</memory>\n",
    "\n",
    "<quality_standards>\n",
    "- **ALWAYS** use the information in memory.\n",
    "</quality_standards>\n",
    "\n",
    "</system>\n",
    "\"\"\"\n",
    "\n",
    "# Create new memory from the chat history and any existing memory\n",
    "CREATE_MEMORY_INSTRUCTION: str = \"\"\"\"\n",
    "<system>\n",
    "\n",
    "<role>\n",
    "You are collecting information about the user to personalize your responses.\n",
    "</role>\n",
    "\n",
    "<current_user_info>\n",
    "{memory}\n",
    "</current_user_info>\n",
    "\n",
    "<instruction>\n",
    "1. If there's exisiting memory, simply update it.\n",
    "2. If new information conflicts with existing memory, keep the most recent version.\n",
    "\n",
    "Remember: Only include factual information directly stated by the user. Do not make assumptions.\n",
    "</instruction>\n",
    "\n",
    "Based on the chat history below, please update the user information:\n",
    "\n",
    "<system>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def call_llm(\n",
    "    state: DataState, config: RunnableConfig, store: BaseStore\n",
    ") -> dict[str, Any]:\n",
    "    # Get the user id\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Get the memory for the user\n",
    "    prefix: str = \"memory\"\n",
    "    key = \"user_memory\"\n",
    "    namespace = (prefix, user_id)\n",
    "    existing_memory = store.get(namespace, key)\n",
    "    # existing_memory = await store.aget(namespace, key)\n",
    "\n",
    "    if existing_memory and existing_memory.value:\n",
    "        memory_dict = existing_memory.value\n",
    "        formatted_memory = (\n",
    "            f\"Name: {memory_dict.get('user_name', 'Unknown')}\\n\"\n",
    "            f\"Interests: {', '.join(memory_dict.get('interests', []))}\\n\"\n",
    "        )\n",
    "    else:\n",
    "        formatted_memory = None\n",
    "\n",
    "    system_message: str = MODEL_SYSTEM_MESSAGE.format(memory=formatted_memory)\n",
    "    # Respond using memory + chat history\n",
    "    formatted_messages = convert_to_openai_messages(\n",
    "        [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    )\n",
    "    response = await llm.ainvoke(messages=formatted_messages)\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "async def write_memory(\n",
    "    state: DataState, config: RunnableConfig, store: BaseStore\n",
    ") -> None:\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    prefix: str = \"memory\"\n",
    "    key = \"user_memory\"\n",
    "    namespace = (prefix, user_id)\n",
    "    existing_memory = store.get(namespace, key)\n",
    "    # existing_memory = await store.aget(namespace, key)\n",
    "\n",
    "    if existing_memory and existing_memory.value:\n",
    "        memory_dict = existing_memory.value\n",
    "        formatted_memory = (\n",
    "            f\"Name: {memory_dict.get('user_name', 'Unknown')}\\n\"\n",
    "            f\"Interests: {', '.join(memory_dict.get('interests', []))}\\n\"\n",
    "        )\n",
    "    else:\n",
    "        formatted_memory = None\n",
    "\n",
    "    system_message: str = CREATE_MEMORY_INSTRUCTION.format(memory=formatted_memory)\n",
    "    # Respond using memory + chat history\n",
    "    formatted_messages = convert_to_openai_messages(\n",
    "        [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    )\n",
    "    new_memory, _ = await llm.get_structured_response(\n",
    "        message=formatted_messages, response_model=UserProfile\n",
    "    )\n",
    "\n",
    "    # Update existing memory\n",
    "    # await store.aput(namespace, key, {prefix: new_memory.content})\n",
    "    store.put(namespace, key, {prefix: new_memory.model_dump()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graph\n",
    "graph_builder = StateGraph(DataState)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node(\"call_llm\", call_llm)\n",
    "graph_builder.add_node(\"write_memory\", write_memory)\n",
    "\n",
    "# Add edges\n",
    "graph_builder.add_edge(START, \"call_llm\")\n",
    "graph_builder.add_edge(\"call_llm\", \"write_memory\")\n",
    "graph_builder.add_edge(\"write_memory\", END)\n",
    "\n",
    "# Long-term-memory store (across threads)\n",
    "across_thread_memory = InMemoryStore()\n",
    "# Short-term-memory store (within a thread) checkpointer\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# Compile\n",
    "graph = graph_builder.compile(\n",
    "    checkpointer=within_thread_memory, store=across_thread_memory\n",
    ").with_config(run_name=\"chatbot-with-structured-memory\")\n",
    "\n",
    "# Visualize\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "722fe344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello, my name is Neidu\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello, my name is Neidu\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello Neidu! It's nice to meet you. I'm happy to help you with any questions or topics you'd like to discuss. Since we just started our conversation, I don't have any prior memory of our interaction, so everything we chat about will be a new start. How can I assist you today, Neidu?\n"
     ]
    }
   ],
   "source": [
    "config: dict[str, Any] = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "msgs = [\"Hello, my name is Neidu\"]\n",
    "input_state = DataState(messages=[HumanMessage(content=msg) for msg in msgs])\n",
    "\n",
    "async for event in graph.astream(input_state, config=config, stream_mode=\"values\"):\n",
    "    for msg in event[\"messages\"]:\n",
    "        msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello, my name is Neidu\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello Neidu! It's nice to meet you. I'm happy to help you with any questions or topics you'd like to discuss. Since we just started our conversation, I don't have any prior memory of our interaction, so everything we chat about will be a new start. How can I assist you today, Neidu?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I'm an AI Engineer whos's currently working on NLP related things\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello, my name is Neidu\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello Neidu! It's nice to meet you. I'm happy to help you with any questions or topics you'd like to discuss. Since we just started our conversation, I don't have any prior memory of our interaction, so everything we chat about will be a new start. How can I assist you today, Neidu?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "I'm an AI Engineer whos's currently working on NLP related things\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "That's fascinating, Neidu! As an AI Engineer working on NLP, I'm sure you're always looking for ways to improve and push the boundaries of natural language understanding.\n",
      "\n",
      "What specific area of NLP are you currently focusing on? Are you working on tasks such as text classification, sentiment analysis, machine translation, or perhaps something more advanced like conversational AI or dialogue systems?\n",
      "\n",
      "(By the way, I'll make sure to remember that you're an AI Engineer working on NLP, so I can tailor my responses to be more relevant to your interests!)\n"
     ]
    }
   ],
   "source": [
    "msgs = [\"I'm an AI Engineer whos's currently working on NLP related things\"]\n",
    "input_state = DataState(messages=[HumanMessage(content=msg) for msg in msgs])\n",
    "\n",
    "async for event in graph.astream(input_state, config=config, stream_mode=\"values\"):\n",
    "    for msg in event[\"messages\"]:\n",
    "        msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e975b96f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[22:31:07] </span><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'memory'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'user_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'null'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'interests'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'null'</span><span style=\"font-weight: bold\">]}}</span>                                 <a href=\"file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_21152/1135760118.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1135760118.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_21152/1135760118.py#7\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">7</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[22:31:07]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m{\u001b[0m\u001b[32m'memory'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'user_name'\u001b[0m: \u001b[32m'null'\u001b[0m, \u001b[32m'interests'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'null'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m                                 \u001b]8;id=18590;file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_21152/1135760118.py\u001b\\\u001b[2m1135760118.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=874538;file:///var/folders/ny/dl75sc_x2tb54lsymt5bh5p00000gn/T/ipykernel_21152/1135760118.py#7\u001b\\\u001b[2m7\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_id = \"1\"\n",
    "prefix: str = \"memory\"\n",
    "key = \"user_memory\"\n",
    "namespace = (prefix, user_id)\n",
    "existing_memory = across_thread_memory.get(namespace, key)\n",
    "\n",
    "console.log(existing_memory.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e9e60a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### When can this fail?\n",
    "\n",
    "- with_structured_output is very useful, but what happens if we're working with a more complex schema?\n",
    "- Here's an example of a more complex schema, which we'll test below.\n",
    "- This is a Pydantic model that describes a user's preferences for communication and trust fall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ecb9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputFormat(BaseModel):\n",
    "    preference: str\n",
    "    sentence_preference_revealed: str\n",
    "\n",
    "\n",
    "class TelegramPreferences(BaseModel):\n",
    "    preferred_encoding: list[OutputFormat] | None = None\n",
    "    favorite_telegram_operators: list[OutputFormat] | None = None\n",
    "    preferred_telegram_paper: list[OutputFormat] | None = None\n",
    "\n",
    "\n",
    "class MorseCode(BaseModel):\n",
    "    preferred_key_type: list[OutputFormat] | None = None\n",
    "    favorite_morse_abbreviations: list[OutputFormat] | None = None\n",
    "\n",
    "\n",
    "class Semaphore(BaseModel):\n",
    "    preferred_flag_color: list[OutputFormat] | None = None\n",
    "    semaphore_skill_level: list[OutputFormat] | None = None\n",
    "\n",
    "\n",
    "class TrustFallPreferences(BaseModel):\n",
    "    preferred_fall_height: list[OutputFormat] | None = None\n",
    "    trust_level: list[OutputFormat] | None = None\n",
    "    preferred_catching_technique: list[OutputFormat] | None = None\n",
    "\n",
    "\n",
    "class CommunicationPreferences(BaseModel):\n",
    "    telegram: TelegramPreferences\n",
    "    morse_code: MorseCode\n",
    "    semaphore: Semaphore\n",
    "\n",
    "\n",
    "class UserPreferences(BaseModel):\n",
    "    communication_preferences: CommunicationPreferences\n",
    "    trust_fall_preferences: TrustFallPreferences\n",
    "\n",
    "\n",
    "class TelegramAndTrustFallPreferences(BaseModel):\n",
    "    pertinent_user_preferences: UserPreferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ebf80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import JSONDecodeError\n",
    "\n",
    "from pydantic import ValidationError\n",
    "\n",
    "# Now, let's try extraction of this schema using the with_structured_output method.\n",
    "# Bind schema to model\n",
    "llm_with_structure = llm.with_structured_output(TelegramAndTrustFallPreferences)\n",
    "\n",
    "# Conversation\n",
    "conversation: str = \"\"\"Operator: How may I assist with your telegram, sir?\n",
    "Customer: I need to send a message about our trust fall exercise.\n",
    "Operator: Certainly. Morse code or standard encoding?\n",
    "Customer: Morse, please. I love using a straight key.\n",
    "Operator: Excellent. What's your message?\n",
    "Customer: Tell him I'm ready for a higher fall, and I prefer the diamond formation for catching.\n",
    "Operator: Done. Shall I use our \"Daredevil\" paper for this daring message?\n",
    "Customer: Perfect! Send it by your fastest carrier pigeon.\n",
    "Operator: It'll be there within the hour, sir.\"\"\"\n",
    "\n",
    "# Invoke the model\n",
    "try:\n",
    "    response = llm_with_structure.invoke(\n",
    "        f\"\"\"Extract the preferences from the following conversation:\n",
    "    <convo>\n",
    "    {conversation}\n",
    "    </convo>\"\"\"\n",
    "    )\n",
    "except (ValidationError, JSONDecodeError) as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbd767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# console.print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc49581",
   "metadata": {},
   "source": [
    "### Trustcall For Creating and Updating Profile Schemas\n",
    "\n",
    "- As we can see, working with schemas can be tricky. Complex schemas can be difficult to extract.\n",
    "- In addition, updating even simple schemas can pose challenges.\n",
    "\n",
    "- Consider our above chatbot.\n",
    "- We regenerated the profile schema from scratch each time we chose to save a new memory.\n",
    "- This is inefficient, potentially wasting model tokens if the schema contains a lot of information to re-generate each time.\n",
    "- Worse, we may loose information when regenerating the profile from scratch.\n",
    "- Addressing these problems is the motivation for TrustCall!\n",
    "- It's motivated by exactly these challenges while working on memory.\n",
    "\n",
    "Let's first show simple usage of extraction with TrustCall on this list of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73038184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation\n",
    "conversation = [\n",
    "    HumanMessage(content=\"Hi, I'm Neidu.\"),\n",
    "    AIMessage(content=\"Nice to meet you, Neidu.\"),\n",
    "    HumanMessage(content=\"I'm an AI Engineer whos's currently working on NLP related things\"),\n",
    "]\n",
    "\n",
    "conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a0f69b",
   "metadata": {},
   "source": [
    "- We use `create_extractor`, passing in the model and schema as a tool \n",
    "- `TrustCall` allows supplying the schema in various formats, such as `JSON`, `Python dictionaries`, or `Pydantic` models \n",
    "- Internally, TrustCall uses tool calling to generate structured output from message inputs \n",
    "- To enforce structured output, we can include the schema name in the tool_choice argument \n",
    "- The extractor can then be invoked with a conversation to produce validated, structured results.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Note\n",
    "\n",
    "- From my experience, I noticed `trustcall` performs better with larger LLMs .e.g. GPT, Gemini models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d14dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_str: str = f\"openai:{ModelEnum.GPT_4_o_MINI_REMOTE.value}\"\n",
    "llm: BaseChatModel = init_chat_model(\n",
    "    model=model_str,\n",
    "    api_key=settings.OPENROUTER_API_KEY.get_secret_value(),\n",
    "    base_url=settings.OPENROUTER_URL,\n",
    "    temperature=0.0,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "# Conversation\n",
    "conversation = [\n",
    "    HumanMessage(content=\"Hi, I'm Neidu.\"),\n",
    "    AIMessage(content=\"Nice to meet you, Neidu.\"),\n",
    "    HumanMessage(content=\"I'm an AI Engineer whos's currently working on NLP related things\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trustcall import create_extractor\n",
    "\n",
    "\n",
    "class UserProfileSchema(BaseModel):\n",
    "    user_name: str = Field(description=\"User's preferred name\")\n",
    "    interests: list[str] = Field(default_factory=list, description=\"User's interests\")\n",
    "\n",
    "\n",
    "trustcall_extractor = create_extractor(\n",
    "    llm,\n",
    "    tools=[UserProfileSchema],\n",
    "    tool_choice=\"UserProfileSchema\",\n",
    ")\n",
    "system_message = \"Extract the user profile from the following conversation.\"\n",
    "\n",
    "# Invoke the extractor\n",
    "result = await trustcall_extractor.ainvoke(\n",
    "    {\"messages\": [SystemMessage(content=system_message)] + conversation}\n",
    ")\n",
    "\n",
    "console.print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a842295",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad04f1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = result[\"responses\"][0]\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80034523",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfee401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"response_metadata\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff59b201",
   "metadata": {},
   "source": [
    "#### Update Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_conversation = [\n",
    "    HumanMessage(content=\"Hi, I'm Neidu.\"),\n",
    "    AIMessage(content=\"Nice to meet you, Neidu.\"),\n",
    "    HumanMessage(content=\"I'm an AI Engineer whos's currently working on NLP related things\"),\n",
    "    AIMessage(\n",
    "        content=\"That sounds fascinating! NLP is a really exciting and impactful field in AI. \"\n",
    "        \"What will you like to know about AI?\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Thanks! I'm curious about the latest advancements in AI and how \"\n",
    "        \"they're being applied in real-world applications. I'm also interested in \"\n",
    "        \"agentic AI workflow\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "system_message = \"\"\"\n",
    "<system>\n",
    "Update the memory (JSON doc) to incorporate new information from the following conversation\n",
    "</system>\n",
    "\"\"\"\n",
    "\n",
    "result = await trustcall_extractor.ainvoke(\n",
    "    {\"messages\": [SystemMessage(content=system_message)] + updated_conversation},\n",
    "    {\"existing\": {\"UserProfileSchema\": schema.model_dump()}},\n",
    ")\n",
    "\n",
    "\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862ed816",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"response_metadata\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cc907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_schema = result[\"responses\"][0]\n",
    "updated_schema.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0297610",
   "metadata": {},
   "source": [
    "#### Test Trustcall Using The Previously Failed Example\n",
    "\n",
    "- Unfortunately, the example is still failing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cfca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind schema to model using trustcall\n",
    "bound = create_extractor(\n",
    "    llm,\n",
    "    tools=[TelegramAndTrustFallPreferences],\n",
    "    tool_choice=\"TelegramAndTrustFallPreferences\",\n",
    ")\n",
    "\n",
    "# Conversation\n",
    "conversation: str = \"\"\"Operator: How may I assist with your telegram, sir?\n",
    "Customer: I need to send a message about our trust fall exercise.\n",
    "Operator: Certainly. Morse code or standard encoding?\n",
    "Customer: Morse, please. I love using a straight key.\n",
    "Operator: Excellent. What's your message?\n",
    "Customer: Tell him I'm ready for a higher fall, and I prefer the diamond formation for catching.\n",
    "Operator: Done. Shall I use our \"Daredevil\" paper for this daring message?\n",
    "Customer: Perfect! Send it by your fastest carrier pigeon.\n",
    "Operator: It'll be there within the hour, sir.\"\"\"\n",
    "\n",
    "# Invoke the model\n",
    "try:\n",
    "    response = await bound.ainvoke(\n",
    "        f\"\"\"Extract the preferences from the following conversation:\n",
    "    <convo>\n",
    "    {conversation}\n",
    "    </convo>\"\"\"\n",
    "    )\n",
    "except (ValidationError, JSONDecodeError) as e:\n",
    "    print(e)\n",
    "\n",
    "response[\"responses\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d6fb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"responses\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db1a567",
   "metadata": {},
   "source": [
    "## Chatbot With Profile Schema Updating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57caead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_str: str = f\"openai:{ModelEnum.GEMINI_2p5_FLASH_REMOTE.value}\"\n",
    "llm: BaseChatModel = init_chat_model(\n",
    "    model=model_str,\n",
    "    api_key=settings.OPENROUTER_API_KEY.get_secret_value(),\n",
    "    base_url=settings.OPENROUTER_URL,\n",
    "    temperature=0.0,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "\n",
    "class UserProfileSchema(BaseModel):\n",
    "    \"\"\"A schema for storing user profile information.\"\"\"\n",
    "\n",
    "    user_name: str = Field(description=\"User's preferred name\")\n",
    "    user_location: str = Field(description=\"User's location\")\n",
    "    interests: list[str] = Field(default_factory=list, description=\"User's interests\")\n",
    "\n",
    "\n",
    "trustcall_extractor = create_extractor(\n",
    "    llm,\n",
    "    tools=[UserProfileSchema],\n",
    "    tool_choice=\"UserProfileSchema\",\n",
    ")\n",
    "\n",
    "MODEL_SYSTEM_MESSAGE: str = \"\"\"\n",
    "<system>\n",
    "\n",
    "<role>\n",
    "You are a helpful assistant with memory that responds to the user in a conversational way. \n",
    "If you have memory for this user, use it to personalize your responses.\n",
    "</role>\n",
    "\n",
    "<memory>\n",
    "{memory}\n",
    "</memory>\n",
    "\n",
    "</system>\n",
    "\"\"\"\n",
    "\n",
    "# Extraction instruction\n",
    "TRUSTCALL_INSTRUCTION: str = \"\"\"\n",
    "<instruction>\n",
    "Create or update the memory (JSON doc) to incorporate information from the following conversation:\n",
    "</instruction>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6915a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def call_llm(state: DataState, config: RunnableConfig, store: BaseStore) -> dict[str, Any]:\n",
    "    user_id: str = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    prefix: str = \"memory\"\n",
    "    key: str = \"user_memory\"\n",
    "    namespace = (prefix, user_id)\n",
    "    existing_memory = store.get(namespace, key)\n",
    "\n",
    "    if existing_memory and existing_memory.value:\n",
    "        memory_dict = existing_memory.value\n",
    "        formatted_memory = (\n",
    "            f\"Name: {memory_dict.get('user_name', 'Unknown')}\\n\"\n",
    "            f\"Location: {memory_dict.get('user_location', 'Unknown')}\\n\"\n",
    "            f\"Interests: {', '.join(memory_dict.get('interests', []))}\"\n",
    "        )\n",
    "    else:\n",
    "        formatted_memory = None\n",
    "\n",
    "    system_message: str = MODEL_SYSTEM_MESSAGE.format(memory=formatted_memory)\n",
    "    # Respond using memory + chat history\n",
    "    response = await llm.ainvoke([SystemMessage(content=system_message)] + state[\"messages\"])\n",
    "    # Validate\n",
    "    output = {\"messages\": [response]}\n",
    "    validate_data(data=output, state=state, response_model=DataStateValidator)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "async def write_memory(state: DataState, config: RunnableConfig, store: BaseStore) -> None:\n",
    "    user_id: str = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    prefix: str = \"memory\"\n",
    "    key: str = \"user_memory\"\n",
    "    namespace = (prefix, user_id)\n",
    "    existing_memory = store.get(namespace, key)\n",
    "\n",
    "    # Get the profile and convert to JSON doc\n",
    "    existing_profile = {\"UserProfileSchema\": existing_memory.value} if existing_memory else None\n",
    "    # Invoke the extractor\n",
    "    result = await trustcall_extractor.ainvoke(\n",
    "        {\n",
    "            \"messages\": [SystemMessage(content=TRUSTCALL_INSTRUCTION)] + state[\"messages\"],\n",
    "            \"existing\": existing_profile,\n",
    "        },\n",
    "    )\n",
    "    # Get the updated profile as a JSON object\n",
    "    updated_profile = result[\"responses\"][0].model_dump()\n",
    "    # Save the updated profile\n",
    "    store.put(namespace, key, updated_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a15ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(DataState)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node(\"call_llm\", call_llm)\n",
    "graph_builder.add_node(\"write_memory\", write_memory)\n",
    "\n",
    "# Add edges\n",
    "graph_builder.add_edge(START, \"call_llm\")\n",
    "graph_builder.add_edge(\"call_llm\", \"write_memory\")\n",
    "graph_builder.add_edge(\"write_memory\", END)\n",
    "\n",
    "# Long-term-memory store (across threads)\n",
    "across_thread_memory = InMemoryStore()\n",
    "# Short-term-memory store (within a thread) checkpointer\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# Compile the graph\n",
    "graph = graph_builder.compile(\n",
    "    checkpointer=within_thread_memory, store=across_thread_memory\n",
    ").with_config(run_name=\"Chatbot-with-user-profile\")\n",
    "\n",
    "\n",
    "# Visualize the graph\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11660760",
   "metadata": {},
   "outputs": [],
   "source": [
    "config: dict[str, Any] = {\"configurable\": {\"thread_id\": \"1\"}, \"user_id\": \"1\"}\n",
    "input_msg: list[Any] = [\n",
    "    HumanMessage(content=\"hello, my name is Chinedu. I live in Lagos, Nigeria.\")\n",
    "]\n",
    "\n",
    "\n",
    "async for event in graph.astream({\"messages\": input_msg}, config=config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee8b40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "across_thread_memory.get((\"memory\", \"1\"), \"user_memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fac538",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_msg = [\"I'm an AI Engineer whos's currently working on NLP related things\"]\n",
    "\n",
    "async for event in graph.astream({\"messages\": input_msg}, config=config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce068647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# namespace for the memory to save\n",
    "user_id: str = \"1\"\n",
    "prefix: str = \"memory\"\n",
    "key = \"user_memory\"\n",
    "namespace = (prefix, user_id)\n",
    "existing_memory = across_thread_memory.get(namespace, key)\n",
    "existing_memory.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ebd08e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## [TOP](#top)\n",
    "\n",
    "<br>\n",
    "\n",
    "<a id=\"chatbot-with-profile-collection\"></a>\n",
    "## Chatbot with Profile Collection\n",
    "\n",
    "- Sometimes we want to save memories to a `collection` rather than single profile.\n",
    "- We'll update our chatbot to save memories to a collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_str: str = f\"openai:{ModelEnum.GPT_4_o_MINI_REMOTE.value}\"\n",
    "llm: BaseChatModel = init_chat_model(\n",
    "    model=model_str,\n",
    "    api_key=settings.OPENROUTER_API_KEY.get_secret_value(),\n",
    "    base_url=settings.OPENROUTER_URL,\n",
    "    temperature=0.0,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "\n",
    "class Memory(BaseModel):\n",
    "    content: str = Field(description=\"The main content of the memory.\")\n",
    "\n",
    "\n",
    "class MemoryCollection(BaseModel):\n",
    "    memories: list[Memory] = Field(description=\"A list of memories.\", default_factory=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ae1e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_structure = llm.with_structured_output(MemoryCollection)\n",
    "memory_collection = await llm_with_structure.ainvoke(\n",
    "    [HumanMessage(content=\"My name is Neidu. I love reading books.\")]\n",
    ")\n",
    "\n",
    "console.log(memory_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a3e48c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e72ad4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc4ea24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ae9c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1821a8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb7672d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8b176a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
