{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc0ce639",
   "metadata": {},
   "source": [
    "# Content\n",
    "\n",
    "\n",
    "### 1. Map Reduce\n",
    "\n",
    "### 2. ...\n",
    "\n",
    "### 3. ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b2aa0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import (\n",
    "    Annotated,\n",
    "    Any,\n",
    "    Generator,\n",
    "    Iterable,\n",
    "    Literal,\n",
    "    Optional,\n",
    "    TypedDict,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "# Standard imports\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"white\": \"#FFFFFF\",  # Bright white\n",
    "        \"info\": \"#00FF00\",  # Bright green\n",
    "        \"warning\": \"#FFD700\",  # Bright gold\n",
    "        \"error\": \"#FF1493\",  # Deep pink\n",
    "        \"success\": \"#00FFFF\",  # Cyan\n",
    "        \"highlight\": \"#FF4500\",  # Orange-red\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "# import matplotlib.pyplot as pltife\n",
    "\n",
    "# NumPy settings\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "# Polars settings\n",
    "pl.Config.set_fmt_str_lengths(1_000)\n",
    "pl.Config.set_tbl_cols(n=1_000)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_up_from_current_directory(*, go_up: int = 1) -> None:\n",
    "    \"\"\"This is used to up a number of directories.\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "    go_up: int, default=1\n",
    "        This indicates the number of times to go back up from the current directory.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    CONST: str = \"../\"\n",
    "    NUM: str = CONST * go_up\n",
    "\n",
    "    # Goto the previous directory\n",
    "    prev_directory = os.path.join(os.path.dirname(__name__), NUM)\n",
    "    # Get the 'absolute path' of the previous directory\n",
    "    abs_path_prev_directory = os.path.abspath(prev_directory)\n",
    "\n",
    "    # Add the path to the System paths\n",
    "    sys.path.insert(0, abs_path_prev_directory)\n",
    "    print(abs_path_prev_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d4f1fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/neidu/Desktop/Projects/Personal/My_Projects/AI-Tutorials\n"
     ]
    }
   ],
   "source": [
    "go_up_from_current_directory(go_up=2)\n",
    "\n",
    "\n",
    "from schemas import ModelEnum  # noqa: E402\n",
    "from settings import refresh_settings  # noqa: E402\n",
    "from utilities.client_utils import check_rate_limit  # noqa: E402\n",
    "\n",
    "settings = refresh_settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f728896",
   "metadata": {},
   "source": [
    "# Map Reduce\n",
    "\n",
    "### Map\n",
    "\n",
    "- This involves breaking a task into smaller sub-tasks, processing each sub-task in parallel.\n",
    "\n",
    "### Reduce\n",
    "\n",
    "-  This involves combining/aggregating the results of the completed, parallelized sub-tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82f1daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage\n",
    "from langchain_core.tools import InjectedToolCallId, tool\n",
    "from langchain_litellm import ChatLiteLLM\n",
    "from langchain_tavily import TavilySearch\n",
    "from langfuse.callback import CallbackHandler\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.types import Command, interrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9c6b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "\n",
    "\n",
    "subject_prompt: str = \"\"\"\n",
    "    <instruction>\n",
    "    Generate a list of 3 sub-topics that are all \n",
    "    related to this overall topic: {topic!r} \n",
    "    </instruction>\n",
    "\"\"\"\n",
    "\n",
    "joke_prompt: str = \"<instruction>Generate a joke about {subject!r}.</instruction>\"\n",
    "best_joke_prompt: str = \"\"\"\n",
    "    <instruction>\n",
    "    Below are a bunch of jokes about {topic!r}. Select the best one! Return the ID \n",
    "    of the best one, starting 0 as the ID for the first joke. Jokes: \\n\\n{jokes}\n",
    "    </instruction>\n",
    "\"\"\"\n",
    "\n",
    "model_str: str = \"mistralai:ministral-8b-latest\"\n",
    "llm: BaseChatModel = init_chat_model(model=model_str, temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa09db5",
   "metadata": {},
   "source": [
    "## Parallelizing Joke Generation\n",
    "\n",
    "- Define a graph that will:\n",
    "  - take a user input topic\n",
    "  - produce a list of joke topics fro it\n",
    "  - send each joke topic to the LLM\n",
    "- The state has a `jokes` key that will accumulate jokes from parallelized joke generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f59d603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Subjects(BaseModel):\n",
    "    subjects: list[str] = Field(description=\"List of subjects related to the topic.\")\n",
    "\n",
    "\n",
    "class BestJoke(BaseModel):\n",
    "    id: int = Field(description=\"ID of the best joke selected from the list of jokes.\")\n",
    "\n",
    "\n",
    "class OverallState(TypedDict):\n",
    "    topic: str\n",
    "    subjects: list[str]\n",
    "    jokes: Annotated[list[str], add_messages]\n",
    "    best_selected_joke: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273e25bd",
   "metadata": {},
   "source": [
    "#### Test The Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85775822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    <instruction>\n",
      "    Generate a list of 3 sub-topics that are all \n",
      "    related to this overall topic: 'animals' \n",
      "    </instruction>\n",
      "\n",
      "subjects=['Mammals', 'Birds', 'Reptiles']\n"
     ]
    }
   ],
   "source": [
    "# OverallState(\n",
    "#     topic=\"Chinedu\",\n",
    "#     subjects=[\"love\", \"work\"],\n",
    "#     jokes=[],\n",
    "#     best_selected_joke=\"\",\n",
    "# )\n",
    "prompt: str = subject_prompt.format(topic=\"animals\")\n",
    "print(prompt)\n",
    "\n",
    "response = await llm.with_structured_output(Subjects).ainvoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08fbdb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_topics(state: OverallState) -> dict[str, Any]:\n",
    "    \"\"\"Generate a list of subjects based on a given topic.\n",
    "\n",
    "    Parameters:\n",
    "        state (OverallState): The current state containing the topic to generate subjects for.\n",
    "            Expected to have a 'topic' key with a string value.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, Any]\n",
    "\n",
    "    Notes:\n",
    "        Uses an LLM to generate structured output in the form of Subjects.\n",
    "    \"\"\"\n",
    "    prompt: str = subject_prompt.format(topic=state[\"topic\"])\n",
    "    response: Subjects = await llm.with_structured_output(Subjects).ainvoke(prompt)\n",
    "    return {\"subjects\": response.subjects}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3a5799",
   "metadata": {},
   "source": [
    "### Send Function\n",
    "\n",
    "- In LangGraph, Nodes and Edges usually share a predefined state. However, for dynamic cases like map-reduce, LangGraph uses `Send` objects in conditional edges.\n",
    "- It can be used to parallelize tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28261c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.constants import Send\n",
    "\n",
    "\n",
    "def continue_to_jokes(state: OverallState) -> dict[str, Any]:\n",
    "    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\"subjects\"]]\n",
    "\n",
    "\n",
    "class JokeState(TypedDict):\n",
    "    subject: str\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    joke: str\n",
    "\n",
    "\n",
    "async def generate_joke(state: JokeState) -> dict[str, Any]:\n",
    "    prompt = joke_prompt.format(subject=state[\"subject\"])\n",
    "    response = await llm.with_structured_output(Joke).ainvoke(prompt)\n",
    "    return {\"jokes\": [response.joke]}\n",
    "\n",
    "\n",
    "async def select_best_joke(state: OverallState) -> dict[str, Any]:\n",
    "    jokes: str = \"\\n\\n\".join(state[\"jokes\"])\n",
    "    prompt: str = best_joke_prompt.format(topic=state[\"topic\"], jokes=jokes)\n",
    "    response = await llm.with_structured_output(BestJoke).ainvoke(prompt)\n",
    "    return {\"best_selected_joke\": state[\"jokes\"][response.id]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2b3fb9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKUAAAGwCAIAAABHJTIRAAAAAXNSR0IArs4c6QAAIABJREFUeJztnXdcE2cfwJ/L3mEEZAthykbBvbGCs4pbAXFVUOuou63irFXcs7TiRCuuurXuUa1btgrK3huyk0vy/hFfShEZmuSgz30//JFbz/O7+/KMu3vuDlGr1QAHGghYB4CjV3DfcIH7hgvcN1zgvuEC9w0XJKwD+CRF2TJxDSqqRlFULZeosA6naah0ApGMMDkkBptkZkvFOpyGQVrb+Xfqk5qsFFFGisjOlYkQAJNDMmxHkYmVWMfVNBQ6sapELqpB1WokK1Vo58a0c2N26MzBOq5/0Yp8x9+rena9gu/BsnNj2rkzEQTrgL4AlQpkpYgyU0TvE4VdAo28ehtgHdEHWoXvwkzplYOFzp3Y3YfyCESso9EqSlT96FL5+0ThoMlm7drTsA6nFfhOelid9kIwaIo5g/3fUl0HsUB5OabAtQvXrRvG1TvGvtNeCgoypH1Hm2AYg964c7LE2pHh4MPCMAYsfT++Ui4WqPqPg0K2hlsnStgGpM6BRlgFgNn5d/orYXWZAirZAAD/8ablRbL3iUKsAsDGd2WxPCNJGBBqhknu2DIozDz9lbCqVIFJ7tj4fnCurLWdmOoTFz/Og3OlmGSNge+8dxKlUm3jwtB/1q0EW1eGXKYqyJDqP2sMfL95WtNzOFzN9sf0+trk9dMa/eerb99igTLnrdjEiqLPTOPi4iIjIz9jwwEDBuTn5+sgImBqTc1MEUpF+r5OrG/fmSkiOzemnjNNSUn5jK3y8vKqqqp0EM4H7NxYmSki3aXfIPo+/74TV+Lgw7Z2ousi8YyMjOjo6OfPnxOJRE9Pz5CQEC8vr2nTpiUkJGhWiI2NdXFxiYuLe/DgQXJyMpVK9fX1nT17toWFBQBg0aJFFArFzMzsyJEj06dP379/v2arPn36bNmyRevR5rwRZyQJ+44x1XrKjaDv8l2QKWEZ6OQmrFwuDw8PVyqV0dHRu3btIhAI3333nUwmi4mJcXd3HzJkyPPnz11cXF68eBEVFeXj4xMbG7t9+/bi4uIVK1ZoUiCTyampqe/evdu6deu4ceO2b98OADh//rwuZAMAmFxSQaa+u2z6vv8tqlEyOTq5Tp6dnV1RUREWFubg4AAA2LBhw6tXr1AUpVL/dSva29s7Li7O1taWSCQCAIKDgxctWiQUClksFpFILC0tjYuLq7eJjmBySOIaVA8Z1UWvvlGFWqlUU2g6qVRsbGwMDQ1XrVo1atQoLy8vV1dXX1/fj1cjEom5ublbtmxJSkqSSCSamRUVFSwWCwBgZ2enH9kAABqTIJeqVEqgz1uCeq3PVSpApetq56hU6m+//dazZ8+YmJjQ0NCRI0deu3bt49Vu3769aNEiT0/PmJiYZ8+eaSrtuonoKLwGoTKIapVe+0969U2hIgqpUiHT1R7a2trOnz//0qVLmzdv5vP5P/74Y1paWr11/vjjDx8fn/DwcCcnJwRBhELMLmXLJColqiaS9TquQ9/9NQaHJNJNo5WZmXnx4kUAAI1G69u378aNGwkEQmpqar3VqqurTUz+udpz584dXQTTHMQ1qI66Mo2gb9+WDnSxQCe+KysrV69evX379ry8vIyMjIMHD6pUKk9PTwCAtbV1amrq8+fPKyoqnJycnj59+vLlSxRFY2NjSSQSAKCoqOjjBG1tbQEAN2/eTE5O1kXAYoHKwl7fF5X17dvYjPIuQSdVaMeOHb///vurV6+OGDFi7NixCQkJ0dHRfD4fABAUFKRWq2fNmpWenj5nzpzOnTvPnz+/W7duZWVlkZGRrq6us2bNunnzZr0Erayshg0btm/fvl27duki4HcJAp6FXq8zYnC9paYC/WNP3uQVtvrMtHVycHXWmHlWOroa8Sn0Xb45RqR2NrSqEmzu/rYeKorkFny6nmVj87yBU0f2o0tlg6eaf2qF6dOnv3v37uP5KIoCADQt7sdcunRJcw6tdRITE+fOndvgIhRFPxWPpjOIfGJY9aNLZe7duNqLsblgM37t1I68XiN4Zp8Yn1taWqpQNFwByGSyT50ia66B64iCgoLP2OpTIRVmSB9dLhv1rdUXx9VisPFdlCVNfVLTf5xebxW0Hm6dKHHvzm1ng8EzR9iMZzKzpRmbU+7/gc2YHmy5d6bU1JqKiWwsx6d69TZA5eqnf1ZgFQAmPLlarlYDjx4YtNwaMH7e4MWtSpUS+A00xDAGvfHkWgWFRvDpi+WzZBg//93J3xBFVX8eaeDy1n+Mq4cKgRpgKxv78q0h/ZXw+rGiHkN53lgfDl3w6k7V31fKAkLM7D2xfJJIQ6vwrblV+uhi2ftEYQc/jp0708SqlT4v33xKcmWZyaLUp9VOPuwew3igdTze3Fp8a5AIlUkPqzOTRWKh0s6NSSQhDDaRY0xGFW3g/Q4kMqGmXCEWKJWoOjNFyGCT+O5Mjx4GNGYremlK6/Jdi7AKLcqWCasUYoESQYCoRsvjdm/duuXv76/dNBlsAoIgDDaRySWb21KZ3Nb4rpRW6lvX+Pn5PXv2DOsoMKAVVTU4egD3DRe4b7jAfcMF7hsucN9wgfuGC9w3XOC+4QL3DRe4b7jAfcMF7hsucN9wgfuGC9w3XOC+4QL3DRe4b7jAfcMF7hsucN9wgfuGC0h983g8rEPABkh9l5WVYR0CNkDqG1pw33CB+4YL3Ddc4L7hAvcNF7hvuMB9wwXuGy5w33CB+4YL3Ddc4L7hAvcNF7hvuIDrfXve3t6az8Rq9hpBEJVK9erVK6zj0h9wlW9LS0sEQRAEIRAIBAIBQRBLS0usg9IrcPn28vJSqf559a5arXZ3d8c0In0Dl+8JEybU/WaQhYVFcHAwphHpG7h8e3h4aL4oWjuJl+//OBMmTDA1NQUAmJmZTZo0Cetw9A10vj08PDp06AAA8PHxcXNzwzocfdPsd7KrQUmurLJELpNq+dXz+uerLtOEBUbd3Uck/lWFdSxfCpVONDSlmFpRm/m9jGadf5fkyh6cK1PIVOb2DIWsDXxaAh4oVGL+exGFSugdxDOxbPqjL037LiuQ34orGTDRgkKDrvJvK8glqpvHCwZMMDU2b+KD4k0oROXqUztyB0+1wmW3Zih0wuBpVnFbc1TKJkpvE+X778vlZDrJuRNm3z/EaT5vnlUrFcqug4waWaeJUluYKeUaN1FF4LQSuMaUokxJ4+s04VsmUbXO7yrhfAyTS5KKm+hNN9V+K1Qw3T9r26jVoMkP8+G9MLjAfcMF7hsucN9wgfuGC9w3XOC+4QL3DRe4b7jAfcMF7hsucN96Ii39TT9/35SURGzDgMX3iKABBYX5GKZgbMQLDZnO45l+SQxfDhT3OvML8qqrv2ho4penYGzMmxIW/iUpaAXt+05JSdyxc2Nefo6nZ8fQ4On7orfb8x3nz1sGAEhKij985Ne3b1ONjHldu/QMDZnBZDIBAGfO/H78xKE1q6I2bV6Tk5PF5zuMHR0cEDBUk+CVq+cvXjqblfWez3fs1/erUUETEAQBAKxYuYhCoZiamp2IO7J61abevfqf/SPu8eMHr18nU6hUH2/fadNmm5tZPHv+eMnSOQCAScFf9+jRZ92aLWVlpXv3bU1JTZRIJF269AgNnm5t3b6RPfo4BYlEEnNg7+PHD0pKi9u1M/fy7Dh71kI6nQ4AGDSkZ2jIjJTUxIcP7zGZTE/PjsuXrWGz2Gnpb2aGB+/eecDNzRMAkJn5ftuODUlJ8Rbmlr169Z82dRaZTK6uqT58OPrx47+qa6qcnVy/+mrwoMDh2rWj5fpcIpF8/+MCY57Jgf0np06J2LU7qrS0mEgiAQBycrKWLJujQBV7dh+KXPFzevqbhYvCNU9zkSkUgaBm1+6opYsjb9981qtn/6gta0tLSwAAN25cidq81sXZ9XjshSlh4adOH9uzd6smLzKZ/PZtakbmu/Vrt3p6+MTHv9i1O8rDw+eXX2J/Wr+9pLT4pw0rAAB+vl03rN8OADgWe37dmi0oin63KDwpOX7RwhWHDpzicLiz54Q1XlHXSwEAsGPnxtt3/pwV8d2Z09enhIXfuXv91992/j8qyukzx4NGjr914+nGDbtysjN379lcL8GCwvx586d7eXbcsnnfuHGhN29d3bN3CwBg8+a1r+KfL1jw/YH9J11c3LZsXZ/6Olm7grTs++GjezU11REz55uZmTs5ukybNru4uEiz6Oatq2QSec2qKBsbWz7fYfHilW/TXj/6+z4AgEAgKBSK2bMWurp6IAgycOAQpVKZlvYaAHDx8llPT595c5caGhr5duoyNSzi3PmTmqqVSCSWlZeuWRXVvXtvAwNDDw/vA/vjJk4Is7SwcnbqMHZMcHJyglAorBdhQuLL3Nzs5cvW+Pl2NTIynjNrIZvDPXv2RPP3sUZQc+v2tcmh33Tv3pvNYvfvNzBo5PjrNy6jKKp5xtie79jRx49AILi5eQ4fPvru3RuaRbWcPn2MSqOFTZ7Z0cdv+LBRU8LCCQSCJraBXw3x8+3arp3ZNzO+3b3roLGRlt/TruX6PDs7g8Ph2tjYaiZ9O3VhsVia38nJCS4ublyugWbS3MzCwsIqIeFlzx59NXNcXD487cFisQEAQqEARdHU1KSwyTNr0/fx8VMqlUlJ8T179gUAtLexo1I/DLomEon5+bl79m5JfZ0kkXwYxlVVVVEbgIakpHgymdzRx08ziSCIt1enpKQWPAKel5eDoqirq0ftHGdnV7FYXFiYr2kX7O2dahdZWljL5fL8/Ny6KbzPSHd2dtU8iQ4AGDJ4hOaHh4d33MmjNTXVXTr3cHf3cnF2bX5UzUTLvkVikaYZq8XQ0FjzQygUpL9728/ft+7Sysry2t+aVrkuUqlUqVTGHNgbc2Dvv7aqqtD8oFD/GWF//8HtyFVLQkOmh8+cb2/v+OTJw+U/zP84QqFQoFAo6oVhbNyCYlRRUQYAoFFptXPodAYAQCwRayapdRbR6HTNolq7AACRSGhq0u7jlJcuWXXhwulbt6+diDvCYrKCgsaHBE8nkbTpSMu+qRRqvbqrvLxU88PImOdBp9fro3I5Bo2kxmKxaDRaYMCw3r396863tLD+eOXLl//w9PSpTV8oql+TazA25tHp9PXrttWdSSK24DgwmSwAgET6z0hQsVgEAOAZm2gmRXWylkokAAAGnSGTy2pnMhjMBsPjsDnBk6ZOmjglOTnh/oPbR47u57C5o0ZNaH5sTaJl3+bmlhUV5dXVVZp6+1X8c7H4w3+9Pd/xzp3r3l6dastxVlaGlZVN4wny+Y4SqcTH+0NxlMvlxcWFpqYNFI6ammoLC6vayb/+uvPJBCUSMzMLc7MPD4LnF+QZ/b8Sag729k5EIjE5OcHJ0UUz5/XrZC7XwMjoQyIJCS9qV05/95ZGo1lYWGVmva+d6eLsduXqORRFNWX31u0/r127sHzZmrv3bg4ZPIJKpXp4eHt4eKelv36b/rr5gTUHLffXunXthSDIjp0bJRJJXn7u0aP7TUw+XGEYOzYEVaK7926RSqU5OVm/RO+YOn1c3aPQIDNnzL1//9aVq+dVKlVi4qs165YvXBwhk8k+XtPe3unFy6cJCS9RFD15KlZzKItLigAA1ja2AIB7926mvk7u0rl7587do6LWFBcXVVdXnf0jLmJW6NVrFxoPo24KHDbH3z/waOz+R4/uC4SC69cv/3EubszoSbX/x6VlJafPHFcqldnZmRcvnend259MJtdNbfiwUXK5fOu2n56/ePLgrzu/7d9lYtKOTKEcPLhv1ZqlKSmJlZUV169fTk9/4+7m1UIDTaDl8m1iYrpg/vKYA3tHjhrg6OgyJSx8x86NmtqSy+HG7I87ceLwzIjgnJwsFxe3pYsjHR2cG0/Q09Mnel/sseMHo3/dKZVK3Fw9163dSqU28GDcjOlzJBLx9z/Ol0gkY0ZPWrI4Mj8/d9HiWZErf+7bZ0BgwLADB/e5u3lt2xq9Yf32CxfPrFm3PDU1ydq6fWDAsKCR4xoPw9LCqm4K385evI+4be3671EUtbS0DgmePm5sSO3Kw4YGJSa+0pw3+vl2nTN7Ub3UrKxsft6wc/PmtVevXaBSqYEBw6ZPm8Nisdat3bprT9ScuVMBAHy+w5zZi7R+/t3E80RHf8ruP8GCY0RuZJ165BfksdkcDpujeUHK0OF9pk+bM3LEWG1E2wb4eqT/qKAJoSHT9Z91dZni7smC4OWNXTvScvmurKyImBWqOfPmcg0OHNhLJBD7/Lu3hYMhWvZtaGi0Yf32/TF7VqxcKJfJOnRw373rYG1HpjUTd/JobGxMg4vs+A47t+/Xe0Q6Qfv1eRtFIBQIhYIGF5FJZB7PRO8RtRgM6vO2C5vFZrPYWEehc2C5/42jAfcNF7hvuMB9wwXuGy5w33CB+4YL3Ddc4L7hognfBiZkVI6/oKltoFSoDEy/7H2aTA6pvECq1ahwdEVpvpTJaeICeRO+XXw5eWkirUaFoyvy0kQdfDmNr9OEbwt7WntXxsPzxVoNDEf7/HWuxN6TaWbXxCuxm/X+85d3qgqzpGxDiqkVDarvlbUBEKQ0VyKoUFg60Lz7NDbY98PqzfRXnCPLeSMSC5Q1FWgzVm/tvEt/5+DogHUUWoBtRGJyiLYuTBPrpl92D933BWvx8/N79uwZ1lFgAKS+X79+rflqDWxA6htaIL2+FhoainUI2ACp79evtfycTlsB0vocb79xoADS+hxvv+ECb7/hAm+/caAA0vocwi9/a4DUd1paGtYhYAOk9XlaWpqTk1MzVvyvAalvaIG0Psfbb7jA22+4wNtvHCiAtD7H22+4wNtvuMDbbxwogLQ+Hz9+PNYhYAOkvuVyOdYhYAOk9fn79+/t7e2xjgIDIPUNLZDW53j7DRfv3zfxXYX/KpDW53j7jQMFkNbnePsNF3j7DRd4+40DBZDW53j7DRd4+w0XWVlZtra2WEeBAZD6hhZI6/MxY8ZgHQI2QOo7KysL6xCwAa76fPDgwSQSCUEQhUJBJBIJBAKKopcvX8Y6Lv0B1/cFCwsLiURi3TkqlQq7cDAArvq8Z8+edSfVanW3bt2wCwcD4PIdEhLCZv/zkVAOhzNlyhRMI9I3cPnu3Llz3de2uLu7+/r6YhqRvoHLNwBgypQpHA4HAGBkZBQWFoZ1OPoGOt9+fn6aIu7h4dGpUyesw9E3X9Q/FwuUZYUymUipvXj0wbB+M0TF7IAeE9NfNfyB91YLjUHiWVLoLGIz1m2Yzzz/VqvBtcPFBRliCzs6TCfwWIOAggyJlQM9cLLZZybwGb7lMvUfe/I9extZOTI+L1ecLyH3jSjpYWXQHEsyBWnptp/jO25rbudAU55lsz6YgaMLSnKlL2+VjZln1dINW9xfe5cg5FnScdnYYmpNMzSlZiS2+NNwLfZdmiejMT+/v4CjLWhMYmm+rKVbtdi3VKxmG5FbuhWO1mEbkSXiFl/8b7FvVKZUo3iPHHtUSjUq071vnDYN7hsucN9wgfuGC9w3XOC+4QL3DRe4b7jAfcMF7hsucN9wgfv+TNLS3/Tz901JSWx8tchVSxYuitBXUE0Dr+8RQQMKCvM/e3NjI15oyHQez1SrQekcuJ4nqiW/IK+6uupLUjA25k0JC9deRHpCH+U7JSXxm5mTBg/ttez7eampSd/Om7Z9x8+aRWVlpWvWLh83YcjwEf3Xb1iRm5utmX/mzO+jxgSkpCROnjK6n7/vtBnj//zzUm2CSUnxixbPGja87+Qpo/f9sl0k+jDM4/SZ46PHBv718K7/V5137dkMAPj77wfrf/px7PjBg4f2WrgoIj7+BQDg2fPHwSEjAACTgr/+ceVCAACKovt+2T55yujBQ3stXT738eO/mtypevX5q/jn8xbMGDKs99cj/ectmPHo0f2PNykvLxszblDkqiWaMWRXrp6PmD150JCes7+dcvrMcf08uKlz3xKJ5PsfFxjzTA7sPzl1SsSu3VGlpcVEEklzlL9bFJ6UHL9o4YpDB05xONzZc8I0dSyZQhEIanbtjlq6OPL2zWe9evaP2rK2tLQEAJCTk7Vk2RwFqtiz+1Dkip/T098sXBSueeyPTKZIJOITcUeWL1sz8uuxYrF43U8/oCi6elXUwZhTlpbWP6xYUFVV6efbdcP67QCAY7Hn163ZAgDYtn3D2T9OjAqa8PvxS7179Y9cveT+g9vN38f8grzvFoZbW7Xf/9uJPbsOGnANI1cvKSsrrXccliybY2pq9sP36xAEuXHjStTmtS7OrsdjL0wJCz91+tievVt1cPjro3PfDx/dq6mpjpg538zM3MnRZdq02cXFRZpFCYkvc3Ozly9b4+fb1cjIeM6shWwO9+zZEwAAAoGgUChmz1ro6uqBIMjAgUOUSmVa2msAwM1bV8kk8ppVUTY2tny+w+LFK9+mvX70930AAJFIFIvF06bOGuAfaGVlw2Aw9v92Yv68ZR1c3Nq1M/tmxlyxWJycnFAvQqlUev3G5YkTwoYPG8XlcIcMHtG/X0BsbEzz9/HChdMmJqbz5y0zN7OwsrJZvGglkUi8fuOfx4yVSuWKlQvFItH6tVspFAoA4OLls56ePvPmLjU0NPLt1GVqWMS58yeFQqGWjvon0bnv7OwMDodrY/PhZSm+nbqwWCzN76SkeDKZ3NHHTzOJIIi3V6ekpFe127q4uGl+sFhsAIBQKAAAJCcnuLi4cbkGmkXmZhYWFlYJCS9rt3J2cq39LRaJdu7aNHpsYD9/32Ff9wUAVFVX1ovwzZsUFEX9fP95UNTH2zf93dvaZqLpfczJdHZyJZFI/4+WZWNtm5GRrtkpBEE2bV6TlvZ608bdBgaGmootNTXpXzn6+CmVyqwsnb9FSOf9NZFYRKfT684xNDTW/BAKBQqFop//v57YMzbm1f5GkAbGVwuFgvR3b+ttVVlZXvtbU4AAAEVFhfMWTPfz7bbih59cXT1UKlXg4B4NJCgSAAC+nTet3vyKijImk9mcfawoL6v9h9ZAo9PFErHmkeOExJcoinK5BnT6h+H6UqlUqVTGHNgbc2BvvV1rTnZfgs59UylUFEXrzikv/9CwGRvz6HT6+nXb/hUQsYmQjIx5HnR6vb4xl2Pw8Zq37/ypUCiWLllFo9E03aWGEzTiAQAWfveDpaV13fnNP9diMJlSmbTuHIlY3N7GTvObyWStWrlxy7b1P2+MjNq0B0EQFotFo9ECA4b17u1fdyu+nUMzc/xsdO7b3NyyoqK8urpKUwO/in8uFos1i/h8R4lEYmZmYW5moZmTX5Bn9P/S/yns+Y537lz39upUW/qzsjKsrGw+XrO6uorN5mhkAwDu3b/VYILW1u0pFAqRSPTx/lBnVFSUIwhSr1pqBGcn1xs3r6AoqqnSawQ12TmZgYHDawP29u60OnLTzIjgE3FHJoyf/GHfpZLaHOVyeXFxYW0jpTt03n5369oLQZAdOzdKJJK8/NyjR/ebmHwoN106d+/cuXtU1Jri4qLq6qqzf8RFzAq9eu1C4wmOHRuCKtHde7dIpdKcnKxfondMnT4us6GWz8Heqby87PKVcyiKPn7yMCnpFYfDLSkpAgBY29gCAO7du5n6OpnNYodNnnnocHRSUrxcLr977+bipbN37NzY/H0cOmSkQFCzddtPxcVFWVkZG35eSaczBv3ftwY+32HG9DkxB/ampb8BAMycMff+/VtXrp5XqVSJia/WrFu+cHGEQqFofqafh87Lt4mJ6YL5y2MO7B05aoCjo8uUsPAdOzfWVtob1m+/cPHMmnXLU1OTrK3bBwYMCxo5rvEEuRxuzP64EycOz4wIzsnJcnFxW7o40tHB+eM1BwwYlJ2TefDQL5u3rOvcufvSxZG/nzh8NDZGIKiZN3dpYMCwAwf3ubt5bdsaPWH8ZAcH5+MnDr18+ZTJZLm7eS1etLL5+2ht3T5y5c9Hj+4fP3GogYFhhw7uu3bEMBj1H64bOyb46dNHq1Ytidkf5+npE70v9tjxg9G/7pRKJW6unuvWbiWTdT6wv8XPj92ILTa1YfC92M1Y9wP5BXlsNofD5mj6L0OH95k+bc7IEWNbHm0r4m3a6/CIkD27Drq6emASwLv4mvJ86YCJLbugq/PyXVlZETErVHPmzeUaHDiwl0gg9vl3P6XNkZWV8fDhXQCAgaER1rG0DJ37NjQ02rB++/6YPStWLpTLZB06uO/eddDIqIlOWWsgJSVx2fK5DS6SyqQoio4bG2Jhbqn3uL4IfdTnbZfCooJPLao9p8CKVlqft2kwl6p14L3/DSe4b7jAfcMF7hsucN9wgfuGC9w3XOC+4QL3DRct9s00IIGGhhnh6BkEIExOi1+E12LfbENSSa6kpVvhaJ2SXAmH1+L75S32be/Bqipu8Wv9cLROZYmM785q6VYt9s3gEH36G96NK2zphjha5M6JQr+vjOisFuv7zPefZ6aIHl0s53uwjS1opJa/lRnn80DlqrIC2fv4mt5BJu07fM7LyD//e3M15WjK4+qacrSmXOej7LROYVGRudlnvjIeQ9hGJC6P7NHdgGX4ma8shuv7grX4+fk9e/YM6ygwAD//hgvcN1zgvuEC9w0XuG+4wH3DBe4bLnDfcIH7hgvcN1zgvuEC9w0XuG+4wH3DBe4bLnDfcIH7hgvcN1zgvuEC9w0XuG+4wH3DBe4bLiD17eTkhHUI2ACp77S0NKxDwAZIfUML7hsucN9wgfuGC9w3XOC+4QL3DRe4b7jAfcMF7hsucN9wgfuGC9w3XOC+4QL3DRdwvW8vMDBQ84nuoqKidu3aIQiCoui1a9ewjkt/wPV9wZKSEgLhQ5VWXFys+V4x1kHpFbjq8y5duqhUqtpJlUrVpUsXTCPSN3D5Dg0NNTAwqJ00MDCYNGkSphHpG7h8d+vWzcHBoXbS1dW1R48emEakb+DyDQCYPHkyl8sFAHA4HNgKN4yBlJl8AAASKElEQVS+e/To4ejoCABwdnbu1q0b1uHom6b752oVKC+UiwWoXuLRByMDZgiKqSMGhua8EWMdi9ZgcEjGZhSkqfLbxPn335fLkx5Vsw3JNMZnvlAfRz9IhKi4RunendN1sHEjqzXm++bxEjqb5NnLCOAfKGkTqEHCvQq5VNl/nMmnVvmk71snSliGFNeuBg0uxWm1JD+skooU/cY0rLzh+r4kRyaTqHHZbRH3HgZigbI0r+FvxDXsu7xIRiDilXhbhUBEyovkDS9qcK6wRmnYjqLjqHB0haEpVVjd8PlUw+djKoVaoYDrRsJ/CYVcRfjEiRl011sgB/cNF7hvuMB9wwXuGy5w33CB+4YL3Ddc4L7hAvcNF7hvuMDMd0bGu37+vklJ8TrN5czZE/5fddZpFh+Tlv6mn79vSkpi46tFrlqycFGEvoL6QFst3xkZ78ZPHKr/fEcEDSgozG98HWMjXmjIdB7PVF9BtYC2+jzR6zfJ+s80vyCvurqqydWMjXlTwsL1ElGL0ZrvrKyMQ4ejX8U/JxKJbq6e48aGuLt7AQBQFP1t/+7HT/4qLS328PAZ+fXYrl17frx5UlL84SO/vn2bamTM69qlZ2jIDCaTqVn08OG9XXuiSktLHOydRo4cFxgwbH/MnmPHDwIA+vn7zopYMGZ0Y8PICQRCQWF+TMyep88e8XimE8ZNHjhwSOOZqtXq02eOX79+OS8/p72NXadOXaZOiXj56tmSpXMAAJOCv+7Ro8+6NVs+lWNa+puZ4cG7dx5wc/NUq9Xnzp+6evV8VnaGgYGhg4PzzBlz27e3q7dJeXlZ+KwQ1w4eqyI3Ighy5er5i5fOZmW95/Md+/X9alTQBATRzvAT7dTncrn8u0XhSqVy25bojT/vIhAIP6z4TiaTAQC2bd9w9o8To4Im/H78Uu9e/SNXL7n/4Ha9zXNyspYsm6NAFXt2H4pc8XN6+puFi8I1D3o9fHgvcvWS6dPm/LxhZ48efTduWn37zvXp02aPHxfarp3ZnVvPG5etkffzxsjAwOFrVm92d/PasDEyNze78UzPnj1x4OC+0aMmHjt6fujQoMtXzp06fczPt+uG9dsBAMdizzciux5/Xr+0c9emgIBhp+KurvxxQ2Fh/uq1y+qtI5FIliybY2pq9sP36xAEuXHjStTmtS7OrsdjL0wJCz91+tievVtbYqMxtFO+c3OzKysrJkwI4/MdAAArV2xITHqFoqharb5+4/LECWHDh40CAAwZPCI5OSE2NqZ3r/51N7956yqZRF6zKorLNQAALF68cuKk4Y/+vt+zR98Dh/b17tV/gH8gAMDPt6tQKBCJhC2KTalUjhwxzs+3KwDAwcH52p8Xb9+5Pjl0RiOZJiS+9PLqFBAwFAAwdMhIb29fmVT6eUfm/PlT/fp+NSpoPACAyzWYPWvh4iWzX79O7tDBvTa8FSsXikWiLVH7KBQKAODi5bOenj7z5i4FAPh26jI1LCJqy9rQ0BkcNufzYqiLdsq3lZWNgYHhxk2rzpz5/c3bVCKR6OPty2Qy37xJQVHUz/efxzh8vH3T370ViUR1N09OTnBxcdMcdwCAuZmFhYVVQsJLpVKZmfm+9tAAAGZFLBg2NKil4XXp/OEhMTaLbWdrX1iY30imAAB3d6/nzx9vilrz18O7AqHAytLa3t7x845MZtZ7V1eP2kkXZzcAwLv3aQAABEEQBNm0eU1a2utNG3cbGBhqmr/U1KR/HTEfP6VS+e7d288LoB7aKd9UKnXHtt8uXzl39FhMdXWVpaV12OSZA/wDhSIBAODbedPqrV9RUVZ3UigUpL9728/ft+7MyspykVikVqvpdMYXhsdg/JMCjU7XRPWpTAEAo4Im0OmMR3/fX7FyEYlE6t8/4Jvp3xob81qar1AolMlkVCqtXiQSiVjT0CQkvkRRlMs1qN1HqVSqVCpjDuyNObC3blI1NdWftev10Vp/zcbGNiJ8/pSw8OfPH1+7fnH9Tz/atucbGfEAAAu/+8HS0rruyjyeaWGdsxojY54HnV6vT8vlGDDoDARBhELBF8YmlUpptA8HXSwWWVnaNJIpAIBIJA4bGjRsaFBWVsaLF08OHY4Wi0Rr12xuab6aTKVSSe0ckVgEANAcFgAAk8latXLjlm3rf94YGbVpD4IgLBaLRqMFBgzr3du/blLtbep38T4P7fjOzs58/SY5MGAYjUbr2bNv1649AwZ1f5uW2ru3P4VC0VTvmjUrKsoRBKHT6XU3t+c73rlz3durU20vNCsrw8rKhkQiOTo4JyS+HD8uVDP/t/27FQrFrIgFLQovPf2Nh4c3AEAkEmVnZ/brO7CRTNVq9fXrl52dXW1t+Zq/GkH1n9cvfcZhIZFIzk4dUlISazuVmoswfDuH2h339u60OnLTzIjgE3FHJoyfDADg8x0lUkntEZPL5cXFhUZGjT0l1Hy0035XVVVu3LR63y/b8wvysrIyjh0/qFKp3Fw92Sx22OSZhw5HJyXFy+Xyu/duLl46e8fOjfU2Hzs2BFWiu/dukUqlOTlZv0TvmDp9XGbWewBA0Mjxz579HXfy6Kv45+cvnP79xGF7vqOmx1BeXvbw4T1NZ/uTqNUkEunQ4ei8vBwURWMO7EFRtG/frxrJFEGQP69fily95O+/H9QIah4//uuvh3fdXD0BANY2tgCAe/dupr5u7tn/8OGj792/dfbsCYFQ8Cr++d59W/18u2p6tbXw+Q4zps+JObA3Lf0NAGDmjLn379+6cvW8SqVKTHy1Zt3yhYsj5PKGx5O3FO2Uby+vjt8t+P7Q4eiTp2I1HeltW6JtbfkAgAnjJzs4OB8/cejly6dMJsvdzWvxopX1NudyuDH7406cODwzIjgnJ8vFxW3p4khHB2cAQEDA0BpB9eEjv4pEImNj3sxv5mq6zV279PRw9/5x5cLJod+ETf7mU4HJ5DImkzVm9KS586dXVlbw+Q4rV2ywtLBqPNOlS1bt3rP5+x8XaC6eDB0ycszoYACApYVVYMCwAwf3ubt5bdsa3ZwjMyhweEVF+YmTR3bt2WzWztzXt+uMGd9+vNrYMcFPnz5atWpJzP44T0+f6H2xx44fjP51p1QqcXP1XLd2q6br/uU0/PzYk6sVCgXw6mOklTxg423a6/CIkD27DtbtmeuT+LsVVBroHNCAvrZ6/bzVkpWV8fDhXQCAgWFrLC1t9fp5LXEnj8bGxjS4yI7vsHP7fj1nSiKTq6oqx40NsTC31EXWX0ibr88FQsGnTtjIJDKP98knodtcps2nkfq8zZdvNovNZrFhyFQr4O03XOC+4QL3DRe4b7jAfcMF7hsucN9wgfuGC9w3XDR8fY3KIKj/O6+ShQ4yhUBlNDx+ueHyzeWRi3MkDS7Caf0UZYkNeOQGFzXs28aJIRMpdRwVjm5QA7lUZeVIb3Bhw76JZKTLYKMbRwt0HBqO9rl+NL/bEGMiqeH6vLH3YRdkSK8dLvToZWTUjkpj4e8/b9VIhcqqEvmru+VDp5mb2dI+tVoT77sXVSvj71UV50hFn3gfZxulpqaGw9HC4xqtBwabaGZL8+lnyGA3VjLh+r5gLX5+fs+ePcM6CgzAz7/hAvcNF7hvuMB9wwXuGy5w33CB+4YL3Ddc4L7hAvcNF7hvuMB9wwXuGy5w33CB+4YL3Ddc4L7hAvcNF7hvuMB9wwXuGy5w33CB+4YLSH17eGDzZlPMgdR3UlIS1iFgA6S+oQX3DRe4b7jAfcMF7hsucN9wgfuGC9w3XOC+4QL3DRe4b7jAfcMF7hsucN9wgfuGC7jetzdw4EAymaxWq4uLi01MTBAEUavV165dwzou/dHmvy/YIsrKyggEAgAAQZCysjIAgFIJ12ug4arPO3XqpFKpaidVKlWXLl0wjUjfwOU7ODjY0NCwdtLQ0HDcuHGYRqRv4PLdp08fOzu72klHR8d+/fphGpG+gcs3ACAkJMTAwAAAwOVyJ06ciHU4+gY637VF3MHBoXfv3liHo2/aRv9cIlSKBUolqp1Tx5GDw8oL0K8DQ0tyZVpJkEhCGGwivS18AqKVnn+rlSAzVfT2lUhQgZbkiskUAseULhW20m8s0JikmlKJQq4ytWFyDInOHZm2rkykVVadrc63SgnunS3LSBZSGBSWMYPNY5AoRNDwx1daGWqAypWCMrGgTIxKFXx3Zp8gXmuz3rp8P/mz6vn1snaORrz2XKxj+VLKsqqL0is6B/A6BxhgHcs/tCLfcVvzCHSGiW2bN12XsqxqpVQyboEl1oF8oFVUN6hCHb08g9XO8D8mGwDAs+UyTbm/fp+hrc7mF4J9+VbIVcej8q08zIjkVvHPpwtQubIgpXjiEivSJz4DpzewP8SxG3LMXUz/w7IBACQKsZ2TSeyGHKwDwbp8XzpQrCYz2byGP3b5H0NQKiapxIPC2mEYA5alKj1eWFWugkQ2AIBtwigvUWYkCTGMAUvfD86XmfKNMAxA/5jwje6fK8cwAMx8p/xdwzJiUhht44KutqAyyXQu4/VTAVYBYOY78WE1y4SJVe5Ncur8hi17gnWRMtuEkfhXtS5Sbg7Y+JYIlTUVCgaXiknu2MIwoFWWyKViVTPW1T7Y+M5IEnJaceHWNRxTRmYyNr02bJrP4hw5jf3Jb5J/OU9eXHjy/FxR8XtzM0cvd/9e3cYjCAIAWLF+QP/ek6Uy0a17B2lUprNjt68Hf8dhGwMAZDLxsdMr32U8N2/n0KPLaN3FBgCgsWnFObIOnXWaScNgU74FlSiJoqusX8RfPXVuvZVFh+Xf/RHQ/5v7j36/cHW7ZhGZTL19/zCZTF37/c3Fc+Mys+Nv3o3RLDp5bn1Zee7MsN2TJ2zML0x7m/5YR+FpLr/UVGBzbxcb3yIBSqLoanTA4+fn+O19goYtZrOMnBw6B/rPfPjklEhUBQAAALG27DCgzxQ6nc3lmDjad87OTQEAVNeUJiTf7NczpL21O4dtPDTgWzKJoqPwNL7FAmzGQWPjm8YgESk6aUqUSjQ7N8nJ8Z9Rxg58X5VKmZmdoJm0suxQu4hOY0tlQgBARWU+AKCd6YehjAiCWFm46CI8DSQqkcrAZjAMNu23XKZEZSiFrv19liukKpXy2s1frt38pe58gaji/z8buGMhElcDAGhUVu0cCkWHV/0UUlQhxaZ8Y+ObySYqZCgA2j8fo9NYFDLN12eop1v/uvN5xlaNxcPgAgAU6D/D2aQykdZjqwWVKRkcbI48NrkamVFLSnR1n8bczFGukDjwO2kmFai8srLQgNvYXQpDAwsAQHZukqW5EwAARRXvMp5zOCY6ilClVLez0GH/oBGwab/N2lNEFboqQEMGzk5Muf3kxQWVSpWR9So27ofoQ3MUisaGohpwTW1tvK7d/KWsPFehkMWe+hEh6PDIiCpE7dpjc60JG998D1ZVoVhXidv6zA8/nJkVv2pj4K+H50ploimTosjkJo7vhFGRVpYdtu4J/mFdPyad6+czVK3S1SWwqiKxnRs2l5swu/99cX8RoLHYxrDcDK2lplRCREVDpmJzFxyz+yU+fbiVuVVY5Y4hlblVPn04WOWO2e1IK0c6g4UIyyWsTxTxh49PXb31S4OLlEoFkUhucNHEUatdXXpqK8i7f8XevHewwUV0GkcirWlw0ZSJUfZ2HRtcJCiTsA0IFvaY1WpYjmcqzZX9ebzMytOswaUyuUQmbbhPJ5WJaVRGg4voDI4WL43JZGKZrOF+hgKVfyojBoNLIjX875iXWDQo1MTYHJvOOfbj1/6+WpGXoTLhGzZj3TZPyfuK9o6kLgFY7izGo0K7DTKi09CqAszGe+iNyjwBk6nEVjb25VvD1SMlUgXV0ILVjHXbJJV5QjpdHhisqws4zadVjPoeFGpKVIrLMiuxDkQnlGZWkhBJa5DdWsq3hkeXyrPfyrkWXIbBf2Sck7hKWlVYY+dC7Ta4tQzDbUW+AQAF76V3z5YpVQTj9oYMLmad2C9HXCUrz64ikVR9gngWfB2O5Gkprcu3hsxkUfz9mqJsCceEwTZhEkgImUoiUUito/FpCBVAZahCjioVamGpqKZUZM5nePfi2GJ00bQRWqNvDTKJKjNFVPBeVlYokwhQMp1YXaqd129oHS6PppChdBaJZ0G1tKfauTEptFb6v9l6fePoglb6b4ijI3DfcIH7hgvcN1zgvuEC9w0XuG+4+B+wZz9BXtBkeQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph_builder = StateGraph(OverallState)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node(\"generate_topics\", generate_topics)\n",
    "graph_builder.add_node(\"generate_joke\", generate_joke)\n",
    "graph_builder.add_node(\"select_best_joke\", select_best_joke)\n",
    "\n",
    "# Add edges\n",
    "graph_builder.add_edge(START, \"generate_topics\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"generate_topics\", continue_to_jokes, [\"generate_joke\"]\n",
    ")\n",
    "graph_builder.add_edge(\"generate_joke\", \"select_best_joke\")\n",
    "graph_builder.add_edge(\"select_best_joke\", END)\n",
    "\n",
    "# Compile the graph\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# Visualize the graph\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc2cfff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b88b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: sglang-pvc\n",
    "spec:\n",
    "  accessModes:\n",
    "    - ReadWriteOnce\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 30Gi\n",
    "  storageClassName: standard-rwo\n",
    "\n",
    "---\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: qwen-8b-slang\n",
    "spec:\n",
    "  replicas: 1\n",
    "  strategy:\n",
    "    type: Recreate\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: qwen-8b-slang\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: qwen-8b-slang\n",
    "        model: Qwen-3-8b\n",
    "        engine: sglang\n",
    "    spec:\n",
    "      restartPolicy: Always\n",
    "      # runtimeClassName: nvidia\n",
    "      nodeSelector:\n",
    "        cloud.google.com/gke-max-shared-clients-per-gpu: \"3\"\n",
    "      tolerations:\n",
    "        - key: \"on-llm-gpu\"\n",
    "          operator: \"Equal\"\n",
    "          value: \"true\"\n",
    "          effect: \"NoSchedule\"\n",
    "      initContainers:\n",
    "        - name: init-hf-cache-dir\n",
    "          image: busybox\n",
    "          command: [\"sh\", \"-c\", \"mkdir -p /root/.cache/huggingface && chmod 755 /root/.cache/huggingface\"]\n",
    "          volumeMounts:\n",
    "          - name: hf-cache\n",
    "            mountPath: /root/.cache/huggingface\n",
    "      containers:\n",
    "        - name: qwen-8b-slang\n",
    "          image: docker.io/lmsysorg/sglang:latest\n",
    "          imagePullPolicy: Always\n",
    "          ports:\n",
    "            - containerPort: 8000\n",
    "          command: [\"python3\", \"-m\", \"sglang.launch_server\"]\n",
    "          args:\n",
    "            [\n",
    "              \"--model-path\",\n",
    "              \"Qwen/Qwen3-8B-AWQ\",\n",
    "              \"--trust-remote-code\",\n",
    "              \"--host\",\n",
    "              \"0.0.0.0\",\n",
    "              \"--port\",\n",
    "              \"8000\",\n",
    "            ]\n",
    "\n",
    "          resources:\n",
    "            limits:\n",
    "              nvidia.com/gpu: 1\n",
    "          volumeMounts:\n",
    "            - name: shm\n",
    "              mountPath: /dev/shm\n",
    "            - name: hf-cache\n",
    "              mountPath: /root/.cache/huggingface\n",
    "              readOnly: true\n",
    "            - name: localtime\n",
    "              mountPath: /etc/localtime\n",
    "              readOnly: true\n",
    "            - name: sglang-storage\n",
    "              mountPath: /data\n",
    "          livenessProbe:\n",
    "            httpGet:\n",
    "              path: /health\n",
    "              port: 8000\n",
    "            initialDelaySeconds: 30\n",
    "            periodSeconds: 10\n",
    "      volumes:\n",
    "        - name: shm\n",
    "          emptyDir:\n",
    "            medium: Memory\n",
    "            sizeLimit: 25Gi\n",
    "        - name: hf-cache\n",
    "          emptyDir: {}\n",
    "        - name: localtime\n",
    "          hostPath:\n",
    "            path: /etc/localtime\n",
    "            type: File\n",
    "        - name: sglang-storage\n",
    "          persistentVolumeClaim:\n",
    "            claimName: sglang-pvc\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: qwen-8b-slang\n",
    "spec:\n",
    "  type: ClusterIP\n",
    "  selector:\n",
    "    app: qwen-8b-slang\n",
    "  ports:\n",
    "    - protocol: TCP\n",
    "      port: 8000\n",
    "      targetPort: 8000\n",
    "---\n",
    "apiVersion: networking.k8s.io/v1\n",
    "kind: Ingress\n",
    "metadata:\n",
    "  name: sglang-ingress\n",
    "  annotations:\n",
    "    cert-manager.io/cluster-issuer: \"cert-manager-http\"\n",
    "spec:\n",
    "  tls:\n",
    "    - hosts:\n",
    "        - indicina-qwen-model.indicina.net\n",
    "      secretName: sglang-tls\n",
    "  rules:\n",
    "    - host: indicina-qwen-model.indicina.net\n",
    "      http:\n",
    "        paths:\n",
    "          - path: /\n",
    "            pathType: ImplementationSpecific\n",
    "            backend:\n",
    "              service:\n",
    "                name: qwen-8b-slang\n",
    "                port:\n",
    "                  number: 8000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8b176a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a52ae07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d872a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input: str = \"Who is Chinedu? What does he do? \"\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "events = graph.astream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "    config=config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "\n",
    "try:\n",
    "    async for event in events:\n",
    "        if \"messages\" in event:\n",
    "            event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "except Exception as e:\n",
    "    console.print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input: str = \"What is 7 plus 3? Divide the result by 4\"\n",
    "\n",
    "events = graph.astream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "    config=config,\n",
    "    stream_mode=\"values\",\n",
    ")\n",
    "\n",
    "try:\n",
    "    async for event in events:\n",
    "        if \"messages\" in event:\n",
    "            event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "except Exception as e:\n",
    "    console.print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077f7c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad313cbe",
   "metadata": {},
   "source": [
    "## State Reducers\n",
    "\n",
    "- Multiple messages can be concatenated using: \n",
    "  - Using operator.add\n",
    "  - Using langgraph's add_message function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9eb8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "\n",
    "class DefaultState(TypedDict):\n",
    "    messages: list\n",
    "\n",
    "\n",
    "class StateWithOperatorAdd(TypedDict):\n",
    "    messages: Annotated[list, add]\n",
    "\n",
    "\n",
    "class StateWithAddMessages(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "def node_1(state: DefaultState) -> dict[str, Any]:\n",
    "    print(\"node 1\")\n",
    "    return {\"messages\": state[\"messages\"]}\n",
    "\n",
    "\n",
    "def node_2(state: DefaultState) -> dict[str, Any]:\n",
    "    print(\"node 2\")\n",
    "    return {\"messages\": [\"processed\"]}\n",
    "\n",
    "\n",
    "def node_3(state: DefaultState) -> dict[str, Any]:\n",
    "    print(\"node 3\")\n",
    "    return {\"messages\": [\"processed\"]}\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(DefaultState)\n",
    "graph_builder.add_node(\"node_1\", node_1)\n",
    "graph_builder.add_node(\"node_2\", node_2)\n",
    "graph_builder.add_node(\"node_3\", node_3)\n",
    "\n",
    "graph_builder.add_edge(START, \"node_1\")\n",
    "graph_builder.add_edge(\"node_1\", \"node_2\")\n",
    "graph_builder.add_edge(\"node_1\", \"node_3\")\n",
    "graph_builder.add_edge(\"node_2\", END)\n",
    "graph_builder.add_edge(\"node_3\", END)\n",
    "\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46db1c4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Comment\n",
    "\n",
    "- When we use the `DefaultState` as the state type, there's no way to concatenate the messages, we get an error because multiple steps/nodes can't write to the same field at the same time.\n",
    "- To solve this, we can use `StateWithOperatorAdd` or `StateWithAddMessages`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339d920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.errors import InvalidUpdateError\n",
    "\n",
    "try:\n",
    "    result = graph.invoke({\"messages\": [\"Hello\"]})\n",
    "    pprint(result)\n",
    "except InvalidUpdateError as e:\n",
    "    console.log(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae06ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how add_messages works.\n",
    "initial_message: list[Any] = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi there!\"},\n",
    "]\n",
    "\n",
    "# Add new message\n",
    "new_message: dict[str, Any] = {\"role\": \"user\", \"content\": \"How are you?\"}\n",
    "\n",
    "# Use add_messages to create a new list with the new message added\n",
    "add_messages_result: list[Any] = add_messages(initial_message, new_message)\n",
    "console.log(f\"Result of add_messages: {add_messages_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ed333f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d5d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_1(state: StateWithOperatorAdd) -> dict[str, Any]:\n",
    "    print(\"node 1\")\n",
    "    return {\"messages\": state[\"messages\"]}\n",
    "\n",
    "\n",
    "def node_2(state: StateWithOperatorAdd) -> dict[str, Any]:\n",
    "    print(\"node 2\")\n",
    "    return {\"messages\": [\"processed\"]}\n",
    "\n",
    "\n",
    "def node_3(state: StateWithOperatorAdd) -> dict[str, Any]:\n",
    "    print(\"node 3\")\n",
    "    return {\"messages\": [\"processed\"]}\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(StateWithOperatorAdd)\n",
    "graph_builder.add_node(\"node_1\", node_1)\n",
    "graph_builder.add_node(\"node_2\", node_2)\n",
    "graph_builder.add_node(\"node_3\", node_3)\n",
    "\n",
    "graph_builder.add_edge(START, \"node_1\")\n",
    "graph_builder.add_edge(\"node_1\", \"node_2\")\n",
    "graph_builder.add_edge(\"node_1\", \"node_3\")\n",
    "graph_builder.add_edge(\"node_2\", END)\n",
    "graph_builder.add_edge(\"node_3\", END)\n",
    "\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b842d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    result = graph.invoke({\"messages\": [\"Hello\"]})\n",
    "    pprint(result)\n",
    "except InvalidUpdateError as e:\n",
    "    console.log(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b786645",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    result = graph.invoke({\"messages\": None})\n",
    "    pprint(result)\n",
    "except (InvalidUpdateError, TypeError) as e:\n",
    "    console.log(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eb886e",
   "metadata": {},
   "source": [
    "### Custom Reducers\n",
    "\n",
    "- We can create custom reducers for handlng edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8c09a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_list(left: list[list[Any]] | None, right: list[Any] | None) -> list[Any]:\n",
    "    \"\"\"Reduce two lists into one.\"\"\"\n",
    "    if left is None:\n",
    "        left = []\n",
    "    if right is None:\n",
    "        right = []\n",
    "    return left + right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateWithCustomReducer(TypedDict):\n",
    "    messages: Annotated[list, reduce_list]\n",
    "\n",
    "\n",
    "def node_1(state: StateWithCustomReducer) -> dict[str, Any]:\n",
    "    print(\"node 1\")\n",
    "    return {\"messages\": state[\"messages\"]}\n",
    "\n",
    "\n",
    "def node_2(state: StateWithCustomReducer) -> dict[str, Any]:\n",
    "    print(\"node 2\")\n",
    "    return {\"messages\": [\"processed\"]}\n",
    "\n",
    "\n",
    "def node_3(state: StateWithCustomReducer) -> dict[str, Any]:\n",
    "    print(\"node 3\")\n",
    "    return {\"messages\": [\"processed\"]}\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(StateWithCustomReducer)\n",
    "graph_builder.add_node(\"node_1\", node_1)\n",
    "graph_builder.add_node(\"node_2\", node_2)\n",
    "graph_builder.add_node(\"node_3\", node_3)\n",
    "\n",
    "graph_builder.add_edge(START, \"node_1\")\n",
    "graph_builder.add_edge(\"node_1\", \"node_2\")\n",
    "graph_builder.add_edge(\"node_1\", \"node_3\")\n",
    "graph_builder.add_edge(\"node_2\", END)\n",
    "graph_builder.add_edge(\"node_3\", END)\n",
    "\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8bf4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    result = graph.invoke({\"messages\": None})\n",
    "    pprint(result)\n",
    "except (InvalidUpdateError, TypeError) as e:\n",
    "    console.log(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e425a62",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## Multiple Schemas\n",
    "\n",
    "- We can use specific schemas for the input and output of the graph.\n",
    "- This can be based on the relevant keys in the input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61629db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputState(TypedDict):\n",
    "    question: str\n",
    "\n",
    "\n",
    "class OutputState(TypedDict):\n",
    "    answer: str\n",
    "\n",
    "\n",
    "class OverallState(TypedDict):\n",
    "    question: str\n",
    "    answer: str\n",
    "    notes: str\n",
    "\n",
    "\n",
    "def thinking_node(state: InputState) -> dict[str, str]:\n",
    "    return {\"answer\": \"bye\", \"notes\": \"... his name is Neidu\"}\n",
    "\n",
    "\n",
    "def answer_mode(state: OverallState) -> OutputState:\n",
    "    return {\"answer\": \"bye Neidu\"}\n",
    "\n",
    "\n",
    "# Build the state graph\n",
    "graph_builder = StateGraph(OverallState, input=InputState, output=OutputState)\n",
    "graph_builder.add_node(\"thinking_node\", thinking_node)\n",
    "graph_builder.add_node(\"answer_mode\", answer_mode)\n",
    "\n",
    "# Connect nodes\n",
    "graph_builder.add_edge(START, \"thinking_node\")\n",
    "graph_builder.add_edge(\"thinking_node\", \"answer_mode\")\n",
    "graph_builder.add_edge(\"answer_mode\", END)\n",
    "\n",
    "# Compile the graph and visualize it\n",
    "graph = graph_builder.compile()\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6ca5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke({\"question\": \"What is your name?\"})\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f276e916",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "\n",
    "## Filtering And Trimming Messages\n",
    "\n",
    "- To avoid sending too many messages to the LLM especially as the context grows, we can filter and trim the messages.\n",
    "\n",
    "### 1. Using A Reducer To Remove Messages\n",
    "- Using `add_messages`  and `RemoveMessage`\n",
    "- This modifies the graph's state.\n",
    "\n",
    "#### Observability\n",
    "\n",
    "- You can view the traces using `LangFuse` or any other observability tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72f006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import RemoveMessage\n",
    "\n",
    "\n",
    "class MessageState(TypedDict):\n",
    "    messages: Annotated[list[Any], add_messages]\n",
    "\n",
    "\n",
    "def filter_messages(state: MessageState) -> dict[str, Any]:\n",
    "    # Delete all but the N most recent messages\n",
    "    N: int = 3\n",
    "    deleted_messages: list[Any] = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-N]]\n",
    "    return {\"messages\": deleted_messages}\n",
    "\n",
    "\n",
    "async def chatbot(state: MessageState) -> dict[str, Any]:\n",
    "    response = await llm.ainvoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Build the state graph\n",
    "graph_builder = StateGraph(MessageState)\n",
    "\n",
    "# Add Nodes\n",
    "graph_builder.add_node(\"filter_messages\", filter_messages)\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# Connect nodes\n",
    "graph_builder.add_edge(START, \"filter_messages\")\n",
    "graph_builder.add_edge(\"filter_messages\", \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "# Compile the graph and visualize it\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ebd601",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "messages: list[Any] = [\n",
    "    {\"role\": \"user\", \"content\": \"Hi!\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello\"},\n",
    "    {\"role\": \"user\", \"content\": \"I'm Chinedu.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello, how can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"can you remember my name?\"},\n",
    "]\n",
    "\n",
    "async for message in graph.astream({\"messages\": messages}, config=config, stream_mode=\"values\"):\n",
    "    if \"messages\" in message:\n",
    "        message[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ebe1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6eefdd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2. Filtering Messages Without Modifying The Graph's State\n",
    "\n",
    "- This can be done by selecting a slice of the message history.\n",
    "- i.e. `state[\"messages\"][-N:]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cc83c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chatbot_with_slice(state: MessageState) -> dict[str, Any]:\n",
    "    N: int = 3\n",
    "    response = await llm.ainvoke(state[\"messages\"][-N:])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Build the state graph\n",
    "graph_builder = StateGraph(MessageState)\n",
    "\n",
    "# Add Nodes\n",
    "graph_builder.add_node(\"chatbot_with_slice\", chatbot_with_slice)\n",
    "\n",
    "# Connect nodes\n",
    "graph_builder.add_edge(START, \"chatbot_with_slice\")\n",
    "graph_builder.add_edge(\"chatbot_with_slice\", END)\n",
    "\n",
    "# Compile the graph and visualize it\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9bd87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "messages: list[Any] = [\n",
    "    {\"role\": \"user\", \"content\": \"Hi!\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello\"},\n",
    "    {\"role\": \"user\", \"content\": \"I'm Chinedu.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello, how can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"can you remember my name?\"},\n",
    "]\n",
    "\n",
    "async for message in graph.astream({\"messages\": messages}, config=config, stream_mode=\"values\"):\n",
    "    if \"messages\" in message:\n",
    "        message[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b418853e",
   "metadata": {},
   "source": [
    "### Comment\n",
    "\n",
    "- Only the last N messages are used to generate the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29d4c166",
   "metadata": {},
   "source": [
    "### Trim Messages\n",
    "\n",
    "- This approach trims the messages based upon the set number of tokens.\n",
    "- It uses the langchain's `trim_messages` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8a13b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import trim_messages\n",
    "\n",
    "\n",
    "async def chatbot_with_trim(state: MessageState) -> dict[str, Any]:\n",
    "    messages = trim_messages(\n",
    "        messages=state[\"messages\"],\n",
    "        max_tokens=100,\n",
    "        token_counter=llm,\n",
    "        strategy=\"last\",  # keep the last N messages\n",
    "        allow_partial=False,  # allow partial messages\n",
    "    )\n",
    "    response = await llm.ainvoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Build the state graph\n",
    "graph_builder = StateGraph(MessageState)\n",
    "\n",
    "# Add Nodes\n",
    "graph_builder.add_node(\"chatbot_with_trim\", chatbot_with_trim)\n",
    "\n",
    "# Connect nodes\n",
    "graph_builder.add_edge(START, \"chatbot_with_trim\")\n",
    "graph_builder.add_edge(\"chatbot_with_trim\", END)\n",
    "\n",
    "# Compile the graph and visualize it\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d01e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "messages: list[Any] = [\n",
    "    {\"role\": \"user\", \"content\": \"Hi!\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello\"},\n",
    "    {\"role\": \"user\", \"content\": \"I'm Chinedu.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello, how can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"can you remember my name?\"},\n",
    "]\n",
    "\n",
    "async for message in graph.astream({\"messages\": messages}, config=config, stream_mode=\"values\"):\n",
    "    if \"messages\" in message:\n",
    "        message[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f2e166",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Chat Summarization\n",
    "\n",
    "- Instead of trimming or filtering messages, we can summarize the chat history (using an LLM) to keep the context while reducing the number of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ce35ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessageState(TypedDict):\n",
    "    messages: Annotated[list[Any], add_messages]\n",
    "    summary: str\n",
    "\n",
    "\n",
    "# Nodes\n",
    "async def summarize_history(state: MessageState) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Summarize conversation history and manage message retention.\n",
    "\n",
    "    Parameters:\n",
    "        state (MessageState): Current state containing messages and summary.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, Any]\n",
    "            Dictionary containing new summary and messages to be deleted.\n",
    "    \"\"\"\n",
    "    summary: str = state.get(\"summary\", \"\")\n",
    "\n",
    "    if summary:\n",
    "        summary_msg: str = f\"This is the summary of the conversation so far: {summary}\\n\\n\"\n",
    "        \"Extend this summary by taking into account the new messages above:\"\n",
    "    else:\n",
    "        summary_msg: str = \"Create a summary of the conversation so far:\\n\\n\"\n",
    "\n",
    "    messages: list[Any] = state[\"messages\"] + [HumanMessage(content=summary_msg)]\n",
    "    response: Any = await llm.ainvoke(messages)\n",
    "\n",
    "    # Delete all but the last N messages\n",
    "    N: int = 2\n",
    "    deleted_messages: list[Any] = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-N]]\n",
    "\n",
    "    return {\"summary\": response.content, \"messages\": deleted_messages}\n",
    "\n",
    "\n",
    "async def chatbot_with_summary(state: MessageState) -> dict[str, Any]:\n",
    "    summary: str = state.get(\"summary\", \"\")\n",
    "    if summary:\n",
    "        summary_msg: str = f\"This is the summary of the conversation so far: {summary}\\n\\n\"\n",
    "        messages: list[Any] = [SystemMessage(content=summary_msg)] + state[\"messages\"]\n",
    "    else:\n",
    "        messages = state[\"messages\"]\n",
    "\n",
    "    response = await llm.ainvoke(messages)\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def should_summarize(state: MessageState) -> Literal[END, \"summarize\"]:\n",
    "    \"\"\"\n",
    "    Determine if conversation history should be summarized.\n",
    "\n",
    "    Parameters:\n",
    "        state (MessageState): Current state containing messages and summary.\n",
    "\n",
    "    Returns:\n",
    "        Literal[\"summarize\", END]\n",
    "            \"summarize\" if messages exceed threshold, END otherwise.\n",
    "    \"\"\"\n",
    "    messages: list[Any] = state[\"messages\"]\n",
    "    # If there are more than 5 messages, we summarize\n",
    "    if len(messages) > 5:\n",
    "        return \"summarize\"\n",
    "\n",
    "    # Otherwise, we end the conversation\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07c1d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.callback import CallbackHandler\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(MessageState)\n",
    "\n",
    "# Add Nodes\n",
    "graph_builder.add_node(\"summarize_history\", summarize_history)\n",
    "graph_builder.add_node(\"chatbot_with_summary\", chatbot_with_summary)\n",
    "\n",
    "# Connect nodes\n",
    "graph_builder.add_edge(START, \"chatbot_with_summary\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot_with_summary\",\n",
    "    should_summarize,\n",
    "    # From Output --> Node\n",
    "    {\n",
    "        \"summarize\": \"summarize_history\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "graph_builder.add_edge(\"summarize_history\", END)\n",
    "\n",
    "# Compile the graph and visualize it\n",
    "memory = MemorySaver()\n",
    "langfuse_handler = CallbackHandler()\n",
    "graph = graph_builder.compile(checkpointer=memory).with_config({\"callbacks\": [langfuse_handler]})\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695e97ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "messages: list[Any] = [\n",
    "    {\"role\": \"user\", \"content\": \"Hi!\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello\"},\n",
    "    {\"role\": \"user\", \"content\": \"How are you doing?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm doing well, thank you!\"},\n",
    "    {\"role\": \"user\", \"content\": \"I'm Chinedu.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello, how can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"can you remember my name?\"},\n",
    "]\n",
    "\n",
    "async for message in graph.astream({\"messages\": messages}, config=config, stream_mode=\"values\"):\n",
    "    if \"messages\" in message:\n",
    "        message[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e951964",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_copy: list[Any] = messages.copy()[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4802b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "response: dict[str, Any] = {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": message[\"messages\"][-1].content,\n",
    "}\n",
    "messages_copy.append(response)\n",
    "\n",
    "message: dict[str, Any] = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Okay, what is the meaning of flash attention in AI?\",\n",
    "}\n",
    "\n",
    "messages_copy.append(message)\n",
    "\n",
    "console.log(messages_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eb6395",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for message in graph.astream(\n",
    "    {\"messages\": messages_copy}, config=config, stream_mode=\"values\"\n",
    "):\n",
    "    if \"messages\" in message:\n",
    "        message[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d97a002",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_copy: list[Any] = messages_copy.copy()[-2:]\n",
    "response: dict[str, Any] = {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": message[\"messages\"][-1].content,\n",
    "}\n",
    "messages_copy.append(response)\n",
    "\n",
    "message: dict[str, Any] = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Do you know about vLLM? Does it use flash attention?\",\n",
    "}\n",
    "\n",
    "messages_copy.append(message)\n",
    "\n",
    "console.log(messages_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698afea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for message in graph.astream(\n",
    "    {\"messages\": messages_copy}, config=config, stream_mode=\"values\"\n",
    "):\n",
    "    if \"messages\" in message:\n",
    "        message[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807bcefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_copy: list[Any] = messages_copy.copy()[-2:]\n",
    "\n",
    "response: dict[str, Any] = {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": message[\"messages\"][-1].content,\n",
    "}\n",
    "messages_copy.append(response)\n",
    "\n",
    "message: dict[str, Any] = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Can you remember my name? I think you forgot it.\",\n",
    "}\n",
    "\n",
    "messages_copy.append(message)\n",
    "\n",
    "console.log(messages_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b9ea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for message in graph.astream(\n",
    "    {\"messages\": messages_copy}, config=config, stream_mode=\"values\"\n",
    "):\n",
    "    if \"messages\" in message:\n",
    "        message[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea5de67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
