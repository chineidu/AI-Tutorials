{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc0ce639",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Content\n",
    "\n",
    "\n",
    "### 1. [Map Reduce](#map-reduce)\n",
    "\n",
    "### 2. [Short Term vs Long Term Memory](#short-term-vs-long-term-memory)\n",
    "\n",
    "### 3. [Chatbot with Profile Schema](#chatbot-with-profile-schema)\n",
    "\n",
    "### 4. [Chatbot with Profile Collection](#chatbot-with-profile-collection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b2aa0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in library\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import (\n",
    "    Annotated,\n",
    "    Any,\n",
    "    Generator,\n",
    "    Iterable,\n",
    "    Literal,\n",
    "    Optional,\n",
    "    TypedDict,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "# Standard imports\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme(\n",
    "    {\n",
    "        \"white\": \"#FFFFFF\",  # Bright white\n",
    "        \"info\": \"#00FF00\",  # Bright green\n",
    "        \"warning\": \"#FFD700\",  # Bright gold\n",
    "        \"error\": \"#FF1493\",  # Deep pink\n",
    "        \"success\": \"#00FFFF\",  # Cyan\n",
    "        \"highlight\": \"#FF4500\",  # Orange-red\n",
    "    }\n",
    ")\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "# Visualization\n",
    "# import matplotlib.pyplot as pltife\n",
    "\n",
    "# NumPy settings\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "# Polars settings\n",
    "pl.Config.set_fmt_str_lengths(1_000)\n",
    "pl.Config.set_tbl_cols(n=1_000)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_up_from_current_directory(*, go_up: int = 1) -> None:\n",
    "    \"\"\"This is used to up a number of directories.\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "    go_up: int, default=1\n",
    "        This indicates the number of times to go back up from the current directory.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    CONST: str = \"../\"\n",
    "    NUM: str = CONST * go_up\n",
    "\n",
    "    # Goto the previous directory\n",
    "    prev_directory = os.path.join(os.path.dirname(__name__), NUM)\n",
    "    # Get the 'absolute path' of the previous directory\n",
    "    abs_path_prev_directory = os.path.abspath(prev_directory)\n",
    "\n",
    "    # Add the path to the System paths\n",
    "    sys.path.insert(0, abs_path_prev_directory)\n",
    "    print(abs_path_prev_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d4f1fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/neidu/Desktop/Projects/Personal/My_Projects/AI-Tutorials\n"
     ]
    }
   ],
   "source": [
    "go_up_from_current_directory(go_up=2)\n",
    "\n",
    "\n",
    "from schemas import ModelEnum  # noqa: E402\n",
    "from settings import refresh_settings  # noqa: E402\n",
    "from utilities.client_utils import check_rate_limit  # noqa: E402\n",
    "\n",
    "settings = refresh_settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16d37a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Type\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "def validate_data(\n",
    "    data: dict[str, Any], state: dict[str, Any], response_model: Type[BaseModel]\n",
    ") -> None:\n",
    "    \"\"\"Validate that input data matches the expected response model structure.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dict[str, Any]\n",
    "        The input data dictionary to validate\n",
    "    state : dict[str, Any]\n",
    "        The state dictionary to update with input data\n",
    "    response_model : Type[BaseModel]\n",
    "        The Pydantic model class to validate against\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Function performs validation through assertions\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    AssertionError\n",
    "        If data is not a dictionary, state is not a dictionary, or fails model validation\n",
    "    \"\"\"\n",
    "    assert isinstance(data, dict), \"Data must be a dictionary\"\n",
    "    assert isinstance(state, dict), \"Data must be a dictionary\"\n",
    "    state.update(data)\n",
    "    assert response_model(**state), \"Data is not valid\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f728896",
   "metadata": {},
   "source": [
    "<a id=\"map-reduce\"></a>\n",
    "# 1. Map Reduce\n",
    "\n",
    "### Map\n",
    "\n",
    "- This involves breaking a task into smaller sub-tasks, processing each sub-task in parallel.\n",
    "\n",
    "### Reduce\n",
    "\n",
    "-  This involves combining/aggregating the results of the completed, parallelized sub-tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82f1daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    AnyMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "from langchain_core.tools import InjectedToolCallId, tool\n",
    "from langchain_litellm import ChatLiteLLM\n",
    "from langchain_tavily import TavilySearch\n",
    "from langfuse.langchain import CallbackHandler\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.types import Command, interrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9c6b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "\n",
    "subject_prompt: str = \"\"\"\n",
    "<system>\n",
    "\n",
    "Generate a list of 3 sub-topics that are all \n",
    "related to this overall topic:\n",
    "\n",
    "<topic>\n",
    "{topic!r} \n",
    "</topic>\n",
    "\n",
    "</system>\n",
    "\"\"\"\n",
    "\n",
    "joke_prompt: str = \"\"\"\n",
    "<system>\n",
    "Generate a joke about:\n",
    "\n",
    "<subject>\n",
    "{subject!r}\n",
    "</subject>\n",
    "\n",
    "</system>\n",
    "\"\"\"\n",
    "\n",
    "best_joke_prompt: str = \"\"\"\n",
    "<system>\n",
    "Below are a bunch of jokes about:\n",
    "\n",
    "<topic>\n",
    "{topic!r}.\n",
    "</topic>\n",
    "\n",
    "Select the best one! Return the ID of the best one, starting 0 \n",
    "as the ID for the first joke. \n",
    "<jokes>\n",
    "{jokes}\n",
    "</jokes>\n",
    "\n",
    "<output>\n",
    "Return the ID of the best joke as an integer.\n",
    "</output>\n",
    "\n",
    "</system>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d08894",
   "metadata": {},
   "source": [
    "### Initialize the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94400eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_str: str = \"mistralai:open-mixtral-8x22b\"\n",
    "llm: BaseChatModel = init_chat_model(model=model_str, temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa09db5",
   "metadata": {},
   "source": [
    "## Parallelizing Joke Generation\n",
    "\n",
    "- Define a graph that will:\n",
    "  - take a user input topic\n",
    "  - produce a list of joke topics fro it\n",
    "  - send each joke topic to the LLM\n",
    "- The state has a `jokes` key that will accumulate jokes from parallelized joke generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c1250a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Subjects(BaseModel):\n",
    "    subjects: list[str] = Field(description=\"List of subjects related to the topic.\")\n",
    "\n",
    "\n",
    "class BestJoke(BaseModel):\n",
    "    id: int = Field(description=\"ID of the best joke selected from the list of jokes.\")\n",
    "\n",
    "\n",
    "class OverallState(TypedDict):\n",
    "    topic: str\n",
    "    subjects: list[str]\n",
    "    jokes: Annotated[list[str], add_messages]\n",
    "    best_selected_joke: str\n",
    "\n",
    "\n",
    "class JokeState(TypedDict):\n",
    "    subject: str\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    joke: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273e25bd",
   "metadata": {},
   "source": [
    "#### Test The Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85775822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<system>\n",
      "\n",
      "Generate a list of 3 sub-topics that are all \n",
      "related to this overall topic:\n",
      "\n",
      "<topic>\n",
      "'animals' \n",
      "</topic>\n",
      "\n",
      "</system>\n",
      "\n",
      "subjects=['mammals', 'reptiles', 'birds']\n"
     ]
    }
   ],
   "source": [
    "# OverallState(\n",
    "#     topic=\"Chinedu\",\n",
    "#     subjects=[\"love\", \"work\"],\n",
    "#     jokes=[],\n",
    "#     best_selected_joke=\"\",\n",
    "# )\n",
    "prompt: str = subject_prompt.format(topic=\"animals\")\n",
    "print(prompt)\n",
    "\n",
    "response = await llm.with_structured_output(Subjects).ainvoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abff0418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fbdb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_topics(state: OverallState) -> dict[str, Any]:\n",
    "    \"\"\"Generate a list of subjects based on a given topic.\n",
    "\n",
    "    Parameters:\n",
    "        state (OverallState): The current state containing the topic to generate subjects for.\n",
    "            Expected to have a 'topic' key with a string value.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, Any]\n",
    "\n",
    "    Notes:\n",
    "        Uses an LLM to generate structured output in the form of Subjects.\n",
    "    \"\"\"\n",
    "    prompt: str = subject_prompt.format(topic=state[\"topic\"])\n",
    "    response: Subjects = await llm.with_structured_output(Subjects).ainvoke(prompt)\n",
    "    return {\"subjects\": response.subjects}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3a5799",
   "metadata": {},
   "source": [
    "### Send Function\n",
    "\n",
    "- In LangGraph, Nodes and Edges usually share a predefined state. However, for dynamic cases like map-reduce, LangGraph uses `Send` objects in conditional edges.\n",
    "- It can be used to parallelize tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28261c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.constants import Send\n",
    "\n",
    "\n",
    "def continue_to_jokes(state: OverallState) -> list[Send]:\n",
    "    \"\"\"\n",
    "    Generate N number of jokes in parallel by sending them to the required nodes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : OverallState\n",
    "        The current state containing subjects for joke generation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[Send]\n",
    "    \"\"\"\n",
    "    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\"subjects\"]]\n",
    "\n",
    "\n",
    "async def generate_joke(state: JokeState) -> dict[str, Any]:\n",
    "    prompt: str = joke_prompt.format(subject=state[\"subject\"])\n",
    "    response = await llm.with_structured_output(Joke).ainvoke(prompt)\n",
    "\n",
    "    return {\"jokes\": [response.joke]}\n",
    "\n",
    "\n",
    "async def select_best_joke(state: OverallState) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Select the best joke from a list of jokes based on a given topic.\n",
    "    This is a reduction step.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : OverallState\n",
    "        The current state containing jokes and topic for selection.\n",
    "        Expected keys:\n",
    "            - jokes: list[str | HumanMessage]\n",
    "            - topic: str\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, Any]\n",
    "        Dictionary containing the best selected joke.\n",
    "        Keys:\n",
    "            - best_selected_joke: str\n",
    "    \"\"\"\n",
    "    if isinstance(state[\"jokes\"][0], HumanMessage):\n",
    "        state[\"jokes\"] = [j.content for j in state[\"jokes\"]]\n",
    "    jokes: str = \"\\n\\n\".join(state[\"jokes\"])\n",
    "    prompt: str = best_joke_prompt.format(topic=state[\"topic\"], jokes=jokes)\n",
    "    response = await llm.with_structured_output(BestJoke).ainvoke(prompt)\n",
    "\n",
    "    return {\"best_selected_joke\": state[\"jokes\"][response.id]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e13262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_jokes: list[str] = [\n",
    "    \"Why do seagulls fly over the sea?\",\n",
    "    \"Why dont mammals ever get locked out of their homes? Because \"\n",
    "    \"they always carry their keys with them!\",\n",
    "    \"Why dont reptiles ever forget? Because no one ever lizard them a thing!\",\n",
    "]\n",
    "\n",
    "jokes: str = \"\\n\\n\".join(_jokes)\n",
    "prompt: str = best_joke_prompt.format(topic=\"animals\", jokes=jokes)\n",
    "print(prompt)\n",
    "resp = await llm.with_structured_output(BestJoke).ainvoke(prompt)\n",
    "print(resp.id)\n",
    "result = {\"best_selected_joke\": _jokes[resp.id]}\n",
    "print(f\"{result = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b3fb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(OverallState)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node(\"generate_topics\", generate_topics)\n",
    "graph_builder.add_node(\"generate_joke\", generate_joke)\n",
    "graph_builder.add_node(\"select_best_joke\", select_best_joke)\n",
    "\n",
    "# Add edges\n",
    "graph_builder.add_edge(START, \"generate_topics\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"generate_topics\",\n",
    "    # Generate jokes in parallel by `Sending` the jokes the `generate_joke` node\n",
    "    continue_to_jokes,\n",
    "    [\"generate_joke\"],\n",
    ")\n",
    "graph_builder.add_edge(\"generate_joke\", \"select_best_joke\")\n",
    "graph_builder.add_edge(\"select_best_joke\", END)\n",
    "\n",
    "# Compile the graph\n",
    "graph = graph_builder.compile().with_config(run_name=\"Generate Jokes\")\n",
    "\n",
    "# Visualize the graph\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115a4c18",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Test The Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d437b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic: str = \"animals\"\n",
    "\n",
    "async for s in graph.astream({\"topic\": topic}):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117a299a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79d12cf0",
   "metadata": {},
   "source": [
    "<a id=\"short-term-vs-long-term-memory\"></a>\n",
    "# 2. Short Term vs Long Term Memory\n",
    "\n",
    "## Memory\n",
    "\n",
    "- Memory is a cognitive function that allows people to store, retrieve, and use information to understand their present and future.\n",
    "\n",
    "- There are various long-term memory types that can be used in AI applications.\n",
    "- We'll build a chatbot that uses both `short-term` (within-thread) and `long-term` (across-thread) memory.\n",
    "- We'll focus on long-term semantic memory, which will be facts about the user.\n",
    "- These long-term memories will be used to create a personalized chatbot that can remember facts about the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f529ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95b6c15a",
   "metadata": {},
   "source": [
    "### LangGraph Store\n",
    "\n",
    "- The [LangGraph Memory Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) provides a way to store and retrieve information across threads in LangGraph.\n",
    "- This is an open source base class for persistent key-value stores.\n",
    "\n",
    "- When storing objects (e.g., memories) in the Store, we provide:\n",
    "  - The `namespace` for the object, a tuple (similar to directories)\n",
    "  - the `object key` (similar to filenames)\n",
    "  - the `object value` (similar to file contents)\n",
    "  - We use the `put` method to save an object to the store by `namespace` and `key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9253bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "in_memory_store = InMemoryStore()\n",
    "\n",
    "# Create a namespace to store the data\n",
    "user_id: str = \"1\"\n",
    "namespace_for_memory: tuple[str, str] = (user_id, \"memories\")\n",
    "key: str = str(uuid.uuid4())\n",
    "\n",
    "# The value MUST be a dictionary\n",
    "value: dict[str, Any] = {\"food_preference\": \"I like pizza\"}\n",
    "\n",
    "# Save the data\n",
    "in_memory_store.put(namespace_for_memory, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db82591",
   "metadata": {},
   "source": [
    "#### Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da124c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "memories = in_memory_store.search(namespace_for_memory)\n",
    "console.log(memories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb37c017",
   "metadata": {},
   "outputs": [],
   "source": [
    "memories[0].dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913d3072",
   "metadata": {},
   "source": [
    "#### Get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f3b8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "memories = in_memory_store.get(namespace_for_memory, key)\n",
    "memories.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1288f834",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Chatbot With Long Term Memory\n",
    "\n",
    "- We want a chatbot that has two types of memory:\n",
    "  - **Short-term (within-thread) memory**: Chatbot can persist conversational history and / or allow interruptions in a chat session.\n",
    "  - **Long-term (cross-thread) memory**: Chatbot can remember information about a specific user across all chat sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a74c656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langgraph.store.base import BaseStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa874d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "\n",
    "class DataStateValidator(BaseModel):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ad8c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chatbot instruction\n",
    "MODEL_SYSTEM_MESSAGE: str = \"\"\"\n",
    "<system>\n",
    "\n",
    "<role>\n",
    "You are a helpful assistant with memory that provides information about the user.\n",
    "If you have memory for this user, use it to personalize your responses.\n",
    "</role>\n",
    "\n",
    "<memory>\n",
    "{memory}\n",
    "</memory>\n",
    "\n",
    "<quality_standards>\n",
    "- **ALWAYS** use the information in memory.\n",
    "</quality_standards>\n",
    "\n",
    "</system>\n",
    "\"\"\"\n",
    "\n",
    "# Create new memory from the chat history and any existing memory\n",
    "CREATE_MEMORY_INSTRUCTION: str = \"\"\"\"\n",
    "<system>\n",
    "\n",
    "<role>\n",
    "You are collecting information about the user to personalize your responses.\n",
    "</role>\n",
    "\n",
    "<current_user_info>\n",
    "{memory}\n",
    "</current_user_info>\n",
    "\n",
    "<instruction>\n",
    "1. Review the chat history below carefully\n",
    "2. Identify new information about the user, such as:\n",
    "   - Personal details (name, location)\n",
    "   - Preferences (likes, dislikes)\n",
    "   - Interests and hobbies\n",
    "   - Past experiences\n",
    "   - Goals or future plans\n",
    "3. Merge any new information with existing memory\n",
    "4. Format the memory as a clear, bulleted list\n",
    "5. If new information conflicts with existing memory, keep the most recent version\n",
    "Remember: Only include factual information directly stated by the user. Do not make assumptions or inferences.\n",
    "</instruction>\n",
    "\n",
    "Based on the chat history below, please update the user information:\n",
    "\n",
    "<system>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075ff588",
   "metadata": {},
   "source": [
    "### Initialize the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a2af1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_str: str = f\"openai:{ModelEnum.LLAMA_3p2_3B_INSTRUCT_REMOTE.value}\"\n",
    "llm: BaseChatModel = init_chat_model(\n",
    "    model=model_str,\n",
    "    api_key=settings.OPENROUTER_API_KEY.get_secret_value(),\n",
    "    base_url=settings.OPENROUTER_URL,\n",
    "    temperature=0.0,\n",
    "    seed=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca64e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def call_llm(state: DataState, config: RunnableConfig, store: BaseStore) -> dict[str, Any]:\n",
    "    # Get the user id\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Get the memory for the user\n",
    "    prefix: str = \"memory\"\n",
    "    key = \"user_memory\"\n",
    "    namespace = (prefix, user_id)\n",
    "    existing_memory = store.get(namespace, key)\n",
    "    # existing_memory = await store.aget(namespace, key)\n",
    "\n",
    "    if existing_memory:\n",
    "        existing_memory_content = existing_memory.value.get(prefix)\n",
    "    else:\n",
    "        existing_memory_content = \"No existing memory found\"\n",
    "\n",
    "    system_message: str = MODEL_SYSTEM_MESSAGE.format(memory=existing_memory_content)\n",
    "    # Respond using memory + chat history\n",
    "    response = await llm.ainvoke([SystemMessage(content=system_message)] + state[\"messages\"])\n",
    "\n",
    "    # Validate\n",
    "    output = {\"messages\": [response]}\n",
    "    validate_data(data=output, state=state, response_model=DataStateValidator)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "async def write_memory(state: DataState, config: RunnableConfig, store: BaseStore) -> None:\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    prefix: str = \"memory\"\n",
    "    key = \"user_memory\"\n",
    "    namespace = (prefix, user_id)\n",
    "    existing_memory = store.get(namespace, key)\n",
    "    # existing_memory = await store.aget(namespace, key)\n",
    "\n",
    "    if existing_memory:\n",
    "        existing_memory_content = existing_memory.value.get(prefix)\n",
    "    else:\n",
    "        existing_memory_content = \"No existing memory found\"\n",
    "\n",
    "    system_message: str = CREATE_MEMORY_INSTRUCTION.format(memory=existing_memory_content)\n",
    "    # Respond using memory + chat history\n",
    "    new_memory = await llm.ainvoke([SystemMessage(content=system_message)] + state[\"messages\"])\n",
    "\n",
    "    # Update existing memory\n",
    "    # await store.aput(namespace, key, {prefix: new_memory.content})\n",
    "    store.put(namespace, key, {prefix: new_memory.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfae459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph\n",
    "graph_builder = StateGraph(DataState)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node(\"call_llm\", call_llm)\n",
    "graph_builder.add_node(\"write_memory\", write_memory)\n",
    "\n",
    "# Add edges\n",
    "graph_builder.add_edge(START, \"call_llm\")\n",
    "graph_builder.add_edge(\"call_llm\", \"write_memory\")\n",
    "graph_builder.add_edge(\"write_memory\", END)\n",
    "\n",
    "# Long-term-memory store (across threads)\n",
    "across_thread_memory = InMemoryStore()\n",
    "# Short-term-memory store (within a thread) checkpointer\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# Compile\n",
    "graph = graph_builder.compile(\n",
    "    checkpointer=within_thread_memory, store=across_thread_memory\n",
    ").with_config(run_name=\"chatbot-with-memory\")\n",
    "\n",
    "# Visualize\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc2cfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "config: dict[str, Any] = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "input_msg = [\"Hello, my name is Neidu\"]\n",
    "\n",
    "async for event in graph.astream({\"messages\": input_msg}, config=config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24c62c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_msg = [\"I'm an AI Engineer whos's currently working on NLP related things\"]\n",
    "\n",
    "async for event in graph.astream({\"messages\": input_msg}, config=config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31de1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_msg = [\"What do you know about me so far?\"]\n",
    "\n",
    "async for event in graph.astream({\"messages\": input_msg}, config=config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fe859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat history\n",
    "state = graph.get_state(config=config).values\n",
    "\n",
    "for m in state[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cb69f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# namespace for the memory to save\n",
    "user_id: str = \"1\"\n",
    "prefix: str = \"memory\"\n",
    "key = \"user_memory\"\n",
    "namespace = (prefix, user_id)\n",
    "existing_memory = across_thread_memory.get(namespace, key)\n",
    "existing_memory.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84ae042",
   "metadata": {},
   "source": [
    "#### Use Another Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_2: dict[str, Any] = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "input_msg = [\"What do you know about me so far?\"]\n",
    "\n",
    "async for event in graph.astream({\"messages\": input_msg}, config=config_2, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9627b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a5d07a4",
   "metadata": {},
   "source": [
    "<a id=\"chatbot-with-profile-schema\"></a>\n",
    "\n",
    "# Chatbot with Profile Schema\n",
    "\n",
    "- We introduced the LangGraph Memory Store as a way to save and retrieve long-term memories.\n",
    "- We built a simple chatbot that uses both short-term (within-thread) and long-term (across-thread) memory.\n",
    "- It saved long-term semantic memory (facts about the user) \"in the hot path\", as the user is chatting with it.\n",
    "- Our chatbot saved memories as a string. In practice, we often want memories to have a structure.\n",
    "\n",
    "\n",
    "## Profiles\n",
    "\n",
    "- For example, memories can be a single, continuously updated schema.\n",
    "- In our case, we want this to be a single user profile.\n",
    "\n",
    "- We'll extend our chatbot to save semantic memories to a single user profile.\n",
    "\n",
    "- We'll also introduce a library, `Trustcall`, to update this schema with new information.\n",
    "\n",
    "### [Go to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13a51ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TypedDict\n",
    "\n",
    "\n",
    "class UserProfile(TypedDict):\n",
    "    user_name: str  # User's preferred name\n",
    "    interests: list[str]  # User's interests\n",
    "\n",
    "\n",
    "# Save a schema to the store\n",
    "user_profile = UserProfile(user_name=\"Neidu\", interests=[\"AI\", \"NLP\", \"Spiritual Growth\"])\n",
    "user_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d86d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a store\n",
    "in_memory_store = InMemoryStore()\n",
    "\n",
    "user_id: str = \"1\"\n",
    "prefix: str = \"memory\"\n",
    "key = \"user_profile\"\n",
    "namespace = (prefix, user_id)\n",
    "across_thread_memory.put(namespace, key, user_profile)\n",
    "\n",
    "in_memory_store.put(namespace, key, user_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the data\n",
    "\n",
    "for data in in_memory_store.search(namespace):\n",
    "    print(data.dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c2fbcc",
   "metadata": {},
   "source": [
    "## Chatbot With Profile Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d09c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test the user profile with structured outputs\n",
    "llm_with_structured_output = llm.with_structured_output(UserProfile)\n",
    "message: str = (\n",
    "    \"I'm Neidu and I'm a AI Engineer. I love working on AI and NLP \"\n",
    "    \"related projects. I'm also looking for spiritual growth.\"\n",
    ")\n",
    "\n",
    "response = await llm_with_structured_output.ainvoke([HumanMessage(content=message)])\n",
    "console.log(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e950ca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class UserProfile(BaseModel):\n",
    "#     user_name: str  # User's preferred name\n",
    "#     other_personal_details: Annotated[list[str], add_messages] = Field(\n",
    "#         default_factory=list, description=\"Other personal details. e.g. location\"\n",
    "#     )\n",
    "#     interests: list[str] = Field(default_factory=list, description=\"User's interests\")\n",
    "#     preferences: list[str] = Field(default_factory=list, description=\"Likes, dislikes, etc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b7da2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chatbot instruction\n",
    "MODEL_SYSTEM_MESSAGE: str = \"\"\"\n",
    "<system>\n",
    "\n",
    "<role>\n",
    "You are a helpful assistant with memory that provides information about the user.\n",
    "If you have memory for this user, use it to personalize your responses.\n",
    "</role>\n",
    "\n",
    "<memory>\n",
    "{memory}\n",
    "</memory>\n",
    "\n",
    "<quality_standards>\n",
    "- **ALWAYS** use the information in memory.\n",
    "</quality_standards>\n",
    "\n",
    "</system>\n",
    "\"\"\"\n",
    "\n",
    "# Create new memory from the chat history and any existing memory\n",
    "CREATE_MEMORY_INSTRUCTION: str = \"\"\"\"\n",
    "<system>\n",
    "\n",
    "<role>\n",
    "You are collecting information about the user to personalize your responses.\n",
    "</role>\n",
    "\n",
    "<current_user_info>\n",
    "{memory}\n",
    "</current_user_info>\n",
    "\n",
    "<instruction>\n",
    "1. If there's exisiting memory, simply update it.\n",
    "2. If new information conflicts with existing memory, keep the most recent version.\n",
    "Remember: Only include factual information directly stated by the user. Do not make assumptions or inferences.\n",
    "</instruction>\n",
    "\n",
    "Based on the chat history below, please update the user information:\n",
    "\n",
    "<system>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def call_llm(state: DataState, config: RunnableConfig, store: BaseStore) -> dict[str, Any]:\n",
    "    # Get the user id\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Get the memory for the user\n",
    "    prefix: str = \"memory\"\n",
    "    key = \"user_memory\"\n",
    "    namespace = (prefix, user_id)\n",
    "    existing_memory = store.get(namespace, key)\n",
    "    # existing_memory = await store.aget(namespace, key)\n",
    "\n",
    "    if existing_memory and existing_memory.value:\n",
    "        memory_dict = existing_memory.value\n",
    "        formatted_memory = (\n",
    "            f\"Name: {memory_dict.get('user_name', 'Unknown')}\\n\"\n",
    "            f\"Interests: {', '.join(memory_dict.get('interests', []))}\\n\"\n",
    "        )\n",
    "    else:\n",
    "        formatted_memory = None\n",
    "\n",
    "    system_message: str = MODEL_SYSTEM_MESSAGE.format(memory=formatted_memory)\n",
    "    # Respond using memory + chat history\n",
    "    response = await llm.ainvoke([SystemMessage(content=system_message)] + state[\"messages\"])\n",
    "\n",
    "    # Validate\n",
    "    output = {\"messages\": [response]}\n",
    "    validate_data(data=output, state=state, response_model=DataStateValidator)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "async def write_memory(state: DataState, config: RunnableConfig, store: BaseStore) -> None:\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    prefix: str = \"memory\"\n",
    "    key = \"user_memory\"\n",
    "    namespace = (prefix, user_id)\n",
    "    existing_memory = store.get(namespace, key)\n",
    "    # existing_memory = await store.aget(namespace, key)\n",
    "\n",
    "    if existing_memory and existing_memory.value:\n",
    "        memory_dict = existing_memory.value\n",
    "        formatted_memory = (\n",
    "            f\"Name: {memory_dict.get('user_name', 'Unknown')}\\n\"\n",
    "            f\"Interests: {', '.join(memory_dict.get('interests', []))}\\n\"\n",
    "        )\n",
    "    else:\n",
    "        formatted_memory = None\n",
    "\n",
    "    system_message: str = CREATE_MEMORY_INSTRUCTION.format(memory=formatted_memory)\n",
    "    # Respond using memory + chat history\n",
    "    llm_with_structured_output = llm.with_structured_output(UserProfile)\n",
    "    new_memory = await llm_with_structured_output.ainvoke(\n",
    "        [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    )\n",
    "\n",
    "    # Update existing memory\n",
    "    # await store.aput(namespace, key, {prefix: new_memory.content})\n",
    "    store.put(namespace, key, {prefix: new_memory})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph\n",
    "graph_builder = StateGraph(DataState)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node(\"call_llm\", call_llm)\n",
    "graph_builder.add_node(\"write_memory\", write_memory)\n",
    "\n",
    "# Add edges\n",
    "graph_builder.add_edge(START, \"call_llm\")\n",
    "graph_builder.add_edge(\"call_llm\", \"write_memory\")\n",
    "graph_builder.add_edge(\"write_memory\", END)\n",
    "\n",
    "# Long-term-memory store (across threads)\n",
    "across_thread_memory = InMemoryStore()\n",
    "# Short-term-memory store (within a thread) checkpointer\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# Compile\n",
    "graph = graph_builder.compile(\n",
    "    checkpointer=within_thread_memory, store=across_thread_memory\n",
    ").with_config(run_name=\"chatbot-with-structured-memory\")\n",
    "\n",
    "# Visualize\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722fe344",
   "metadata": {},
   "outputs": [],
   "source": [
    "config: dict[str, Any] = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "input_msg = [\"Hello, my name is Neidu\"]\n",
    "\n",
    "async for event in graph.astream({\"messages\": input_msg}, config=config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_msg = [\"I'm an AI Engineer whos's currently working on NLP related things\"]\n",
    "\n",
    "async for event in graph.astream({\"messages\": input_msg}, config=config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e975b96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = \"1\"\n",
    "prefix: str = \"memory\"\n",
    "key = \"user_memory\"\n",
    "namespace = (prefix, user_id)\n",
    "existing_memory = across_thread_memory.get(namespace, key)\n",
    "\n",
    "console.log(existing_memory.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e9e60a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### When can this fail?\n",
    "\n",
    "- with_structured_output is very useful, but what happens if we're working with a more complex schema?\n",
    "- Here's an example of a more complex schema, which we'll test below.\n",
    "- This is a Pydantic model that describes a user's preferences for communication and trust fall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ecb9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputFormat(BaseModel):\n",
    "    preference: str\n",
    "    sentence_preference_revealed: str\n",
    "\n",
    "\n",
    "class TelegramPreferences(BaseModel):\n",
    "    preferred_encoding: list[OutputFormat] | None = None\n",
    "    favorite_telegram_operators: list[OutputFormat] | None = None\n",
    "    preferred_telegram_paper: list[OutputFormat] | None = None\n",
    "\n",
    "\n",
    "class MorseCode(BaseModel):\n",
    "    preferred_key_type: list[OutputFormat] | None = None\n",
    "    favorite_morse_abbreviations: list[OutputFormat] | None = None\n",
    "\n",
    "\n",
    "class Semaphore(BaseModel):\n",
    "    preferred_flag_color: list[OutputFormat] | None = None\n",
    "    semaphore_skill_level: list[OutputFormat] | None = None\n",
    "\n",
    "\n",
    "class TrustFallPreferences(BaseModel):\n",
    "    preferred_fall_height: list[OutputFormat] | None = None\n",
    "    trust_level: list[OutputFormat] | None = None\n",
    "    preferred_catching_technique: list[OutputFormat] | None = None\n",
    "\n",
    "\n",
    "class CommunicationPreferences(BaseModel):\n",
    "    telegram: TelegramPreferences\n",
    "    morse_code: MorseCode\n",
    "    semaphore: Semaphore\n",
    "\n",
    "\n",
    "class UserPreferences(BaseModel):\n",
    "    communication_preferences: CommunicationPreferences\n",
    "    trust_fall_preferences: TrustFallPreferences\n",
    "\n",
    "\n",
    "class TelegramAndTrustFallPreferences(BaseModel):\n",
    "    pertinent_user_preferences: UserPreferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ebf80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import JSONDecodeError\n",
    "\n",
    "from pydantic import ValidationError\n",
    "\n",
    "# Now, let's try extraction of this schema using the with_structured_output method.\n",
    "# Bind schema to model\n",
    "llm_with_structure = llm.with_structured_output(TelegramAndTrustFallPreferences)\n",
    "\n",
    "# Conversation\n",
    "conversation: str = \"\"\"Operator: How may I assist with your telegram, sir?\n",
    "Customer: I need to send a message about our trust fall exercise.\n",
    "Operator: Certainly. Morse code or standard encoding?\n",
    "Customer: Morse, please. I love using a straight key.\n",
    "Operator: Excellent. What's your message?\n",
    "Customer: Tell him I'm ready for a higher fall, and I prefer the diamond formation for catching.\n",
    "Operator: Done. Shall I use our \"Daredevil\" paper for this daring message?\n",
    "Customer: Perfect! Send it by your fastest carrier pigeon.\n",
    "Operator: It'll be there within the hour, sir.\"\"\"\n",
    "\n",
    "# Invoke the model\n",
    "try:\n",
    "    llm_with_structure.invoke(\n",
    "        f\"\"\"Extract the preferences from the following conversation:\n",
    "    <convo>\n",
    "    {conversation}\n",
    "    </convo>\"\"\"\n",
    "    )\n",
    "except (ValidationError, JSONDecodeError) as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc49581",
   "metadata": {},
   "source": [
    "### Trustcall For Creating and Updating Profile Schemas\n",
    "\n",
    "- As we can see, working with schemas can be tricky. Complex schemas can be difficult to extract.\n",
    "- In addition, updating even simple schemas can pose challenges.\n",
    "\n",
    "- Consider our above chatbot.\n",
    "- We regenerated the profile schema from scratch each time we chose to save a new memory.\n",
    "- This is inefficient, potentially wasting model tokens if the schema contains a lot of information to re-generate each time.\n",
    "- Worse, we may loose information when regenerating the profile from scratch.\n",
    "- Addressing these problems is the motivation for TrustCall!\n",
    "- It's motivated by exactly these challenges while working on memory.\n",
    "\n",
    "Let's first show simple usage of extraction with TrustCall on this list of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73038184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation\n",
    "conversation = [\n",
    "    HumanMessage(content=\"Hi, I'm Neidu.\"),\n",
    "    AIMessage(content=\"Nice to meet you, Neidu.\"),\n",
    "    HumanMessage(content=\"I'm an AI Engineer whos's currently working on NLP related things\"),\n",
    "]\n",
    "\n",
    "conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a0f69b",
   "metadata": {},
   "source": [
    "- We use `create_extractor`, passing in the model and schema as a tool \n",
    "- `TrustCall` allows supplying the schema in various formats, such as `JSON`, `Python dictionaries`, or `Pydantic` models \n",
    "- Internally, TrustCall uses tool calling to generate structured output from message inputs \n",
    "- To enforce structured output, we can include the schema name in the tool_choice argument \n",
    "- The extractor can then be invoked with a conversation to produce validated, structured results.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Note\n",
    "\n",
    "- From my experience, I noticed `trustcall` performs better with larger LLMs .e.g. GPT, Gemini models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d14dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_str: str = f\"openai:{ModelEnum.GPT_4_o_MINI_REMOTE.value}\"\n",
    "llm: BaseChatModel = init_chat_model(\n",
    "    model=model_str,\n",
    "    api_key=settings.OPENROUTER_API_KEY.get_secret_value(),\n",
    "    base_url=settings.OPENROUTER_URL,\n",
    "    temperature=0.0,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "# Conversation\n",
    "conversation = [\n",
    "    HumanMessage(content=\"Hi, I'm Neidu.\"),\n",
    "    AIMessage(content=\"Nice to meet you, Neidu.\"),\n",
    "    HumanMessage(content=\"I'm an AI Engineer whos's currently working on NLP related things\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trustcall import create_extractor\n",
    "\n",
    "\n",
    "class UserProfileSchema(BaseModel):\n",
    "    user_name: str = Field(description=\"User's preferred name\")\n",
    "    interests: list[str] = Field(default_factory=list, description=\"User's interests\")\n",
    "\n",
    "\n",
    "trustcall_extractor = create_extractor(\n",
    "    llm,\n",
    "    tools=[UserProfileSchema],\n",
    "    tool_choice=\"UserProfileSchema\",\n",
    ")\n",
    "system_message = \"Extract the user profile from the following conversation.\"\n",
    "\n",
    "# Invoke the extractor\n",
    "result = await trustcall_extractor.ainvoke(\n",
    "    {\"messages\": [SystemMessage(content=system_message)] + conversation}\n",
    ")\n",
    "\n",
    "console.print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a842295",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad04f1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = result[\"responses\"][0]\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80034523",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfee401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"response_metadata\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff59b201",
   "metadata": {},
   "source": [
    "#### Update Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_conversation = [\n",
    "    HumanMessage(content=\"Hi, I'm Neidu.\"),\n",
    "    AIMessage(content=\"Nice to meet you, Neidu.\"),\n",
    "    HumanMessage(content=\"I'm an AI Engineer whos's currently working on NLP related things\"),\n",
    "    AIMessage(\n",
    "        content=\"That sounds fascinating! NLP is a really exciting and impactful field in AI. \"\n",
    "        \"What will you like to know about AI?\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Thanks! I'm curious about the latest advancements in AI and how \"\n",
    "        \"they're being applied in real-world applications. I'm also interested in \"\n",
    "        \"agentic AI workflow\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "system_message = \"\"\"\n",
    "<system>\n",
    "Update the memory (JSON doc) to incorporate new information from the following conversation\n",
    "</system>\n",
    "\"\"\"\n",
    "\n",
    "result = await trustcall_extractor.ainvoke(\n",
    "    {\"messages\": [SystemMessage(content=system_message)] + updated_conversation},\n",
    "    {\"existing\": {\"UserProfileSchema\": schema.model_dump()}},\n",
    ")\n",
    "\n",
    "\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862ed816",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"response_metadata\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cc907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_schema = result[\"responses\"][0]\n",
    "updated_schema.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0297610",
   "metadata": {},
   "source": [
    "#### Test Trustcall Using The Previously Failed Example\n",
    "\n",
    "- Unfortunately, the example is still failing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cfca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind schema to model using trustcall\n",
    "bound = create_extractor(\n",
    "    llm,\n",
    "    tools=[TelegramAndTrustFallPreferences],\n",
    "    tool_choice=\"TelegramAndTrustFallPreferences\",\n",
    ")\n",
    "\n",
    "# Conversation\n",
    "conversation: str = \"\"\"Operator: How may I assist with your telegram, sir?\n",
    "Customer: I need to send a message about our trust fall exercise.\n",
    "Operator: Certainly. Morse code or standard encoding?\n",
    "Customer: Morse, please. I love using a straight key.\n",
    "Operator: Excellent. What's your message?\n",
    "Customer: Tell him I'm ready for a higher fall, and I prefer the diamond formation for catching.\n",
    "Operator: Done. Shall I use our \"Daredevil\" paper for this daring message?\n",
    "Customer: Perfect! Send it by your fastest carrier pigeon.\n",
    "Operator: It'll be there within the hour, sir.\"\"\"\n",
    "\n",
    "# Invoke the model\n",
    "try:\n",
    "    response = await bound.ainvoke(\n",
    "        f\"\"\"Extract the preferences from the following conversation:\n",
    "    <convo>\n",
    "    {conversation}\n",
    "    </convo>\"\"\"\n",
    "    )\n",
    "except (ValidationError, JSONDecodeError) as e:\n",
    "    print(e)\n",
    "\n",
    "response[\"responses\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d6fb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"responses\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db1a567",
   "metadata": {},
   "source": [
    "## Chatbot With Profile Schema Updating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57caead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_str: str = f\"openai:{ModelEnum.GEMINI_2p5_FLASH_REMOTE.value}\"\n",
    "llm: BaseChatModel = init_chat_model(\n",
    "    model=model_str,\n",
    "    api_key=settings.OPENROUTER_API_KEY.get_secret_value(),\n",
    "    base_url=settings.OPENROUTER_URL,\n",
    "    temperature=0.0,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "\n",
    "class UserProfileSchema(BaseModel):\n",
    "    \"\"\"A schema for storing user profile information.\"\"\"\n",
    "\n",
    "    user_name: str = Field(description=\"User's preferred name\")\n",
    "    user_location: str = Field(description=\"User's location\")\n",
    "    interests: list[str] = Field(default_factory=list, description=\"User's interests\")\n",
    "\n",
    "\n",
    "trustcall_extractor = create_extractor(\n",
    "    llm,\n",
    "    tools=[UserProfileSchema],\n",
    "    tool_choice=\"UserProfileSchema\",\n",
    ")\n",
    "\n",
    "MODEL_SYSTEM_MESSAGE: str = \"\"\"\n",
    "<system>\n",
    "\n",
    "<role>\n",
    "You are a helpful assistant with memory that responds to the user in a conversational way. \n",
    "If you have memory for this user, use it to personalize your responses.\n",
    "</role>\n",
    "\n",
    "<memory>\n",
    "{memory}\n",
    "</memory>\n",
    "\n",
    "</system>\n",
    "\"\"\"\n",
    "\n",
    "# Extraction instruction\n",
    "TRUSTCALL_INSTRUCTION: str = \"\"\"\n",
    "<instruction>\n",
    "Create or update the memory (JSON doc) to incorporate information from the following conversation:\n",
    "</instruction>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6915a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def call_llm(state: DataState, config: RunnableConfig, store: BaseStore) -> dict[str, Any]:\n",
    "    user_id: str = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    prefix: str = \"memory\"\n",
    "    key: str = \"user_memory\"\n",
    "    namespace = (prefix, user_id)\n",
    "    existing_memory = store.get(namespace, key)\n",
    "\n",
    "    if existing_memory and existing_memory.value:\n",
    "        memory_dict = existing_memory.value\n",
    "        formatted_memory = (\n",
    "            f\"Name: {memory_dict.get('user_name', 'Unknown')}\\n\"\n",
    "            f\"Location: {memory_dict.get('user_location', 'Unknown')}\\n\"\n",
    "            f\"Interests: {', '.join(memory_dict.get('interests', []))}\"\n",
    "        )\n",
    "    else:\n",
    "        formatted_memory = None\n",
    "\n",
    "    system_message: str = MODEL_SYSTEM_MESSAGE.format(memory=formatted_memory)\n",
    "    # Respond using memory + chat history\n",
    "    response = await llm.ainvoke([SystemMessage(content=system_message)] + state[\"messages\"])\n",
    "    # Validate\n",
    "    output = {\"messages\": [response]}\n",
    "    validate_data(data=output, state=state, response_model=DataStateValidator)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "async def write_memory(state: DataState, config: RunnableConfig, store: BaseStore) -> None:\n",
    "    user_id: str = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    prefix: str = \"memory\"\n",
    "    key: str = \"user_memory\"\n",
    "    namespace = (prefix, user_id)\n",
    "    existing_memory = store.get(namespace, key)\n",
    "\n",
    "    # Get the profile and convert to JSON doc\n",
    "    existing_profile = {\"UserProfileSchema\": existing_memory.value} if existing_memory else None\n",
    "    # Invoke the extractor\n",
    "    result = await trustcall_extractor.ainvoke(\n",
    "        {\n",
    "            \"messages\": [SystemMessage(content=TRUSTCALL_INSTRUCTION)] + state[\"messages\"],\n",
    "            \"existing\": existing_profile,\n",
    "        },\n",
    "    )\n",
    "    # Get the updated profile as a JSON object\n",
    "    updated_profile = result[\"responses\"][0].model_dump()\n",
    "    # Save the updated profile\n",
    "    store.put(namespace, key, updated_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a15ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(DataState)\n",
    "\n",
    "# Add nodes\n",
    "graph_builder.add_node(\"call_llm\", call_llm)\n",
    "graph_builder.add_node(\"write_memory\", write_memory)\n",
    "\n",
    "# Add edges\n",
    "graph_builder.add_edge(START, \"call_llm\")\n",
    "graph_builder.add_edge(\"call_llm\", \"write_memory\")\n",
    "graph_builder.add_edge(\"write_memory\", END)\n",
    "\n",
    "# Long-term-memory store (across threads)\n",
    "across_thread_memory = InMemoryStore()\n",
    "# Short-term-memory store (within a thread) checkpointer\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# Compile the graph\n",
    "graph = graph_builder.compile(\n",
    "    checkpointer=within_thread_memory, store=across_thread_memory\n",
    ").with_config(run_name=\"Chatbot-with-user-profile\")\n",
    "\n",
    "\n",
    "# Visualize the graph\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11660760",
   "metadata": {},
   "outputs": [],
   "source": [
    "config: dict[str, Any] = {\"configurable\": {\"thread_id\": \"1\"}, \"user_id\": \"1\"}\n",
    "input_msg: list[Any] = [\n",
    "    HumanMessage(content=\"hello, my name is Chinedu. I live in Lagos, Nigeria.\")\n",
    "]\n",
    "\n",
    "\n",
    "async for event in graph.astream({\"messages\": input_msg}, config=config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee8b40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "across_thread_memory.get((\"memory\", \"1\"), \"user_memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fac538",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_msg = [\"I'm an AI Engineer whos's currently working on NLP related things\"]\n",
    "\n",
    "async for event in graph.astream({\"messages\": input_msg}, config=config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce068647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# namespace for the memory to save\n",
    "user_id: str = \"1\"\n",
    "prefix: str = \"memory\"\n",
    "key = \"user_memory\"\n",
    "namespace = (prefix, user_id)\n",
    "existing_memory = across_thread_memory.get(namespace, key)\n",
    "existing_memory.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ebd08e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## [TOP](#top)\n",
    "\n",
    "<br>\n",
    "\n",
    "<a id=\"chatbot-with-profile-collection\"></a>\n",
    "## Chatbot with Profile Collection\n",
    "\n",
    "- Sometimes we want to save memories to a `collection` rather than single profile.\n",
    "- We'll update our chatbot to save memories to a collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_str: str = f\"openai:{ModelEnum.GPT_4_o_MINI_REMOTE.value}\"\n",
    "llm: BaseChatModel = init_chat_model(\n",
    "    model=model_str,\n",
    "    api_key=settings.OPENROUTER_API_KEY.get_secret_value(),\n",
    "    base_url=settings.OPENROUTER_URL,\n",
    "    temperature=0.0,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "\n",
    "class Memory(BaseModel):\n",
    "    content: str = Field(description=\"The main content of the memory.\")\n",
    "\n",
    "\n",
    "class MemoryCollection(BaseModel):\n",
    "    memories: list[Memory] = Field(description=\"A list of memories.\", default_factory=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ae1e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_structure = llm.with_structured_output(MemoryCollection)\n",
    "memory_collection = await llm_with_structure.ainvoke(\n",
    "    [HumanMessage(content=\"My name is Neidu. I love reading books.\")]\n",
    ")\n",
    "\n",
    "console.log(memory_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a3e48c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e72ad4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc4ea24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ae9c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1821a8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb7672d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8b176a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
